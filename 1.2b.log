using world size: 8, data-parallel-size: 8, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.999
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ False
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  data_impl ....................................... mmap
  data_parallel_size .............................. 8
  data_path ....................................... ['../bigdata/']
  dataloader_type ................................. cyclic
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  embedding_path .................................. None
  encoder_seq_length .............................. 127
  eod_mask_loss ................................... False
  eval_interval ................................... 500
  eval_iters ...................................... 10
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  ffn_hidden_size ................................. 8192
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 32
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 2048
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  init_method_std ................................. 0.02
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  kv_channels ..................................... 64
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ verify1
  local_rank ...................................... 0
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 100
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0001
  lr_decay_iters .................................. 99000
  lr_decay_samples ................................ None
  lr_decay_style .................................. linear
  lr_warmup_fraction .............................. 0.002
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 127
  merge_file ...................................... None
  micro_batch_size ................................ 4
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mmap_warmup ..................................... False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 32
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_layers ...................................... 24
  num_layers_per_virtual_pipeline_stage ........... None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  sample_rate ..................................... 1.0
  save ............................................ verify1
  save_interval ................................... 50000
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  segment_length .................................. 2048
  seq_length ...................................... 127
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 949,50,1
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. None
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  titles_data_path ................................ None
  tokenizer_type .................................. BertWordPieceLowerCase
  train_iters ..................................... 200000
  train_samples ................................... None
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... ./bert_vocab_files/bert-base-uncased-vocab.txt
  weight_decay .................................... 0.01
  world_size ...................................... 8
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 1
> building BertWordPieceLowerCase tokenizer ...
 > padded vocab (size: 30524) with 68 dummy tokens (new size: 30592)
> initializing torch distributed ...
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /workspace/zhouy/megatron-gw/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /workspace/zhouy/megatron-gw/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /workspace/zhouy/megatron-gw/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_mix_prec_layer_norm_cuda...
>>> done with compiling and loading fused kernels. Compilation time: 7.453 seconds
time to initialize megatron (seconds): 27.328
[after megatron is initialized] datetime: 2021-12-23 13:03:23 
building BERT model ...
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1216997376
> learning rate decay style: linear
WARNING: could not find the metadata file verify1/latest_checkpointed_iteration.txt 
    will not load any checkpoints and will start from random
time (ms) | load-checkpoint: 0.53
[after model, optimizer, and learning rate scheduler are built] datetime: 2021-12-23 13:03:24 
> building train, validation, and test datasets ...
> building train, validation, and test datasets for BERT ...
> finished creating BERT datasets ...
[after dataloaders are built] datetime: 2021-12-23 13:03:30 time (ms) | model-and-optimizer-setup: 837.92 | train/valid/test-data-iterators-setup: 6100.09

done with setup ...
training ...
[before the start of training step] datetime: 2021-12-23 13:03:30 
 iteration      100/  200000 | consumed samples:         3200 | elapsed time per iteration (ms): 303.6 | learning rate: 4.444E-05 | global batch size:    32 | lm loss: 9.706122E-02 | loss scale: 2097152.0 | grad norm: 0.024 | number of skipped iterations:  12 | number of nan iterations:   0 |
time (ms) | backward-compute: 56.97 | backward-params-all-reduce: 66.16 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 14.00 | optimizer-unscale-and-check-inf: 19.24 | optimizer-clip-main-grad: 8.19 | optimizer-copy-main-to-model-params: 10.71 | optimizer: 99.68 | batch-generator: 13.67
[Rank 0] (after 100 iterations) memory (MB) | allocated: 23212.96435546875 | max allocated: 23214.6025390625 | reserved: 26164.0 | max reserved: 26164.0
 iteration      200/  200000 | consumed samples:         6400 | elapsed time per iteration (ms): 264.3 | learning rate: 9.495E-05 | global batch size:    32 | lm loss: 2.122366E-03 | loss scale: 2097152.0 | grad norm: 0.021 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.77 | backward-params-all-reduce: 65.66 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.94 | optimizer-unscale-and-check-inf: 14.99 | optimizer-clip-main-grad: 7.26 | optimizer-copy-main-to-model-params: 13.71 | optimizer: 95.87 | batch-generator: 7.89
 iteration      300/  200000 | consumed samples:         9600 | elapsed time per iteration (ms): 261.4 | learning rate: 9.992E-05 | global batch size:    32 | lm loss: 1.275565E-03 | loss scale: 2097152.0 | grad norm: 0.012 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.58 | backward-params-all-reduce: 67.07 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.02 | optimizer-unscale-and-check-inf: 19.65 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 99.14 | batch-generator: 7.96
 iteration      400/  200000 | consumed samples:        12800 | elapsed time per iteration (ms): 262.2 | learning rate: 9.983E-05 | global batch size:    32 | lm loss: 1.008312E-03 | loss scale: 2097152.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.86 | backward-params-all-reduce: 65.04 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.91 | optimizer-unscale-and-check-inf: 16.60 | optimizer-clip-main-grad: 7.27 | optimizer-copy-main-to-model-params: 12.14 | optimizer: 95.88 | batch-generator: 7.89
 iteration      500/  200000 | consumed samples:        16000 | elapsed time per iteration (ms): 259.3 | learning rate: 9.974E-05 | global batch size:    32 | lm loss: 9.232771E-04 | loss scale: 2097152.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.70 | backward-params-all-reduce: 64.83 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.88 | optimizer-unscale-and-check-inf: 21.64 | optimizer-clip-main-grad: 7.40 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 101.03 | batch-generator: 8.13
-----------------------------------------------------------------------------------------------
 validation loss at iteration 500 | lm loss value: 6.948888E-04 | lm loss PPL: 1.000695E+00 | 
 at iteration 500, match long value: 0.0011605112495240327 | match short value: 0.019676115992471748 
------------------------------------------------------------------------------------------------------
 iteration      600/  200000 | consumed samples:        19200 | elapsed time per iteration (ms): 266.6 | learning rate: 9.964E-05 | global batch size:    32 | lm loss: 6.531002E-04 | loss scale: 2097152.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.77 | backward-params-all-reduce: 61.88 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.96 | optimizer-unscale-and-check-inf: 17.76 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.16 | batch-generator: 17.53
 iteration      700/  200000 | consumed samples:        22400 | elapsed time per iteration (ms): 253.1 | learning rate: 9.955E-05 | global batch size:    32 | lm loss: 6.531756E-04 | loss scale: 2097152.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.34 | backward-params-all-reduce: 62.79 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.21 | optimizer-unscale-and-check-inf: 13.81 | optimizer-clip-main-grad: 7.22 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 93.37 | batch-generator: 7.78
 iteration      800/  200000 | consumed samples:        25600 | elapsed time per iteration (ms): 253.3 | learning rate: 9.946E-05 | global batch size:    32 | lm loss: 6.483161E-04 | loss scale: 2097152.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.71 | backward-params-all-reduce: 61.63 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.00 | optimizer-unscale-and-check-inf: 18.63 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.13 | batch-generator: 8.03
 iteration      900/  200000 | consumed samples:        28800 | elapsed time per iteration (ms): 256.7 | learning rate: 9.937E-05 | global batch size:    32 | lm loss: 6.528109E-04 | loss scale: 2097152.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.90 | backward-params-all-reduce: 63.08 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.93 | optimizer-unscale-and-check-inf: 16.92 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 13.72 | optimizer: 97.85 | batch-generator: 8.08
 iteration     1000/  200000 | consumed samples:        32000 | elapsed time per iteration (ms): 254.2 | learning rate: 9.928E-05 | global batch size:    32 | lm loss: 6.433538E-04 | loss scale: 2097152.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.77 | backward-params-all-reduce: 63.25 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.15 | optimizer-unscale-and-check-inf: 16.92 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 96.58 | batch-generator: 8.10
------------------------------------------------------------------------------------------------
 validation loss at iteration 1000 | lm loss value: 6.471841E-04 | lm loss PPL: 1.000647E+00 | 
 at iteration 1000, match long value: 0.001305984535178022 | match short value: 0.02493577873348988 
-----------------------------------------------------------------------------------------------------
 iteration     1100/  200000 | consumed samples:        35200 | elapsed time per iteration (ms): 266.7 | learning rate: 9.919E-05 | global batch size:    32 | lm loss: 6.440675E-04 | loss scale: 4194304.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.75 | backward-params-all-reduce: 63.53 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.99 | optimizer-unscale-and-check-inf: 19.13 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.62 | batch-generator: 14.58
 iteration     1200/  200000 | consumed samples:        38400 | elapsed time per iteration (ms): 254.1 | learning rate: 9.910E-05 | global batch size:    32 | lm loss: 6.573156E-04 | loss scale: 4194304.0 | grad norm: 0.012 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.71 | backward-params-all-reduce: 64.09 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 16.83 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 96.15 | batch-generator: 8.01
 iteration     1300/  200000 | consumed samples:        41600 | elapsed time per iteration (ms): 254.4 | learning rate: 9.901E-05 | global batch size:    32 | lm loss: 6.447437E-04 | loss scale: 4194304.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.81 | backward-params-all-reduce: 63.03 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.92 | optimizer-unscale-and-check-inf: 18.98 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 98.36 | batch-generator: 8.00
 iteration     1400/  200000 | consumed samples:        44800 | elapsed time per iteration (ms): 253.7 | learning rate: 9.892E-05 | global batch size:    32 | lm loss: 6.527977E-04 | loss scale: 4194304.0 | grad norm: 0.019 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.72 | backward-params-all-reduce: 62.44 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 18.53 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 97.83 | batch-generator: 8.06
 iteration     1500/  200000 | consumed samples:        48000 | elapsed time per iteration (ms): 254.7 | learning rate: 9.882E-05 | global batch size:    32 | lm loss: 6.486205E-04 | loss scale: 4194304.0 | grad norm: 0.011 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.81 | backward-params-all-reduce: 62.66 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 13.61 | optimizer-unscale-and-check-inf: 16.28 | optimizer-clip-main-grad: 7.52 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.58 | batch-generator: 7.95
------------------------------------------------------------------------------------------------
 validation loss at iteration 1500 | lm loss value: 6.502272E-04 | lm loss PPL: 1.000650E+00 | 
 at iteration 1500, match long value: 0.0013199325367931013 | match short value: 0.017527291528781255 
-------------------------------------------------------------------------------------------------------
 iteration     1600/  200000 | consumed samples:        51200 | elapsed time per iteration (ms): 264.6 | learning rate: 9.873E-05 | global batch size:    32 | lm loss: 6.633323E-04 | loss scale: 4194304.0 | grad norm: 0.012 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.08 | backward-params-all-reduce: 62.47 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.01 | optimizer-unscale-and-check-inf: 18.82 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 98.25 | batch-generator: 14.37
 iteration     1700/  200000 | consumed samples:        54400 | elapsed time per iteration (ms): 258.4 | learning rate: 9.864E-05 | global batch size:    32 | lm loss: 6.533711E-04 | loss scale: 4194304.0 | grad norm: 0.011 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.05 | backward-params-all-reduce: 64.38 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.11 | optimizer-unscale-and-check-inf: 16.87 | optimizer-clip-main-grad: 7.26 | optimizer-copy-main-to-model-params: 13.71 | optimizer: 97.95 | batch-generator: 8.08
 iteration     1800/  200000 | consumed samples:        57600 | elapsed time per iteration (ms): 253.8 | learning rate: 9.855E-05 | global batch size:    32 | lm loss: 6.604908E-04 | loss scale: 4194304.0 | grad norm: 0.015 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.87 | backward-params-all-reduce: 62.72 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.97 | optimizer-unscale-and-check-inf: 18.23 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.14 | optimizer: 97.64 | batch-generator: 8.01
 iteration     1900/  200000 | consumed samples:        60800 | elapsed time per iteration (ms): 259.0 | learning rate: 9.846E-05 | global batch size:    32 | lm loss: 6.657952E-04 | loss scale: 4194304.0 | grad norm: 0.012 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.96 | backward-params-all-reduce: 64.23 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.38 | optimizer-unscale-and-check-inf: 18.84 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 98.67 | batch-generator: 7.99
 iteration     2000/  200000 | consumed samples:        64000 | elapsed time per iteration (ms): 260.7 | learning rate: 9.837E-05 | global batch size:    32 | lm loss: 6.632921E-04 | loss scale: 4194304.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.06 | backward-params-all-reduce: 64.88 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.92 | optimizer-unscale-and-check-inf: 20.00 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 99.41 | batch-generator: 8.01
------------------------------------------------------------------------------------------------
 validation loss at iteration 2000 | lm loss value: 6.489762E-04 | lm loss PPL: 1.000649E+00 | 
 at iteration 2000, match long value: 4.677141232891406e-05 | match short value: -0.0022298297418027204 
---------------------------------------------------------------------------------------------------------
 iteration     2100/  200000 | consumed samples:        67200 | elapsed time per iteration (ms): 274.5 | learning rate: 9.828E-05 | global batch size:    32 | lm loss: 6.739845E-04 | loss scale: 8388608.0 | grad norm: 0.016 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.98 | backward-params-all-reduce: 63.89 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.01 | optimizer-unscale-and-check-inf: 20.13 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.14 | optimizer: 99.62 | batch-generator: 17.76
 iteration     2200/  200000 | consumed samples:        70400 | elapsed time per iteration (ms): 258.3 | learning rate: 9.819E-05 | global batch size:    32 | lm loss: 6.662659E-04 | loss scale: 8388608.0 | grad norm: 0.014 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.84 | backward-params-all-reduce: 65.42 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.90 | optimizer-unscale-and-check-inf: 19.05 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.43 | batch-generator: 8.03
 iteration     2300/  200000 | consumed samples:        73600 | elapsed time per iteration (ms): 257.1 | learning rate: 9.810E-05 | global batch size:    32 | lm loss: 6.718781E-04 | loss scale: 8388608.0 | grad norm: 0.011 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.74 | backward-params-all-reduce: 64.06 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.91 | optimizer-unscale-and-check-inf: 19.22 | optimizer-clip-main-grad: 7.27 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 98.54 | batch-generator: 8.08
 iteration     2400/  200000 | consumed samples:        76800 | elapsed time per iteration (ms): 259.0 | learning rate: 9.801E-05 | global batch size:    32 | lm loss: 6.627337E-04 | loss scale: 8388608.0 | grad norm: 0.022 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.26 | backward-params-all-reduce: 63.07 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.92 | optimizer-unscale-and-check-inf: 18.53 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 13.72 | optimizer: 99.47 | batch-generator: 8.07
 iteration     2500/  200000 | consumed samples:        80000 | elapsed time per iteration (ms): 256.1 | learning rate: 9.791E-05 | global batch size:    32 | lm loss: 6.646355E-04 | loss scale: 8388608.0 | grad norm: 0.012 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.12 | backward-params-all-reduce: 63.93 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.03 | optimizer-unscale-and-check-inf: 18.70 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 98.22 | batch-generator: 8.12
------------------------------------------------------------------------------------------------
 validation loss at iteration 2500 | lm loss value: 6.756119E-04 | lm loss PPL: 1.000676E+00 | 
 at iteration 2500, match long value: 0.0009392717878881689 | match short value: 0.009333466422590043 
-------------------------------------------------------------------------------------------------------
 iteration     2600/  200000 | consumed samples:        83200 | elapsed time per iteration (ms): 269.5 | learning rate: 9.782E-05 | global batch size:    32 | lm loss: 6.516803E-04 | loss scale: 8388608.0 | grad norm: 0.020 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.03 | backward-params-all-reduce: 63.52 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.57 | optimizer-unscale-and-check-inf: 18.51 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.14 | optimizer: 98.52 | batch-generator: 12.17
 iteration     2700/  200000 | consumed samples:        86400 | elapsed time per iteration (ms): 261.2 | learning rate: 9.773E-05 | global batch size:    32 | lm loss: 6.710421E-04 | loss scale: 8388608.0 | grad norm: 0.012 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.20 | backward-params-all-reduce: 63.29 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.14 | optimizer-unscale-and-check-inf: 20.33 | optimizer-clip-main-grad: 7.38 | optimizer-copy-main-to-model-params: 12.14 | optimizer: 99.98 | batch-generator: 7.94
 iteration     2800/  200000 | consumed samples:        89600 | elapsed time per iteration (ms): 255.9 | learning rate: 9.764E-05 | global batch size:    32 | lm loss: 6.740583E-04 | loss scale: 8388608.0 | grad norm: 0.011 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.96 | backward-params-all-reduce: 63.70 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.13 | optimizer-unscale-and-check-inf: 18.37 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.06 | batch-generator: 8.16
 iteration     2900/  200000 | consumed samples:        92800 | elapsed time per iteration (ms): 257.4 | learning rate: 9.755E-05 | global batch size:    32 | lm loss: 6.590704E-04 | loss scale: 8388608.0 | grad norm: 0.012 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.95 | backward-params-all-reduce: 64.23 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.01 | optimizer-unscale-and-check-inf: 19.16 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.14 | optimizer: 98.62 | batch-generator: 7.98
 iteration     3000/  200000 | consumed samples:        96000 | elapsed time per iteration (ms): 257.4 | learning rate: 9.746E-05 | global batch size:    32 | lm loss: 6.700991E-04 | loss scale: 8388608.0 | grad norm: 0.020 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.04 | backward-params-all-reduce: 63.29 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.95 | optimizer-unscale-and-check-inf: 19.26 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.70 | batch-generator: 7.92
------------------------------------------------------------------------------------------------
 validation loss at iteration 3000 | lm loss value: 6.773787E-04 | lm loss PPL: 1.000678E+00 | 
 at iteration 3000, match long value: 0.001046301355478694 | match short value: -0.007681360992047134 
-------------------------------------------------------------------------------------------------------
 iteration     3100/  200000 | consumed samples:        99200 | elapsed time per iteration (ms): 274.1 | learning rate: 9.737E-05 | global batch size:    32 | lm loss: 6.570277E-04 | loss scale: 16777216.0 | grad norm: 0.023 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.56 | backward-params-all-reduce: 63.90 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.02 | optimizer-unscale-and-check-inf: 18.99 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 13.69 | optimizer: 100.01 | batch-generator: 14.73
 iteration     3200/  200000 | consumed samples:       102400 | elapsed time per iteration (ms): 257.9 | learning rate: 9.728E-05 | global batch size:    32 | lm loss: 6.794741E-04 | loss scale: 16777216.0 | grad norm: 0.013 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.35 | backward-params-all-reduce: 64.64 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.92 | optimizer-unscale-and-check-inf: 18.76 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.14 | optimizer: 98.14 | batch-generator: 7.93
 iteration     3300/  200000 | consumed samples:       105600 | elapsed time per iteration (ms): 259.1 | learning rate: 9.719E-05 | global batch size:    32 | lm loss: 6.458659E-04 | loss scale: 16777216.0 | grad norm: 0.012 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.01 | backward-params-all-reduce: 63.59 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.88 | optimizer-unscale-and-check-inf: 19.21 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.14 | optimizer: 98.55 | batch-generator: 7.95
 iteration     3400/  200000 | consumed samples:       108800 | elapsed time per iteration (ms): 257.4 | learning rate: 9.709E-05 | global batch size:    32 | lm loss: 6.754433E-04 | loss scale: 16777216.0 | grad norm: 0.014 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.97 | backward-params-all-reduce: 64.84 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.11 | optimizer-unscale-and-check-inf: 19.78 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.14 | optimizer: 99.37 | batch-generator: 7.95
 iteration     3500/  200000 | consumed samples:       112000 | elapsed time per iteration (ms): 257.8 | learning rate: 9.700E-05 | global batch size:    32 | lm loss: 6.659480E-04 | loss scale: 16777216.0 | grad norm: 0.011 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.09 | backward-params-all-reduce: 64.45 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.07 | optimizer-unscale-and-check-inf: 18.15 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 97.74 | batch-generator: 8.03
------------------------------------------------------------------------------------------------
 validation loss at iteration 3500 | lm loss value: 6.357734E-04 | lm loss PPL: 1.000636E+00 | 
 at iteration 3500, match long value: 0.003147490109555821 | match short value: 0.00855956883722807 
-----------------------------------------------------------------------------------------------------
 iteration     3600/  200000 | consumed samples:       115200 | elapsed time per iteration (ms): 273.4 | learning rate: 9.691E-05 | global batch size:    32 | lm loss: 6.666736E-04 | loss scale: 16777216.0 | grad norm: 0.010 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.21 | backward-params-all-reduce: 62.77 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.88 | optimizer-unscale-and-check-inf: 19.86 | optimizer-clip-main-grad: 7.42 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 99.34 | batch-generator: 21.34
 iteration     3700/  200000 | consumed samples:       118400 | elapsed time per iteration (ms): 256.9 | learning rate: 9.682E-05 | global batch size:    32 | lm loss: 6.585941E-04 | loss scale: 16777216.0 | grad norm: 0.017 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.98 | backward-params-all-reduce: 61.83 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.97 | optimizer-unscale-and-check-inf: 19.68 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 99.19 | batch-generator: 8.17
 iteration     3800/  200000 | consumed samples:       121600 | elapsed time per iteration (ms): 263.0 | learning rate: 9.673E-05 | global batch size:    32 | lm loss: 6.816215E-04 | loss scale: 16777216.0 | grad norm: 0.015 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.22 | backward-params-all-reduce: 63.79 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.28 | optimizer-unscale-and-check-inf: 18.75 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 98.61 | batch-generator: 8.20
 iteration     3900/  200000 | consumed samples:       124800 | elapsed time per iteration (ms): 257.3 | learning rate: 9.664E-05 | global batch size:    32 | lm loss: 6.839024E-04 | loss scale: 16777216.0 | grad norm: 0.010 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.22 | backward-params-all-reduce: 61.66 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.44 | optimizer-unscale-and-check-inf: 17.98 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 13.73 | optimizer: 99.47 | batch-generator: 8.17
 iteration     4000/  200000 | consumed samples:       128000 | elapsed time per iteration (ms): 252.9 | learning rate: 9.655E-05 | global batch size:    32 | lm loss: 6.417631E-04 | loss scale: 16777216.0 | grad norm: 0.022 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.08 | backward-params-all-reduce: 61.69 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.87 | optimizer-unscale-and-check-inf: 15.37 | optimizer-clip-main-grad: 7.27 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 94.71 | batch-generator: 7.92
------------------------------------------------------------------------------------------------
 validation loss at iteration 4000 | lm loss value: 6.799516E-04 | lm loss PPL: 1.000680E+00 | 
 at iteration 4000, match long value: 0.005326727714209906 | match short value: 0.01639648839480421 
-----------------------------------------------------------------------------------------------------
 iteration     4100/  200000 | consumed samples:       131200 | elapsed time per iteration (ms): 270.4 | learning rate: 9.646E-05 | global batch size:    32 | lm loss: 6.542465E-04 | loss scale: 33554432.0 | grad norm: 0.012 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.09 | backward-params-all-reduce: 64.09 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.94 | optimizer-unscale-and-check-inf: 19.59 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 99.07 | batch-generator: 13.38
 iteration     4200/  200000 | consumed samples:       134400 | elapsed time per iteration (ms): 254.7 | learning rate: 9.637E-05 | global batch size:    32 | lm loss: 6.580620E-04 | loss scale: 33554432.0 | grad norm: 0.023 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.33 | backward-params-all-reduce: 62.41 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.58 | optimizer-unscale-and-check-inf: 18.23 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 98.34 | batch-generator: 8.09
 iteration     4300/  200000 | consumed samples:       137600 | elapsed time per iteration (ms): 252.8 | learning rate: 9.627E-05 | global batch size:    32 | lm loss: 6.576505E-04 | loss scale: 33554432.0 | grad norm: 0.015 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.03 | backward-params-all-reduce: 61.83 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.97 | optimizer-unscale-and-check-inf: 18.29 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.78 | batch-generator: 8.13
 iteration     4400/  200000 | consumed samples:       140800 | elapsed time per iteration (ms): 255.6 | learning rate: 9.618E-05 | global batch size:    32 | lm loss: 6.230443E-04 | loss scale: 33554432.0 | grad norm: 0.015 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.99 | backward-params-all-reduce: 62.79 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.96 | optimizer-unscale-and-check-inf: 19.37 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.89 | batch-generator: 7.99
 iteration     4500/  200000 | consumed samples:       144000 | elapsed time per iteration (ms): 262.4 | learning rate: 9.609E-05 | global batch size:    32 | lm loss: 9.362479E-04 | loss scale: 33554432.0 | grad norm: 0.012 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.87 | backward-params-all-reduce: 62.65 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.99 | optimizer-unscale-and-check-inf: 19.32 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.86 | batch-generator: 8.09
------------------------------------------------------------------------------------------------
 validation loss at iteration 4500 | lm loss value: 7.425734E-04 | lm loss PPL: 1.000743E+00 | 
 at iteration 4500, match long value: 0.0021327255922030937 | match short value: 0.012301835104456019 
-------------------------------------------------------------------------------------------------------
 iteration     4600/  200000 | consumed samples:       147200 | elapsed time per iteration (ms): 270.3 | learning rate: 9.600E-05 | global batch size:    32 | lm loss: 6.850941E-04 | loss scale: 33554432.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.11 | backward-params-all-reduce: 62.94 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 17.64 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 13.67 | optimizer: 98.45 | batch-generator: 14.90
 iteration     4700/  200000 | consumed samples:       150400 | elapsed time per iteration (ms): 255.9 | learning rate: 9.591E-05 | global batch size:    32 | lm loss: 6.668676E-04 | loss scale: 33554432.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.88 | backward-params-all-reduce: 63.12 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.94 | optimizer-unscale-and-check-inf: 18.18 | optimizer-clip-main-grad: 7.28 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.59 | batch-generator: 8.06
 iteration     4800/  200000 | consumed samples:       153600 | elapsed time per iteration (ms): 255.6 | learning rate: 9.582E-05 | global batch size:    32 | lm loss: 6.672812E-04 | loss scale: 33554432.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.96 | backward-params-all-reduce: 62.79 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 18.89 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.23 | batch-generator: 8.05
 iteration     4900/  200000 | consumed samples:       156800 | elapsed time per iteration (ms): 256.3 | learning rate: 9.573E-05 | global batch size:    32 | lm loss: 6.636014E-04 | loss scale: 33554432.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.93 | backward-params-all-reduce: 63.65 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 18.24 | optimizer-clip-main-grad: 7.28 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.56 | batch-generator: 8.02
 iteration     5000/  200000 | consumed samples:       160000 | elapsed time per iteration (ms): 254.9 | learning rate: 9.564E-05 | global batch size:    32 | lm loss: 6.549595E-04 | loss scale: 33554432.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.88 | backward-params-all-reduce: 62.95 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 17.34 | optimizer-clip-main-grad: 7.28 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 96.66 | batch-generator: 8.11
------------------------------------------------------------------------------------------------
 validation loss at iteration 5000 | lm loss value: 5.945347E-04 | lm loss PPL: 1.000595E+00 | 
 at iteration 5000, match long value: 0.00627795903462642 | match short value: -0.018503755467173234 
------------------------------------------------------------------------------------------------------
 iteration     5100/  200000 | consumed samples:       163200 | elapsed time per iteration (ms): 267.2 | learning rate: 9.555E-05 | global batch size:    32 | lm loss: 6.576794E-04 | loss scale: 67108864.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.21 | backward-params-all-reduce: 62.96 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 14.86 | optimizer-clip-main-grad: 7.26 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 94.15 | batch-generator: 19.20
 iteration     5200/  200000 | consumed samples:       166400 | elapsed time per iteration (ms): 249.3 | learning rate: 9.545E-05 | global batch size:    32 | lm loss: 6.704904E-04 | loss scale: 67108864.0 | grad norm: 0.014 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.00 | backward-params-all-reduce: 61.53 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 14.22 | optimizer-clip-main-grad: 7.24 | optimizer-copy-main-to-model-params: 12.22 | optimizer: 93.54 | batch-generator: 8.30
 iteration     5300/  200000 | consumed samples:       169600 | elapsed time per iteration (ms): 258.4 | learning rate: 9.536E-05 | global batch size:    32 | lm loss: 6.583305E-04 | loss scale: 67108864.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.17 | backward-params-all-reduce: 62.69 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.60 | optimizer-unscale-and-check-inf: 15.16 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 13.78 | optimizer: 97.11 | batch-generator: 8.37
 iteration     5400/  200000 | consumed samples:       172800 | elapsed time per iteration (ms): 253.7 | learning rate: 9.527E-05 | global batch size:    32 | lm loss: 6.717139E-04 | loss scale: 67108864.0 | grad norm: 0.015 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.97 | backward-params-all-reduce: 61.94 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.55 | optimizer-unscale-and-check-inf: 15.66 | optimizer-clip-main-grad: 7.72 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 96.16 | batch-generator: 8.43
 iteration     5500/  200000 | consumed samples:       176000 | elapsed time per iteration (ms): 254.6 | learning rate: 9.518E-05 | global batch size:    32 | lm loss: 7.205898E-04 | loss scale: 67108864.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.01 | backward-params-all-reduce: 62.42 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.80 | optimizer-unscale-and-check-inf: 15.31 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 12.19 | optimizer: 96.02 | batch-generator: 8.43
------------------------------------------------------------------------------------------------
 validation loss at iteration 5500 | lm loss value: 6.776529E-04 | lm loss PPL: 1.000678E+00 | 
 at iteration 5500, match long value: 0.006297584225754647 | match short value: 0.03624938877968978 
-----------------------------------------------------------------------------------------------------
 iteration     5600/  200000 | consumed samples:       179200 | elapsed time per iteration (ms): 265.7 | learning rate: 9.509E-05 | global batch size:    32 | lm loss: 6.694280E-04 | loss scale: 67108864.0 | grad norm: 0.015 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.04 | backward-params-all-reduce: 63.00 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 17.13 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 96.47 | batch-generator: 14.44
 iteration     5700/  200000 | consumed samples:       182400 | elapsed time per iteration (ms): 257.8 | learning rate: 9.500E-05 | global batch size:    32 | lm loss: 6.564434E-04 | loss scale: 67108864.0 | grad norm: 0.010 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.24 | backward-params-all-reduce: 67.44 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.87 | optimizer-unscale-and-check-inf: 15.56 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 94.91 | batch-generator: 8.19
 iteration     5800/  200000 | consumed samples:       185600 | elapsed time per iteration (ms): 255.1 | learning rate: 9.491E-05 | global batch size:    32 | lm loss: 6.675154E-04 | loss scale: 67108864.0 | grad norm: 0.011 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.08 | backward-params-all-reduce: 63.82 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.01 | optimizer-unscale-and-check-inf: 15.96 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 95.44 | batch-generator: 8.12
 iteration     5900/  200000 | consumed samples:       188800 | elapsed time per iteration (ms): 253.7 | learning rate: 9.482E-05 | global batch size:    32 | lm loss: 6.914335E-04 | loss scale: 67108864.0 | grad norm: 0.011 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.04 | backward-params-all-reduce: 63.42 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.92 | optimizer-unscale-and-check-inf: 15.64 | optimizer-clip-main-grad: 7.25 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 94.98 | batch-generator: 8.05
 iteration     6000/  200000 | consumed samples:       192000 | elapsed time per iteration (ms): 251.8 | learning rate: 9.473E-05 | global batch size:    32 | lm loss: 6.628920E-04 | loss scale: 67108864.0 | grad norm: 0.015 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.18 | backward-params-all-reduce: 63.25 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.00 | optimizer-unscale-and-check-inf: 14.76 | optimizer-clip-main-grad: 7.24 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 94.19 | batch-generator: 8.27
------------------------------------------------------------------------------------------------
 validation loss at iteration 6000 | lm loss value: 6.850993E-04 | lm loss PPL: 1.000685E+00 | 
 at iteration 6000, match long value: 0.0024178760996495475 | match short value: 0.010794997810666873 
-------------------------------------------------------------------------------------------------------
 iteration     6100/  200000 | consumed samples:       195200 | elapsed time per iteration (ms): 271.0 | learning rate: 9.463E-05 | global batch size:    32 | lm loss: 6.649253E-04 | loss scale: 134217728.0 | grad norm: 0.015 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.19 | backward-params-all-reduce: 65.04 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.03 | optimizer-unscale-and-check-inf: 15.14 | optimizer-clip-main-grad: 7.26 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 95.69 | batch-generator: 13.72
 iteration     6200/  200000 | consumed samples:       198400 | elapsed time per iteration (ms): 256.0 | learning rate: 9.454E-05 | global batch size:    32 | lm loss: 6.666468E-04 | loss scale: 134217728.0 | grad norm: 0.021 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.26 | backward-params-all-reduce: 64.55 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.96 | optimizer-unscale-and-check-inf: 16.42 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 95.86 | batch-generator: 8.15
 iteration     6300/  200000 | consumed samples:       201600 | elapsed time per iteration (ms): 259.7 | learning rate: 9.445E-05 | global batch size:    32 | lm loss: 6.685780E-04 | loss scale: 134217728.0 | grad norm: 0.013 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.21 | backward-params-all-reduce: 66.04 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.30 | optimizer-unscale-and-check-inf: 16.35 | optimizer-clip-main-grad: 7.27 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 96.10 | batch-generator: 8.07
 iteration     6400/  200000 | consumed samples:       204800 | elapsed time per iteration (ms): 254.1 | learning rate: 9.436E-05 | global batch size:    32 | lm loss: 6.554129E-04 | loss scale: 134217728.0 | grad norm: 0.017 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.11 | backward-params-all-reduce: 64.08 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.96 | optimizer-unscale-and-check-inf: 16.30 | optimizer-clip-main-grad: 7.28 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 95.72 | batch-generator: 8.05
 iteration     6500/  200000 | consumed samples:       208000 | elapsed time per iteration (ms): 249.3 | learning rate: 9.427E-05 | global batch size:    32 | lm loss: 6.379052E-04 | loss scale: 67108864.0 | grad norm: 0.021 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.06 | backward-params-all-reduce: 63.29 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.92 | optimizer-unscale-and-check-inf: 15.49 | optimizer-clip-main-grad: 7.11 | optimizer-copy-main-to-model-params: 11.92 | optimizer: 93.49 | batch-generator: 8.23
------------------------------------------------------------------------------------------------
 validation loss at iteration 6500 | lm loss value: 6.743786E-04 | lm loss PPL: 1.000675E+00 | 
 at iteration 6500, match long value: 0.005602140517465603 | match short value: 0.03013577808505266 
-----------------------------------------------------------------------------------------------------
 iteration     6600/  200000 | consumed samples:       211200 | elapsed time per iteration (ms): 266.5 | learning rate: 9.418E-05 | global batch size:    32 | lm loss: 6.199125E-04 | loss scale: 67108864.0 | grad norm: 0.015 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.33 | backward-params-all-reduce: 63.06 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.04 | optimizer-unscale-and-check-inf: 15.28 | optimizer-clip-main-grad: 7.25 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 94.77 | batch-generator: 17.83
 iteration     6700/  200000 | consumed samples:       214400 | elapsed time per iteration (ms): 253.1 | learning rate: 9.409E-05 | global batch size:    32 | lm loss: 6.038041E-04 | loss scale: 67108864.0 | grad norm: 0.016 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.03 | backward-params-all-reduce: 63.22 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.87 | optimizer-unscale-and-check-inf: 15.49 | optimizer-clip-main-grad: 7.27 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 94.83 | batch-generator: 8.27
 iteration     6800/  200000 | consumed samples:       217600 | elapsed time per iteration (ms): 264.5 | learning rate: 9.400E-05 | global batch size:    32 | lm loss: 6.062541E-04 | loss scale: 67108864.0 | grad norm: 0.015 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.67 | backward-params-all-reduce: 62.78 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 16.70 | optimizer-clip-main-grad: 7.27 | optimizer-copy-main-to-model-params: 13.57 | optimizer: 97.41 | batch-generator: 8.17
 iteration     6900/  200000 | consumed samples:       220800 | elapsed time per iteration (ms): 251.5 | learning rate: 9.391E-05 | global batch size:    32 | lm loss: 6.083208E-04 | loss scale: 67108864.0 | grad norm: 0.012 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.08 | backward-params-all-reduce: 62.59 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.92 | optimizer-unscale-and-check-inf: 15.20 | optimizer-clip-main-grad: 7.27 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 94.58 | batch-generator: 8.35
 iteration     7000/  200000 | consumed samples:       224000 | elapsed time per iteration (ms): 255.6 | learning rate: 9.382E-05 | global batch size:    32 | lm loss: 5.684185E-04 | loss scale: 67108864.0 | grad norm: 0.020 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.08 | backward-params-all-reduce: 63.64 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.92 | optimizer-unscale-and-check-inf: 15.99 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 95.39 | batch-generator: 8.20
------------------------------------------------------------------------------------------------
 validation loss at iteration 7000 | lm loss value: 5.827330E-04 | lm loss PPL: 1.000583E+00 | 
 at iteration 7000, match long value: 0.0018467114605622524 | match short value: -0.04635529469033603 
-------------------------------------------------------------------------------------------------------
 iteration     7100/  200000 | consumed samples:       227200 | elapsed time per iteration (ms): 270.0 | learning rate: 9.373E-05 | global batch size:    32 | lm loss: 5.564332E-04 | loss scale: 67108864.0 | grad norm: 0.020 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.15 | backward-params-all-reduce: 62.31 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.54 | optimizer-unscale-and-check-inf: 18.72 | optimizer-clip-main-grad: 7.39 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.85 | batch-generator: 18.64
 iteration     7200/  200000 | consumed samples:       230400 | elapsed time per iteration (ms): 253.8 | learning rate: 9.363E-05 | global batch size:    32 | lm loss: 5.029616E-04 | loss scale: 67108864.0 | grad norm: 0.015 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.18 | backward-params-all-reduce: 62.33 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.99 | optimizer-unscale-and-check-inf: 18.86 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 98.36 | batch-generator: 8.23
 iteration     7300/  200000 | consumed samples:       233600 | elapsed time per iteration (ms): 253.9 | learning rate: 9.354E-05 | global batch size:    32 | lm loss: 4.911084E-04 | loss scale: 67108864.0 | grad norm: 0.018 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.15 | backward-params-all-reduce: 62.89 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.58 | optimizer-unscale-and-check-inf: 18.15 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.24 | batch-generator: 8.15
 iteration     7400/  200000 | consumed samples:       236800 | elapsed time per iteration (ms): 253.3 | learning rate: 9.345E-05 | global batch size:    32 | lm loss: 3.637996E-04 | loss scale: 67108864.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.06 | backward-params-all-reduce: 62.64 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.48 | optimizer-unscale-and-check-inf: 17.80 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.76 | batch-generator: 8.07
 iteration     7500/  200000 | consumed samples:       240000 | elapsed time per iteration (ms): 263.7 | learning rate: 9.336E-05 | global batch size:    32 | lm loss: 3.247115E-04 | loss scale: 134217728.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.40 | backward-params-all-reduce: 64.37 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.04 | optimizer-unscale-and-check-inf: 19.29 | optimizer-clip-main-grad: 7.39 | optimizer-copy-main-to-model-params: 13.58 | optimizer: 100.32 | batch-generator: 8.00
------------------------------------------------------------------------------------------------
 validation loss at iteration 7500 | lm loss value: 2.992494E-04 | lm loss PPL: 1.000299E+00 | 
 at iteration 7500, match long value: 0.013486362379114086 | match short value: -0.015062045082788867 
-------------------------------------------------------------------------------------------------------
 iteration     7600/  200000 | consumed samples:       243200 | elapsed time per iteration (ms): 271.8 | learning rate: 9.327E-05 | global batch size:    32 | lm loss: 2.859428E-04 | loss scale: 134217728.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.24 | backward-params-all-reduce: 64.48 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.25 | optimizer-unscale-and-check-inf: 19.28 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 99.01 | batch-generator: 13.57
 iteration     7700/  200000 | consumed samples:       246400 | elapsed time per iteration (ms): 259.5 | learning rate: 9.318E-05 | global batch size:    32 | lm loss: 2.435664E-04 | loss scale: 134217728.0 | grad norm: 0.018 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.16 | backward-params-all-reduce: 63.64 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 14.83 | optimizer-unscale-and-check-inf: 13.84 | optimizer-clip-main-grad: 7.21 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 96.08 | batch-generator: 7.84
 iteration     7800/  200000 | consumed samples:       249600 | elapsed time per iteration (ms): 254.3 | learning rate: 9.309E-05 | global batch size:    32 | lm loss: 2.313434E-04 | loss scale: 134217728.0 | grad norm: 0.010 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.15 | backward-params-all-reduce: 63.06 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 13.40 | optimizer-unscale-and-check-inf: 16.78 | optimizer-clip-main-grad: 7.26 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.61 | batch-generator: 7.98
 iteration     7900/  200000 | consumed samples:       252800 | elapsed time per iteration (ms): 255.8 | learning rate: 9.300E-05 | global batch size:    32 | lm loss: 2.083290E-04 | loss scale: 134217728.0 | grad norm: 0.010 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.22 | backward-params-all-reduce: 63.84 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.89 | optimizer-unscale-and-check-inf: 19.09 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.47 | batch-generator: 8.01
 iteration     8000/  200000 | consumed samples:       256000 | elapsed time per iteration (ms): 255.0 | learning rate: 9.291E-05 | global batch size:    32 | lm loss: 1.941747E-04 | loss scale: 134217728.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.11 | backward-params-all-reduce: 64.12 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 18.83 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.15 | batch-generator: 7.99
------------------------------------------------------------------------------------------------
 validation loss at iteration 8000 | lm loss value: 1.952711E-04 | lm loss PPL: 1.000195E+00 | 
 at iteration 8000, match long value: 0.006769473088153232 | match short value: -0.02730216666806583 
------------------------------------------------------------------------------------------------------
 iteration     8100/  200000 | consumed samples:       259200 | elapsed time per iteration (ms): 269.8 | learning rate: 9.281E-05 | global batch size:    32 | lm loss: 1.974218E-04 | loss scale: 134217728.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.23 | backward-params-all-reduce: 63.02 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.00 | optimizer-unscale-and-check-inf: 17.95 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.47 | batch-generator: 18.72
 iteration     8200/  200000 | consumed samples:       262400 | elapsed time per iteration (ms): 257.0 | learning rate: 9.272E-05 | global batch size:    32 | lm loss: 1.809771E-04 | loss scale: 134217728.0 | grad norm: 0.009 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.99 | backward-params-all-reduce: 63.08 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.03 | optimizer-unscale-and-check-inf: 16.92 | optimizer-clip-main-grad: 7.21 | optimizer-copy-main-to-model-params: 13.53 | optimizer: 97.22 | batch-generator: 8.02
 iteration     8300/  200000 | consumed samples:       265600 | elapsed time per iteration (ms): 256.4 | learning rate: 9.263E-05 | global batch size:    32 | lm loss: 1.720501E-04 | loss scale: 67108864.0 | grad norm: 0.007 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.23 | backward-params-all-reduce: 62.62 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.96 | optimizer-unscale-and-check-inf: 18.07 | optimizer-clip-main-grad: 7.24 | optimizer-copy-main-to-model-params: 12.04 | optimizer: 96.83 | batch-generator: 8.05
 iteration     8400/  200000 | consumed samples:       268800 | elapsed time per iteration (ms): 253.6 | learning rate: 9.254E-05 | global batch size:    32 | lm loss: 1.522172E-04 | loss scale: 67108864.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.31 | backward-params-all-reduce: 62.61 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 17.68 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 97.02 | batch-generator: 8.09
 iteration     8500/  200000 | consumed samples:       272000 | elapsed time per iteration (ms): 254.7 | learning rate: 9.245E-05 | global batch size:    32 | lm loss: 1.474098E-04 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.30 | backward-params-all-reduce: 62.53 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 17.80 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.14 | batch-generator: 8.05
------------------------------------------------------------------------------------------------
 validation loss at iteration 8500 | lm loss value: 1.309695E-04 | lm loss PPL: 1.000131E+00 | 
 at iteration 8500, match long value: 0.0058164422793951995 | match short value: -0.04718268702171878 
-------------------------------------------------------------------------------------------------------
 iteration     8600/  200000 | consumed samples:       275200 | elapsed time per iteration (ms): 264.1 | learning rate: 9.236E-05 | global batch size:    32 | lm loss: 1.429363E-04 | loss scale: 67108864.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.31 | backward-params-all-reduce: 63.50 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.00 | optimizer-unscale-and-check-inf: 17.04 | optimizer-clip-main-grad: 7.28 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 96.48 | batch-generator: 14.38
 iteration     8700/  200000 | consumed samples:       278400 | elapsed time per iteration (ms): 258.0 | learning rate: 9.227E-05 | global batch size:    32 | lm loss: 1.324781E-04 | loss scale: 67108864.0 | grad norm: 0.010 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 54.32 | backward-params-all-reduce: 61.63 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.61 | optimizer-unscale-and-check-inf: 17.25 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.34 | batch-generator: 7.92
 iteration     8800/  200000 | consumed samples:       281600 | elapsed time per iteration (ms): 264.2 | learning rate: 9.218E-05 | global batch size:    32 | lm loss: 1.364638E-04 | loss scale: 67108864.0 | grad norm: 0.012 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 55.53 | backward-params-all-reduce: 60.85 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 13.04 | optimizer-unscale-and-check-inf: 16.73 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 97.24 | batch-generator: 7.92
 iteration     8900/  200000 | consumed samples:       284800 | elapsed time per iteration (ms): 255.7 | learning rate: 9.209E-05 | global batch size:    32 | lm loss: 1.339422E-04 | loss scale: 67108864.0 | grad norm: 0.016 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 55.12 | backward-params-all-reduce: 59.70 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.35 | optimizer-unscale-and-check-inf: 16.71 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 96.56 | batch-generator: 7.99
 iteration     9000/  200000 | consumed samples:       288000 | elapsed time per iteration (ms): 260.8 | learning rate: 9.200E-05 | global batch size:    32 | lm loss: 1.307620E-04 | loss scale: 67108864.0 | grad norm: 0.011 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.53 | backward-params-all-reduce: 62.89 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.92 | optimizer-unscale-and-check-inf: 16.13 | optimizer-clip-main-grad: 7.25 | optimizer-copy-main-to-model-params: 13.60 | optimizer: 96.88 | batch-generator: 8.03
------------------------------------------------------------------------------------------------
 validation loss at iteration 9000 | lm loss value: 1.227051E-04 | lm loss PPL: 1.000123E+00 | 
 at iteration 9000, match long value: 0.0020662965621656656 | match short value: -0.03364817865173323 
-------------------------------------------------------------------------------------------------------
 iteration     9100/  200000 | consumed samples:       291200 | elapsed time per iteration (ms): 265.4 | learning rate: 9.191E-05 | global batch size:    32 | lm loss: 1.301183E-04 | loss scale: 67108864.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.04 | backward-params-all-reduce: 63.35 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.87 | optimizer-unscale-and-check-inf: 16.55 | optimizer-clip-main-grad: 7.26 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 95.83 | batch-generator: 14.27
 iteration     9200/  200000 | consumed samples:       294400 | elapsed time per iteration (ms): 256.1 | learning rate: 9.181E-05 | global batch size:    32 | lm loss: 1.142497E-04 | loss scale: 67108864.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.08 | backward-params-all-reduce: 63.52 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 17.38 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 96.68 | batch-generator: 7.87
 iteration     9300/  200000 | consumed samples:       297600 | elapsed time per iteration (ms): 253.7 | learning rate: 9.172E-05 | global batch size:    32 | lm loss: 1.120337E-04 | loss scale: 134217728.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.12 | backward-params-all-reduce: 62.61 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.18 | optimizer-unscale-and-check-inf: 15.93 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 95.58 | batch-generator: 7.95
 iteration     9400/  200000 | consumed samples:       300800 | elapsed time per iteration (ms): 254.2 | learning rate: 9.163E-05 | global batch size:    32 | lm loss: 1.204555E-04 | loss scale: 134217728.0 | grad norm: 0.012 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.37 | backward-params-all-reduce: 63.24 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.87 | optimizer-unscale-and-check-inf: 16.01 | optimizer-clip-main-grad: 7.24 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 95.28 | batch-generator: 7.94
 iteration     9500/  200000 | consumed samples:       304000 | elapsed time per iteration (ms): 254.0 | learning rate: 9.154E-05 | global batch size:    32 | lm loss: 1.187519E-04 | loss scale: 134217728.0 | grad norm: 0.010 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.35 | backward-params-all-reduce: 64.14 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.87 | optimizer-unscale-and-check-inf: 15.34 | optimizer-clip-main-grad: 7.26 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 94.65 | batch-generator: 8.03
------------------------------------------------------------------------------------------------
 validation loss at iteration 9500 | lm loss value: 1.094406E-04 | lm loss PPL: 1.000109E+00 | 
 at iteration 9500, match long value: 0.007256680471798105 | match short value: -0.008399359040954783 
-------------------------------------------------------------------------------------------------------
 iteration     9600/  200000 | consumed samples:       307200 | elapsed time per iteration (ms): 266.7 | learning rate: 9.145E-05 | global batch size:    32 | lm loss: 1.095530E-04 | loss scale: 134217728.0 | grad norm: 0.010 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.36 | backward-params-all-reduce: 62.39 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.97 | optimizer-unscale-and-check-inf: 15.70 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 95.16 | batch-generator: 18.04
 iteration     9700/  200000 | consumed samples:       310400 | elapsed time per iteration (ms): 253.7 | learning rate: 9.136E-05 | global batch size:    32 | lm loss: 1.011459E-04 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.54 | backward-params-all-reduce: 60.37 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.92 | optimizer-unscale-and-check-inf: 14.72 | optimizer-clip-main-grad: 7.27 | optimizer-copy-main-to-model-params: 13.69 | optimizer: 95.60 | batch-generator: 8.32
 iteration     9800/  200000 | consumed samples:       313600 | elapsed time per iteration (ms): 250.4 | learning rate: 9.127E-05 | global batch size:    32 | lm loss: 1.049916E-04 | loss scale: 134217728.0 | grad norm: 0.009 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.12 | backward-params-all-reduce: 62.09 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 14.10 | optimizer-clip-main-grad: 7.16 | optimizer-copy-main-to-model-params: 12.04 | optimizer: 92.68 | batch-generator: 8.02
 iteration     9900/  200000 | consumed samples:       316800 | elapsed time per iteration (ms): 252.2 | learning rate: 9.118E-05 | global batch size:    32 | lm loss: 1.144618E-04 | loss scale: 134217728.0 | grad norm: 0.013 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.71 | backward-params-all-reduce: 63.07 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 15.04 | optimizer-clip-main-grad: 7.24 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 94.30 | batch-generator: 8.08
 iteration    10000/  200000 | consumed samples:       320000 | elapsed time per iteration (ms): 252.4 | learning rate: 9.109E-05 | global batch size:    32 | lm loss: 1.202325E-04 | loss scale: 134217728.0 | grad norm: 0.011 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.37 | backward-params-all-reduce: 63.66 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.88 | optimizer-unscale-and-check-inf: 15.18 | optimizer-clip-main-grad: 7.25 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 94.48 | batch-generator: 8.02
-------------------------------------------------------------------------------------------------
 validation loss at iteration 10000 | lm loss value: 1.018302E-04 | lm loss PPL: 1.000102E+00 | 
 at iteration 10000, match long value: 0.005996477185757027 | match short value: 0.015246487524455154 
-------------------------------------------------------------------------------------------------------
 iteration    10100/  200000 | consumed samples:       323200 | elapsed time per iteration (ms): 265.2 | learning rate: 9.100E-05 | global batch size:    32 | lm loss: 9.871851E-05 | loss scale: 134217728.0 | grad norm: 0.014 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.15 | backward-params-all-reduce: 63.52 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.93 | optimizer-unscale-and-check-inf: 16.03 | optimizer-clip-main-grad: 7.26 | optimizer-copy-main-to-model-params: 12.21 | optimizer: 95.43 | batch-generator: 15.78
 iteration    10200/  200000 | consumed samples:       326400 | elapsed time per iteration (ms): 259.5 | learning rate: 9.090E-05 | global batch size:    32 | lm loss: 1.041510E-04 | loss scale: 134217728.0 | grad norm: 0.011 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.09 | backward-params-all-reduce: 63.77 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.05 | optimizer-unscale-and-check-inf: 17.22 | optimizer-clip-main-grad: 7.27 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 96.71 | batch-generator: 8.01
 iteration    10300/  200000 | consumed samples:       329600 | elapsed time per iteration (ms): 255.1 | learning rate: 9.081E-05 | global batch size:    32 | lm loss: 1.134543E-04 | loss scale: 134217728.0 | grad norm: 0.012 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.99 | backward-params-all-reduce: 62.72 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 17.22 | optimizer-clip-main-grad: 7.28 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 96.49 | batch-generator: 8.01
 iteration    10400/  200000 | consumed samples:       332800 | elapsed time per iteration (ms): 260.4 | learning rate: 9.072E-05 | global batch size:    32 | lm loss: 9.927821E-05 | loss scale: 134217728.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.78 | backward-params-all-reduce: 62.52 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.91 | optimizer-unscale-and-check-inf: 16.36 | optimizer-clip-main-grad: 7.26 | optimizer-copy-main-to-model-params: 13.71 | optimizer: 97.23 | batch-generator: 8.03
 iteration    10500/  200000 | consumed samples:       336000 | elapsed time per iteration (ms): 251.9 | learning rate: 9.063E-05 | global batch size:    32 | lm loss: 9.240475E-05 | loss scale: 134217728.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.12 | backward-params-all-reduce: 63.08 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.07 | optimizer-unscale-and-check-inf: 15.57 | optimizer-clip-main-grad: 7.26 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 95.06 | batch-generator: 8.16
-------------------------------------------------------------------------------------------------
 validation loss at iteration 10500 | lm loss value: 8.892057E-05 | lm loss PPL: 1.000089E+00 | 
 at iteration 10500, match long value: 0.012628706263515144 | match short value: 0.022391853773364192 
-------------------------------------------------------------------------------------------------------
 iteration    10600/  200000 | consumed samples:       339200 | elapsed time per iteration (ms): 274.2 | learning rate: 9.054E-05 | global batch size:    32 | lm loss: 9.571443E-05 | loss scale: 134217728.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.37 | backward-params-all-reduce: 62.74 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.91 | optimizer-unscale-and-check-inf: 20.12 | optimizer-clip-main-grad: 7.37 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 99.57 | batch-generator: 18.91
 iteration    10700/  200000 | consumed samples:       342400 | elapsed time per iteration (ms): 257.5 | learning rate: 9.045E-05 | global batch size:    32 | lm loss: 9.286555E-05 | loss scale: 134217728.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.29 | backward-params-all-reduce: 61.99 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.17 | optimizer-unscale-and-check-inf: 19.21 | optimizer-clip-main-grad: 7.37 | optimizer-copy-main-to-model-params: 12.21 | optimizer: 98.96 | batch-generator: 8.29
 iteration    10800/  200000 | consumed samples:       345600 | elapsed time per iteration (ms): 252.9 | learning rate: 9.036E-05 | global batch size:    32 | lm loss: 1.056184E-04 | loss scale: 67108864.0 | grad norm: 0.007 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.24 | backward-params-all-reduce: 61.18 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.04 | optimizer-unscale-and-check-inf: 19.78 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.08 | optimizer: 98.75 | batch-generator: 8.42
 iteration    10900/  200000 | consumed samples:       348800 | elapsed time per iteration (ms): 253.2 | learning rate: 9.027E-05 | global batch size:    32 | lm loss: 8.993842E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.18 | backward-params-all-reduce: 61.59 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.05 | optimizer-unscale-and-check-inf: 19.03 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 98.64 | batch-generator: 8.33
 iteration    11000/  200000 | consumed samples:       352000 | elapsed time per iteration (ms): 255.1 | learning rate: 9.018E-05 | global batch size:    32 | lm loss: 9.151815E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.10 | backward-params-all-reduce: 62.00 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.90 | optimizer-unscale-and-check-inf: 19.78 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 99.19 | batch-generator: 8.11
-------------------------------------------------------------------------------------------------
 validation loss at iteration 11000 | lm loss value: 9.594142E-05 | lm loss PPL: 1.000096E+00 | 
 at iteration 11000, match long value: -0.0023703957996087342 | match short value: -0.04734827961062023 
---------------------------------------------------------------------------------------------------------
 iteration    11100/  200000 | consumed samples:       355200 | elapsed time per iteration (ms): 270.2 | learning rate: 9.009E-05 | global batch size:    32 | lm loss: 8.987840E-05 | loss scale: 67108864.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.28 | backward-params-all-reduce: 63.25 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.95 | optimizer-unscale-and-check-inf: 19.14 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.58 | batch-generator: 17.07
 iteration    11200/  200000 | consumed samples:       358400 | elapsed time per iteration (ms): 261.9 | learning rate: 8.999E-05 | global batch size:    32 | lm loss: 1.034726E-04 | loss scale: 67108864.0 | grad norm: 0.011 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.57 | backward-params-all-reduce: 62.61 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.88 | optimizer-unscale-and-check-inf: 17.96 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 13.63 | optimizer: 98.80 | batch-generator: 8.05
 iteration    11300/  200000 | consumed samples:       361600 | elapsed time per iteration (ms): 254.9 | learning rate: 8.990E-05 | global batch size:    32 | lm loss: 9.026983E-05 | loss scale: 67108864.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.06 | backward-params-all-reduce: 63.33 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 17.40 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 96.78 | batch-generator: 8.09
 iteration    11400/  200000 | consumed samples:       364800 | elapsed time per iteration (ms): 254.1 | learning rate: 8.981E-05 | global batch size:    32 | lm loss: 8.876670E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.03 | backward-params-all-reduce: 62.14 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 18.17 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 97.53 | batch-generator: 8.04
 iteration    11500/  200000 | consumed samples:       368000 | elapsed time per iteration (ms): 253.7 | learning rate: 8.972E-05 | global batch size:    32 | lm loss: 8.447748E-05 | loss scale: 67108864.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.18 | backward-params-all-reduce: 62.82 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 18.42 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.20 | optimizer: 97.83 | batch-generator: 8.19
-------------------------------------------------------------------------------------------------
 validation loss at iteration 11500 | lm loss value: 8.748881E-05 | lm loss PPL: 1.000087E+00 | 
 at iteration 11500, match long value: 0.006615929266951882 | match short value: -0.0032349974539730435 
---------------------------------------------------------------------------------------------------------
 iteration    11600/  200000 | consumed samples:       371200 | elapsed time per iteration (ms): 265.0 | learning rate: 8.963E-05 | global batch size:    32 | lm loss: 8.218158E-05 | loss scale: 67108864.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.48 | backward-params-all-reduce: 63.31 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.96 | optimizer-unscale-and-check-inf: 18.18 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.64 | batch-generator: 14.55
 iteration    11700/  200000 | consumed samples:       374400 | elapsed time per iteration (ms): 251.9 | learning rate: 8.954E-05 | global batch size:    32 | lm loss: 8.542181E-05 | loss scale: 33554432.0 | grad norm: 0.009 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.05 | backward-params-all-reduce: 61.62 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.96 | optimizer-unscale-and-check-inf: 17.89 | optimizer-clip-main-grad: 7.23 | optimizer-copy-main-to-model-params: 12.06 | optimizer: 96.67 | batch-generator: 8.11
 iteration    11800/  200000 | consumed samples:       377600 | elapsed time per iteration (ms): 252.5 | learning rate: 8.945E-05 | global batch size:    32 | lm loss: 8.541317E-05 | loss scale: 33554432.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.32 | backward-params-all-reduce: 62.83 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 16.80 | optimizer-clip-main-grad: 7.28 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 96.12 | batch-generator: 8.07
 iteration    11900/  200000 | consumed samples:       380800 | elapsed time per iteration (ms): 263.2 | learning rate: 8.936E-05 | global batch size:    32 | lm loss: 8.118355E-05 | loss scale: 33554432.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.31 | backward-params-all-reduce: 63.34 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.16 | optimizer-unscale-and-check-inf: 18.12 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 13.65 | optimizer: 99.26 | batch-generator: 8.04
 iteration    12000/  200000 | consumed samples:       384000 | elapsed time per iteration (ms): 253.5 | learning rate: 8.927E-05 | global batch size:    32 | lm loss: 8.654658E-05 | loss scale: 33554432.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.01 | backward-params-all-reduce: 62.42 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.92 | optimizer-unscale-and-check-inf: 17.24 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 96.65 | batch-generator: 8.22
-------------------------------------------------------------------------------------------------
 validation loss at iteration 12000 | lm loss value: 8.559191E-05 | lm loss PPL: 1.000086E+00 | 
 at iteration 12000, match long value: 0.009072494128957515 | match short value: 0.02152848250422629 
------------------------------------------------------------------------------------------------------
 iteration    12100/  200000 | consumed samples:       387200 | elapsed time per iteration (ms): 266.7 | learning rate: 8.918E-05 | global batch size:    32 | lm loss: 8.903409E-05 | loss scale: 33554432.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.18 | backward-params-all-reduce: 63.55 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.96 | optimizer-unscale-and-check-inf: 18.29 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.71 | batch-generator: 13.68
 iteration    12200/  200000 | consumed samples:       390400 | elapsed time per iteration (ms): 253.7 | learning rate: 8.908E-05 | global batch size:    32 | lm loss: 8.642395E-05 | loss scale: 33554432.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.35 | backward-params-all-reduce: 62.12 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 17.79 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.13 | batch-generator: 7.92
 iteration    12300/  200000 | consumed samples:       393600 | elapsed time per iteration (ms): 254.8 | learning rate: 8.899E-05 | global batch size:    32 | lm loss: 8.255916E-05 | loss scale: 33554432.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.44 | backward-params-all-reduce: 62.70 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 16.95 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 96.28 | batch-generator: 8.11
 iteration    12400/  200000 | consumed samples:       396800 | elapsed time per iteration (ms): 254.9 | learning rate: 8.890E-05 | global batch size:    32 | lm loss: 7.565868E-05 | loss scale: 33554432.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.30 | backward-params-all-reduce: 62.70 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.96 | optimizer-unscale-and-check-inf: 17.33 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 96.80 | batch-generator: 8.03
 iteration    12500/  200000 | consumed samples:       400000 | elapsed time per iteration (ms): 254.1 | learning rate: 8.881E-05 | global batch size:    32 | lm loss: 8.689513E-05 | loss scale: 33554432.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.13 | backward-params-all-reduce: 62.23 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 17.88 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.20 | batch-generator: 7.93
-------------------------------------------------------------------------------------------------
 validation loss at iteration 12500 | lm loss value: 8.127779E-05 | lm loss PPL: 1.000081E+00 | 
 at iteration 12500, match long value: 0.004657965890719699 | match short value: -0.008475144116243323 
--------------------------------------------------------------------------------------------------------
 iteration    12600/  200000 | consumed samples:       403200 | elapsed time per iteration (ms): 275.9 | learning rate: 8.872E-05 | global batch size:    32 | lm loss: 8.464648E-05 | loss scale: 33554432.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.37 | backward-params-all-reduce: 63.65 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 17.20 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 13.64 | optimizer: 98.03 | batch-generator: 17.03
 iteration    12700/  200000 | consumed samples:       406400 | elapsed time per iteration (ms): 254.1 | learning rate: 8.863E-05 | global batch size:    32 | lm loss: 7.828865E-05 | loss scale: 67108864.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.07 | backward-params-all-reduce: 63.05 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.00 | optimizer-unscale-and-check-inf: 17.11 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 96.62 | batch-generator: 8.05
 iteration    12800/  200000 | consumed samples:       409600 | elapsed time per iteration (ms): 261.7 | learning rate: 8.854E-05 | global batch size:    32 | lm loss: 8.274710E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.10 | backward-params-all-reduce: 61.92 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 19.49 | optimizer-clip-main-grad: 7.37 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.88 | batch-generator: 8.03
 iteration    12900/  200000 | consumed samples:       412800 | elapsed time per iteration (ms): 257.5 | learning rate: 8.845E-05 | global batch size:    32 | lm loss: 7.419335E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.99 | backward-params-all-reduce: 62.66 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.90 | optimizer-unscale-and-check-inf: 17.82 | optimizer-clip-main-grad: 7.28 | optimizer-copy-main-to-model-params: 12.21 | optimizer: 97.21 | batch-generator: 8.15
 iteration    13000/  200000 | consumed samples:       416000 | elapsed time per iteration (ms): 252.3 | learning rate: 8.836E-05 | global batch size:    32 | lm loss: 7.658334E-05 | loss scale: 67108864.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.05 | backward-params-all-reduce: 62.09 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.91 | optimizer-unscale-and-check-inf: 16.53 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.21 | optimizer: 95.98 | batch-generator: 7.97
-------------------------------------------------------------------------------------------------
 validation loss at iteration 13000 | lm loss value: 7.524450E-05 | lm loss PPL: 1.000075E+00 | 
 at iteration 13000, match long value: 0.004617617474645407 | match short value: -0.03287058280601329 
-------------------------------------------------------------------------------------------------------
 iteration    13100/  200000 | consumed samples:       419200 | elapsed time per iteration (ms): 266.5 | learning rate: 8.826E-05 | global batch size:    32 | lm loss: 7.449465E-05 | loss scale: 67108864.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.08 | backward-params-all-reduce: 63.83 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.10 | optimizer-unscale-and-check-inf: 18.32 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 97.92 | batch-generator: 14.19
 iteration    13200/  200000 | consumed samples:       422400 | elapsed time per iteration (ms): 251.7 | learning rate: 8.817E-05 | global batch size:    32 | lm loss: 8.570069E-05 | loss scale: 67108864.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.93 | backward-params-all-reduce: 62.72 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.01 | optimizer-unscale-and-check-inf: 15.76 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 95.27 | batch-generator: 8.17
 iteration    13300/  200000 | consumed samples:       425600 | elapsed time per iteration (ms): 254.6 | learning rate: 8.808E-05 | global batch size:    32 | lm loss: 7.562949E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.10 | backward-params-all-reduce: 63.70 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 16.68 | optimizer-clip-main-grad: 7.28 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 96.00 | batch-generator: 8.02
 iteration    13400/  200000 | consumed samples:       428800 | elapsed time per iteration (ms): 261.7 | learning rate: 8.799E-05 | global batch size:    32 | lm loss: 8.011111E-05 | loss scale: 67108864.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.50 | backward-params-all-reduce: 63.50 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.91 | optimizer-unscale-and-check-inf: 16.55 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 13.64 | optimizer: 97.42 | batch-generator: 8.04
 iteration    13500/  200000 | consumed samples:       432000 | elapsed time per iteration (ms): 253.6 | learning rate: 8.790E-05 | global batch size:    32 | lm loss: 7.830413E-05 | loss scale: 67108864.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.41 | backward-params-all-reduce: 62.14 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 17.73 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 97.13 | batch-generator: 8.04
-------------------------------------------------------------------------------------------------
 validation loss at iteration 13500 | lm loss value: 7.904798E-05 | lm loss PPL: 1.000079E+00 | 
 at iteration 13500, match long value: 0.006476417360652864 | match short value: 0.00860225413580336 
------------------------------------------------------------------------------------------------------
 iteration    13600/  200000 | consumed samples:       435200 | elapsed time per iteration (ms): 263.9 | learning rate: 8.781E-05 | global batch size:    32 | lm loss: 8.892001E-05 | loss scale: 67108864.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.07 | backward-params-all-reduce: 62.47 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 16.91 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 96.24 | batch-generator: 14.28
 iteration    13700/  200000 | consumed samples:       438400 | elapsed time per iteration (ms): 254.7 | learning rate: 8.772E-05 | global batch size:    32 | lm loss: 7.455702E-05 | loss scale: 134217728.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.52 | backward-params-all-reduce: 63.27 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.87 | optimizer-unscale-and-check-inf: 17.25 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 96.64 | batch-generator: 7.92
 iteration    13800/  200000 | consumed samples:       441600 | elapsed time per iteration (ms): 253.6 | learning rate: 8.763E-05 | global batch size:    32 | lm loss: 7.497571E-05 | loss scale: 134217728.0 | grad norm: 0.007 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.44 | backward-params-all-reduce: 64.27 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.87 | optimizer-unscale-and-check-inf: 16.47 | optimizer-clip-main-grad: 7.20 | optimizer-copy-main-to-model-params: 12.07 | optimizer: 95.13 | batch-generator: 7.93
 iteration    13900/  200000 | consumed samples:       444800 | elapsed time per iteration (ms): 254.4 | learning rate: 8.754E-05 | global batch size:    32 | lm loss: 7.445254E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.19 | backward-params-all-reduce: 62.85 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.91 | optimizer-unscale-and-check-inf: 17.69 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.19 | optimizer: 97.07 | batch-generator: 8.05
 iteration    14000/  200000 | consumed samples:       448000 | elapsed time per iteration (ms): 254.4 | learning rate: 8.745E-05 | global batch size:    32 | lm loss: 6.997975E-05 | loss scale: 134217728.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.30 | backward-params-all-reduce: 61.86 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 17.07 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 96.40 | batch-generator: 8.16
-------------------------------------------------------------------------------------------------
 validation loss at iteration 14000 | lm loss value: 8.673936E-05 | lm loss PPL: 1.000087E+00 | 
 at iteration 14000, match long value: 0.005632387314937408 | match short value: -0.009006322436388612 
--------------------------------------------------------------------------------------------------------
 iteration    14100/  200000 | consumed samples:       451200 | elapsed time per iteration (ms): 285.5 | learning rate: 8.736E-05 | global batch size:    32 | lm loss: 8.220960E-05 | loss scale: 67108864.0 | grad norm: 0.010 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.02 | backward-params-all-reduce: 63.52 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.12 | optimizer-unscale-and-check-inf: 20.15 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 13.40 | optimizer: 100.49 | batch-generator: 21.61
 iteration    14200/  200000 | consumed samples:       454400 | elapsed time per iteration (ms): 258.4 | learning rate: 8.726E-05 | global batch size:    32 | lm loss: 7.312521E-05 | loss scale: 67108864.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.13 | backward-params-all-reduce: 63.56 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.19 | optimizer-unscale-and-check-inf: 20.04 | optimizer-clip-main-grad: 7.37 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 99.76 | batch-generator: 7.94
 iteration    14300/  200000 | consumed samples:       457600 | elapsed time per iteration (ms): 259.3 | learning rate: 8.717E-05 | global batch size:    32 | lm loss: 7.279892E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.08 | backward-params-all-reduce: 66.09 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.12 | optimizer-unscale-and-check-inf: 20.77 | optimizer-clip-main-grad: 7.39 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 100.46 | batch-generator: 8.05
 iteration    14400/  200000 | consumed samples:       460800 | elapsed time per iteration (ms): 264.3 | learning rate: 8.708E-05 | global batch size:    32 | lm loss: 6.783302E-05 | loss scale: 67108864.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.10 | backward-params-all-reduce: 63.52 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.10 | optimizer-unscale-and-check-inf: 19.03 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.65 | batch-generator: 8.06
 iteration    14500/  200000 | consumed samples:       464000 | elapsed time per iteration (ms): 259.8 | learning rate: 8.699E-05 | global batch size:    32 | lm loss: 7.111533E-05 | loss scale: 67108864.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.04 | backward-params-all-reduce: 66.60 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.95 | optimizer-unscale-and-check-inf: 21.29 | optimizer-clip-main-grad: 7.40 | optimizer-copy-main-to-model-params: 12.19 | optimizer: 100.82 | batch-generator: 8.02
-------------------------------------------------------------------------------------------------
 validation loss at iteration 14500 | lm loss value: 7.918688E-05 | lm loss PPL: 1.000079E+00 | 
 at iteration 14500, match long value: 0.007190713510384619 | match short value: -0.019527978435993285 
--------------------------------------------------------------------------------------------------------
 iteration    14600/  200000 | consumed samples:       467200 | elapsed time per iteration (ms): 269.4 | learning rate: 8.690E-05 | global batch size:    32 | lm loss: 7.380549E-05 | loss scale: 67108864.0 | grad norm: 0.015 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.21 | backward-params-all-reduce: 65.58 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.89 | optimizer-unscale-and-check-inf: 20.53 | optimizer-clip-main-grad: 7.38 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 99.96 | batch-generator: 14.59
 iteration    14700/  200000 | consumed samples:       470400 | elapsed time per iteration (ms): 260.6 | learning rate: 8.681E-05 | global batch size:    32 | lm loss: 6.998019E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.73 | backward-params-all-reduce: 63.35 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.99 | optimizer-unscale-and-check-inf: 20.96 | optimizer-clip-main-grad: 7.39 | optimizer-copy-main-to-model-params: 12.22 | optimizer: 100.57 | batch-generator: 8.29
 iteration    14800/  200000 | consumed samples:       473600 | elapsed time per iteration (ms): 267.0 | learning rate: 8.672E-05 | global batch size:    32 | lm loss: 6.076205E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.67 | backward-params-all-reduce: 63.94 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.05 | optimizer-unscale-and-check-inf: 19.75 | optimizer-clip-main-grad: 7.41 | optimizer-copy-main-to-model-params: 13.59 | optimizer: 100.80 | batch-generator: 8.27
 iteration    14900/  200000 | consumed samples:       476800 | elapsed time per iteration (ms): 256.4 | learning rate: 8.663E-05 | global batch size:    32 | lm loss: 6.932076E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.73 | backward-params-all-reduce: 64.53 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.11 | optimizer-unscale-and-check-inf: 19.41 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 99.01 | batch-generator: 8.14
 iteration    15000/  200000 | consumed samples:       480000 | elapsed time per iteration (ms): 257.4 | learning rate: 8.654E-05 | global batch size:    32 | lm loss: 6.495927E-05 | loss scale: 67108864.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.91 | backward-params-all-reduce: 63.60 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.30 | optimizer-unscale-and-check-inf: 19.21 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 99.00 | batch-generator: 8.08
-------------------------------------------------------------------------------------------------
 validation loss at iteration 15000 | lm loss value: 7.438125E-05 | lm loss PPL: 1.000074E+00 | 
 at iteration 15000, match long value: 0.006128545502984964 | match short value: 0.01190838978560674 
------------------------------------------------------------------------------------------------------
 iteration    15100/  200000 | consumed samples:       483200 | elapsed time per iteration (ms): 267.8 | learning rate: 8.644E-05 | global batch size:    32 | lm loss: 6.733441E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.14 | backward-params-all-reduce: 64.02 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.00 | optimizer-unscale-and-check-inf: 19.60 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 99.11 | batch-generator: 14.55
 iteration    15200/  200000 | consumed samples:       486400 | elapsed time per iteration (ms): 258.9 | learning rate: 8.635E-05 | global batch size:    32 | lm loss: 7.175343E-05 | loss scale: 134217728.0 | grad norm: 0.014 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.19 | backward-params-all-reduce: 63.83 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.24 | optimizer-unscale-and-check-inf: 19.04 | optimizer-clip-main-grad: 7.38 | optimizer-copy-main-to-model-params: 12.19 | optimizer: 98.86 | batch-generator: 8.20
 iteration    15300/  200000 | consumed samples:       489600 | elapsed time per iteration (ms): 257.9 | learning rate: 8.626E-05 | global batch size:    32 | lm loss: 7.123641E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.16 | backward-params-all-reduce: 63.96 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.30 | optimizer-unscale-and-check-inf: 20.17 | optimizer-clip-main-grad: 7.41 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 100.06 | batch-generator: 8.01
 iteration    15400/  200000 | consumed samples:       492800 | elapsed time per iteration (ms): 254.0 | learning rate: 8.617E-05 | global batch size:    32 | lm loss: 6.294495E-05 | loss scale: 134217728.0 | grad norm: 0.006 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.24 | backward-params-all-reduce: 62.45 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 19.97 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.05 | optimizer: 98.73 | batch-generator: 8.19
 iteration    15500/  200000 | consumed samples:       496000 | elapsed time per iteration (ms): 257.9 | learning rate: 8.608E-05 | global batch size:    32 | lm loss: 6.720989E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.24 | backward-params-all-reduce: 62.87 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.90 | optimizer-unscale-and-check-inf: 20.02 | optimizer-clip-main-grad: 7.38 | optimizer-copy-main-to-model-params: 13.73 | optimizer: 101.04 | batch-generator: 8.12
-------------------------------------------------------------------------------------------------
 validation loss at iteration 15500 | lm loss value: 7.327267E-05 | lm loss PPL: 1.000073E+00 | 
 at iteration 15500, match long value: 0.006790344749226475 | match short value: -0.009922865024047663 
--------------------------------------------------------------------------------------------------------
 iteration    15600/  200000 | consumed samples:       499200 | elapsed time per iteration (ms): 274.7 | learning rate: 8.599E-05 | global batch size:    32 | lm loss: 6.536708E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.97 | backward-params-all-reduce: 63.65 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.97 | optimizer-unscale-and-check-inf: 18.75 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.23 | batch-generator: 15.33
 iteration    15700/  200000 | consumed samples:       502400 | elapsed time per iteration (ms): 256.2 | learning rate: 8.590E-05 | global batch size:    32 | lm loss: 7.285119E-05 | loss scale: 134217728.0 | grad norm: 0.010 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.08 | backward-params-all-reduce: 64.04 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 20.41 | optimizer-clip-main-grad: 7.39 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 99.82 | batch-generator: 8.12
 iteration    15800/  200000 | consumed samples:       505600 | elapsed time per iteration (ms): 258.1 | learning rate: 8.581E-05 | global batch size:    32 | lm loss: 8.534914E-05 | loss scale: 67108864.0 | grad norm: 0.008 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.11 | backward-params-all-reduce: 64.54 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.99 | optimizer-unscale-and-check-inf: 21.32 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.04 | optimizer: 100.22 | batch-generator: 8.10
 iteration    15900/  200000 | consumed samples:       508800 | elapsed time per iteration (ms): 255.5 | learning rate: 8.572E-05 | global batch size:    32 | lm loss: 6.654515E-05 | loss scale: 67108864.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.19 | backward-params-all-reduce: 62.59 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.07 | optimizer-unscale-and-check-inf: 19.11 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.69 | batch-generator: 8.07
 iteration    16000/  200000 | consumed samples:       512000 | elapsed time per iteration (ms): 257.7 | learning rate: 8.563E-05 | global batch size:    32 | lm loss: 7.090880E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.40 | backward-params-all-reduce: 62.68 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.96 | optimizer-unscale-and-check-inf: 19.75 | optimizer-clip-main-grad: 7.42 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 99.34 | batch-generator: 8.05
-------------------------------------------------------------------------------------------------
 validation loss at iteration 16000 | lm loss value: 6.694133E-05 | lm loss PPL: 1.000067E+00 | 
 at iteration 16000, match long value: 0.004656365347008725 | match short value: -0.006938893326582231 
--------------------------------------------------------------------------------------------------------
 iteration    16100/  200000 | consumed samples:       515200 | elapsed time per iteration (ms): 275.4 | learning rate: 8.554E-05 | global batch size:    32 | lm loss: 6.398832E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.27 | backward-params-all-reduce: 63.89 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.91 | optimizer-unscale-and-check-inf: 18.87 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.29 | batch-generator: 16.58
 iteration    16200/  200000 | consumed samples:       518400 | elapsed time per iteration (ms): 257.9 | learning rate: 8.544E-05 | global batch size:    32 | lm loss: 6.320606E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.07 | backward-params-all-reduce: 63.66 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 19.49 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.83 | batch-generator: 7.99
 iteration    16300/  200000 | consumed samples:       521600 | elapsed time per iteration (ms): 267.4 | learning rate: 8.535E-05 | global batch size:    32 | lm loss: 6.068806E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.74 | backward-params-all-reduce: 62.53 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.83 | optimizer-unscale-and-check-inf: 19.12 | optimizer-clip-main-grad: 7.37 | optimizer-copy-main-to-model-params: 13.58 | optimizer: 99.91 | batch-generator: 7.98
 iteration    16400/  200000 | consumed samples:       524800 | elapsed time per iteration (ms): 254.9 | learning rate: 8.526E-05 | global batch size:    32 | lm loss: 6.262591E-05 | loss scale: 67108864.0 | grad norm: 0.010 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.11 | backward-params-all-reduce: 62.52 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.95 | optimizer-unscale-and-check-inf: 19.14 | optimizer-clip-main-grad: 7.39 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.65 | batch-generator: 8.02
 iteration    16500/  200000 | consumed samples:       528000 | elapsed time per iteration (ms): 253.2 | learning rate: 8.517E-05 | global batch size:    32 | lm loss: 6.421274E-05 | loss scale: 67108864.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.11 | backward-params-all-reduce: 62.41 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 18.31 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.64 | batch-generator: 8.00
-------------------------------------------------------------------------------------------------
 validation loss at iteration 16500 | lm loss value: 6.642663E-05 | lm loss PPL: 1.000066E+00 | 
 at iteration 16500, match long value: 0.005813606466764835 | match short value: -0.011864865835936052 
--------------------------------------------------------------------------------------------------------
 iteration    16600/  200000 | consumed samples:       531200 | elapsed time per iteration (ms): 266.9 | learning rate: 8.508E-05 | global batch size:    32 | lm loss: 6.641977E-05 | loss scale: 67108864.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.09 | backward-params-all-reduce: 64.14 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.99 | optimizer-unscale-and-check-inf: 19.37 | optimizer-clip-main-grad: 7.37 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.90 | batch-generator: 13.53
 iteration    16700/  200000 | consumed samples:       534400 | elapsed time per iteration (ms): 253.9 | learning rate: 8.499E-05 | global batch size:    32 | lm loss: 6.077047E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.04 | backward-params-all-reduce: 62.46 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.92 | optimizer-unscale-and-check-inf: 18.06 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.50 | batch-generator: 8.12
 iteration    16800/  200000 | consumed samples:       537600 | elapsed time per iteration (ms): 254.4 | learning rate: 8.490E-05 | global batch size:    32 | lm loss: 6.534823E-05 | loss scale: 134217728.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.04 | backward-params-all-reduce: 63.83 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 18.67 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.02 | batch-generator: 8.09
 iteration    16900/  200000 | consumed samples:       540800 | elapsed time per iteration (ms): 257.8 | learning rate: 8.481E-05 | global batch size:    32 | lm loss: 6.468730E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.80 | backward-params-all-reduce: 63.64 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.26 | optimizer-unscale-and-check-inf: 18.65 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.43 | batch-generator: 8.00
 iteration    17000/  200000 | consumed samples:       544000 | elapsed time per iteration (ms): 266.3 | learning rate: 8.472E-05 | global batch size:    32 | lm loss: 6.042966E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.76 | backward-params-all-reduce: 62.50 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.09 | optimizer-unscale-and-check-inf: 19.25 | optimizer-clip-main-grad: 7.39 | optimizer-copy-main-to-model-params: 13.54 | optimizer: 100.29 | batch-generator: 8.13
-------------------------------------------------------------------------------------------------
 validation loss at iteration 17000 | lm loss value: 6.686539E-05 | lm loss PPL: 1.000067E+00 | 
 at iteration 17000, match long value: 0.005092244054290499 | match short value: -0.027977671673066185 
--------------------------------------------------------------------------------------------------------
 iteration    17100/  200000 | consumed samples:       547200 | elapsed time per iteration (ms): 275.4 | learning rate: 8.462E-05 | global batch size:    32 | lm loss: 6.240158E-05 | loss scale: 134217728.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.96 | backward-params-all-reduce: 65.93 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.02 | optimizer-unscale-and-check-inf: 19.23 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.77 | batch-generator: 13.82
 iteration    17200/  200000 | consumed samples:       550400 | elapsed time per iteration (ms): 258.0 | learning rate: 8.453E-05 | global batch size:    32 | lm loss: 6.069713E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.63 | backward-params-all-reduce: 64.92 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.18 | optimizer-unscale-and-check-inf: 17.35 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.02 | batch-generator: 7.96
 iteration    17300/  200000 | consumed samples:       553600 | elapsed time per iteration (ms): 255.6 | learning rate: 8.444E-05 | global batch size:    32 | lm loss: 6.962299E-05 | loss scale: 134217728.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.08 | backward-params-all-reduce: 63.04 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.91 | optimizer-unscale-and-check-inf: 19.04 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.46 | batch-generator: 8.06
 iteration    17400/  200000 | consumed samples:       556800 | elapsed time per iteration (ms): 260.0 | learning rate: 8.435E-05 | global batch size:    32 | lm loss: 6.353446E-05 | loss scale: 67108864.0 | grad norm: 0.007 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.42 | backward-params-all-reduce: 62.50 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.12 | optimizer-unscale-and-check-inf: 19.53 | optimizer-clip-main-grad: 7.23 | optimizer-copy-main-to-model-params: 11.93 | optimizer: 97.90 | batch-generator: 8.06
 iteration    17500/  200000 | consumed samples:       560000 | elapsed time per iteration (ms): 264.7 | learning rate: 8.426E-05 | global batch size:    32 | lm loss: 6.868062E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.12 | backward-params-all-reduce: 62.44 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.90 | optimizer-unscale-and-check-inf: 21.21 | optimizer-clip-main-grad: 7.37 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 100.64 | batch-generator: 8.19
-------------------------------------------------------------------------------------------------
 validation loss at iteration 17500 | lm loss value: 6.056869E-05 | lm loss PPL: 1.000061E+00 | 
 at iteration 17500, match long value: 0.004985581100985622 | match short value: 0.000988988158197718 
-------------------------------------------------------------------------------------------------------
 iteration    17600/  200000 | consumed samples:       563200 | elapsed time per iteration (ms): 282.2 | learning rate: 8.417E-05 | global batch size:    32 | lm loss: 5.739539E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.08 | backward-params-all-reduce: 64.43 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.40 | optimizer-unscale-and-check-inf: 18.18 | optimizer-clip-main-grad: 7.45 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.20 | batch-generator: 25.12
 iteration    17700/  200000 | consumed samples:       566400 | elapsed time per iteration (ms): 263.4 | learning rate: 8.408E-05 | global batch size:    32 | lm loss: 6.001322E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 56.62 | backward-params-all-reduce: 61.76 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.00 | optimizer-unscale-and-check-inf: 15.06 | optimizer-clip-main-grad: 7.25 | optimizer-copy-main-to-model-params: 13.72 | optimizer: 96.01 | batch-generator: 7.81
 iteration    17800/  200000 | consumed samples:       569600 | elapsed time per iteration (ms): 269.9 | learning rate: 8.399E-05 | global batch size:    32 | lm loss: 6.305661E-05 | loss scale: 67108864.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 55.08 | backward-params-all-reduce: 62.39 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.97 | optimizer-unscale-and-check-inf: 17.66 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 97.07 | batch-generator: 7.89
 iteration    17900/  200000 | consumed samples:       572800 | elapsed time per iteration (ms): 263.9 | learning rate: 8.390E-05 | global batch size:    32 | lm loss: 5.858616E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.05 | backward-params-all-reduce: 64.76 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.82 | optimizer-unscale-and-check-inf: 22.01 | optimizer-clip-main-grad: 7.41 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 101.39 | batch-generator: 8.02
 iteration    18000/  200000 | consumed samples:       576000 | elapsed time per iteration (ms): 259.7 | learning rate: 8.381E-05 | global batch size:    32 | lm loss: 5.964084E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.37 | backward-params-all-reduce: 64.40 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 19.63 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.99 | batch-generator: 8.01
-------------------------------------------------------------------------------------------------
 validation loss at iteration 18000 | lm loss value: 5.953495E-05 | lm loss PPL: 1.000060E+00 | 
 at iteration 18000, match long value: 0.0034548900365235853 | match short value: 0.00816466083579158 
-------------------------------------------------------------------------------------------------------
 iteration    18100/  200000 | consumed samples:       579200 | elapsed time per iteration (ms): 274.1 | learning rate: 8.372E-05 | global batch size:    32 | lm loss: 5.555817E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.10 | backward-params-all-reduce: 65.39 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.57 | optimizer-unscale-and-check-inf: 16.06 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 96.34 | batch-generator: 14.10
 iteration    18200/  200000 | consumed samples:       582400 | elapsed time per iteration (ms): 259.7 | learning rate: 8.362E-05 | global batch size:    32 | lm loss: 5.595219E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.18 | backward-params-all-reduce: 65.24 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.01 | optimizer-unscale-and-check-inf: 21.01 | optimizer-clip-main-grad: 7.41 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 100.59 | batch-generator: 8.08
 iteration    18300/  200000 | consumed samples:       585600 | elapsed time per iteration (ms): 256.0 | learning rate: 8.353E-05 | global batch size:    32 | lm loss: 5.792126E-05 | loss scale: 67108864.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.99 | backward-params-all-reduce: 62.95 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 19.87 | optimizer-clip-main-grad: 7.38 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 99.24 | batch-generator: 8.14
 iteration    18400/  200000 | consumed samples:       588800 | elapsed time per iteration (ms): 261.2 | learning rate: 8.344E-05 | global batch size:    32 | lm loss: 5.598560E-05 | loss scale: 134217728.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.09 | backward-params-all-reduce: 65.33 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.93 | optimizer-unscale-and-check-inf: 21.50 | optimizer-clip-main-grad: 7.40 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 100.99 | batch-generator: 8.05
 iteration    18500/  200000 | consumed samples:       592000 | elapsed time per iteration (ms): 266.3 | learning rate: 8.335E-05 | global batch size:    32 | lm loss: 6.265325E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.40 | backward-params-all-reduce: 63.69 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.83 | optimizer-unscale-and-check-inf: 19.23 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 13.59 | optimizer: 99.95 | batch-generator: 8.00
-------------------------------------------------------------------------------------------------
 validation loss at iteration 18500 | lm loss value: 6.354386E-05 | lm loss PPL: 1.000064E+00 | 
 at iteration 18500, match long value: 0.0008223274098051151 | match short value: -0.012875532069876874 
---------------------------------------------------------------------------------------------------------
 iteration    18600/  200000 | consumed samples:       595200 | elapsed time per iteration (ms): 269.5 | learning rate: 8.326E-05 | global batch size:    32 | lm loss: 7.057164E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.04 | backward-params-all-reduce: 64.75 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.93 | optimizer-unscale-and-check-inf: 19.46 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 98.86 | batch-generator: 14.75
 iteration    18700/  200000 | consumed samples:       598400 | elapsed time per iteration (ms): 257.1 | learning rate: 8.317E-05 | global batch size:    32 | lm loss: 5.647944E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.93 | backward-params-all-reduce: 64.21 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 19.28 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.62 | batch-generator: 8.06
 iteration    18800/  200000 | consumed samples:       601600 | elapsed time per iteration (ms): 253.1 | learning rate: 8.308E-05 | global batch size:    32 | lm loss: 6.182975E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.06 | backward-params-all-reduce: 62.68 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.83 | optimizer-unscale-and-check-inf: 20.05 | optimizer-clip-main-grad: 7.18 | optimizer-copy-main-to-model-params: 11.91 | optimizer: 98.01 | batch-generator: 7.91
 iteration    18900/  200000 | consumed samples:       604800 | elapsed time per iteration (ms): 255.6 | learning rate: 8.299E-05 | global batch size:    32 | lm loss: 6.514828E-05 | loss scale: 67108864.0 | grad norm: 0.016 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.10 | backward-params-all-reduce: 62.88 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 20.70 | optimizer-clip-main-grad: 7.39 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 100.07 | batch-generator: 8.00
 iteration    19000/  200000 | consumed samples:       608000 | elapsed time per iteration (ms): 255.2 | learning rate: 8.290E-05 | global batch size:    32 | lm loss: 6.185324E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.01 | backward-params-all-reduce: 62.45 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 20.26 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 99.62 | batch-generator: 8.04
-------------------------------------------------------------------------------------------------
 validation loss at iteration 19000 | lm loss value: 6.091855E-05 | lm loss PPL: 1.000061E+00 | 
 at iteration 19000, match long value: 0.006888342485436751 | match short value: 0.024775426589066032 
-------------------------------------------------------------------------------------------------------
 iteration    19100/  200000 | consumed samples:       611200 | elapsed time per iteration (ms): 272.3 | learning rate: 8.281E-05 | global batch size:    32 | lm loss: 5.628190E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.11 | backward-params-all-reduce: 63.45 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.94 | optimizer-unscale-and-check-inf: 19.83 | optimizer-clip-main-grad: 7.40 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 99.33 | batch-generator: 16.91
 iteration    19200/  200000 | consumed samples:       614400 | elapsed time per iteration (ms): 267.8 | learning rate: 8.272E-05 | global batch size:    32 | lm loss: 5.515790E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.73 | backward-params-all-reduce: 62.99 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 13.16 | optimizer-unscale-and-check-inf: 13.80 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 13.66 | optimizer: 96.27 | batch-generator: 8.03
 iteration    19300/  200000 | consumed samples:       617600 | elapsed time per iteration (ms): 255.4 | learning rate: 8.262E-05 | global batch size:    32 | lm loss: 5.745310E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.10 | backward-params-all-reduce: 63.29 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.16 | optimizer-unscale-and-check-inf: 18.21 | optimizer-clip-main-grad: 7.41 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.97 | batch-generator: 7.92
 iteration    19400/  200000 | consumed samples:       620800 | elapsed time per iteration (ms): 256.8 | learning rate: 8.253E-05 | global batch size:    32 | lm loss: 6.439643E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.02 | backward-params-all-reduce: 62.35 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 13.26 | optimizer-unscale-and-check-inf: 17.71 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 98.40 | batch-generator: 7.81
 iteration    19500/  200000 | consumed samples:       624000 | elapsed time per iteration (ms): 256.4 | learning rate: 8.244E-05 | global batch size:    32 | lm loss: 5.937604E-05 | loss scale: 67108864.0 | grad norm: 0.010 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.01 | backward-params-all-reduce: 62.65 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.05 | optimizer-unscale-and-check-inf: 19.97 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 99.48 | batch-generator: 7.82
-------------------------------------------------------------------------------------------------
 validation loss at iteration 19500 | lm loss value: 7.898835E-05 | lm loss PPL: 1.000079E+00 | 
 at iteration 19500, match long value: 0.007015894929559641 | match short value: -0.00424118368984006 
-------------------------------------------------------------------------------------------------------
 iteration    19600/  200000 | consumed samples:       627200 | elapsed time per iteration (ms): 267.9 | learning rate: 8.235E-05 | global batch size:    32 | lm loss: 5.497758E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.15 | backward-params-all-reduce: 63.12 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 20.25 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 99.62 | batch-generator: 14.35
 iteration    19700/  200000 | consumed samples:       630400 | elapsed time per iteration (ms): 255.0 | learning rate: 8.226E-05 | global batch size:    32 | lm loss: 5.162123E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.08 | backward-params-all-reduce: 62.28 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.12 | optimizer-unscale-and-check-inf: 19.08 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.69 | batch-generator: 7.84
 iteration    19800/  200000 | consumed samples:       633600 | elapsed time per iteration (ms): 255.6 | learning rate: 8.217E-05 | global batch size:    32 | lm loss: 5.308630E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.07 | backward-params-all-reduce: 62.70 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.98 | optimizer-unscale-and-check-inf: 18.84 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.27 | batch-generator: 7.94
 iteration    19900/  200000 | consumed samples:       636800 | elapsed time per iteration (ms): 263.3 | learning rate: 8.208E-05 | global batch size:    32 | lm loss: 5.507422E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.45 | backward-params-all-reduce: 62.44 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.19 | optimizer-unscale-and-check-inf: 19.66 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 13.60 | optimizer: 100.80 | batch-generator: 7.98
 iteration    20000/  200000 | consumed samples:       640000 | elapsed time per iteration (ms): 265.9 | learning rate: 8.199E-05 | global batch size:    32 | lm loss: 5.129917E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.55 | backward-params-all-reduce: 63.71 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.95 | optimizer-unscale-and-check-inf: 19.65 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 99.11 | batch-generator: 8.02
-------------------------------------------------------------------------------------------------
 validation loss at iteration 20000 | lm loss value: 5.384025E-05 | lm loss PPL: 1.000054E+00 | 
 at iteration 20000, match long value: 0.005614450384431502 | match short value: 0.03731200168115949 
------------------------------------------------------------------------------------------------------
 iteration    20100/  200000 | consumed samples:       643200 | elapsed time per iteration (ms): 267.5 | learning rate: 8.190E-05 | global batch size:    32 | lm loss: 5.598148E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.21 | backward-params-all-reduce: 63.50 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.96 | optimizer-unscale-and-check-inf: 19.13 | optimizer-clip-main-grad: 7.23 | optimizer-copy-main-to-model-params: 11.94 | optimizer: 97.30 | batch-generator: 14.30
 iteration    20200/  200000 | consumed samples:       646400 | elapsed time per iteration (ms): 252.8 | learning rate: 8.181E-05 | global batch size:    32 | lm loss: 5.668776E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.20 | backward-params-all-reduce: 62.32 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.54 | optimizer-unscale-and-check-inf: 14.80 | optimizer-clip-main-grad: 7.52 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 95.04 | batch-generator: 8.14
 iteration    20300/  200000 | consumed samples:       649600 | elapsed time per iteration (ms): 255.1 | learning rate: 8.172E-05 | global batch size:    32 | lm loss: 6.006235E-05 | loss scale: 67108864.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.07 | backward-params-all-reduce: 63.20 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.65 | optimizer-unscale-and-check-inf: 17.23 | optimizer-clip-main-grad: 7.37 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.40 | batch-generator: 8.03
 iteration    20400/  200000 | consumed samples:       652800 | elapsed time per iteration (ms): 255.7 | learning rate: 8.162E-05 | global batch size:    32 | lm loss: 5.997133E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.15 | backward-params-all-reduce: 64.10 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 18.74 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 98.11 | batch-generator: 8.00
 iteration    20500/  200000 | consumed samples:       656000 | elapsed time per iteration (ms): 255.8 | learning rate: 8.153E-05 | global batch size:    32 | lm loss: 5.806594E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.05 | backward-params-all-reduce: 63.72 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 19.02 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 98.34 | batch-generator: 7.96
-------------------------------------------------------------------------------------------------
 validation loss at iteration 20500 | lm loss value: 5.770736E-05 | lm loss PPL: 1.000058E+00 | 
 at iteration 20500, match long value: 0.0073038775558253765 | match short value: 0.06169005625396952 
-------------------------------------------------------------------------------------------------------
 iteration    20600/  200000 | consumed samples:       659200 | elapsed time per iteration (ms): 271.7 | learning rate: 8.144E-05 | global batch size:    32 | lm loss: 5.281864E-05 | loss scale: 67108864.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.74 | backward-params-all-reduce: 63.15 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.89 | optimizer-unscale-and-check-inf: 19.58 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.14 | optimizer: 98.95 | batch-generator: 16.47
 iteration    20700/  200000 | consumed samples:       662400 | elapsed time per iteration (ms): 266.8 | learning rate: 8.135E-05 | global batch size:    32 | lm loss: 5.304739E-05 | loss scale: 67108864.0 | grad norm: 0.013 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.70 | backward-params-all-reduce: 62.09 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.05 | optimizer-unscale-and-check-inf: 19.60 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 13.58 | optimizer: 100.55 | batch-generator: 8.01
 iteration    20800/  200000 | consumed samples:       665600 | elapsed time per iteration (ms): 263.4 | learning rate: 8.126E-05 | global batch size:    32 | lm loss: 5.331791E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.42 | backward-params-all-reduce: 64.57 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.00 | optimizer-unscale-and-check-inf: 19.67 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 99.18 | batch-generator: 8.00
 iteration    20900/  200000 | consumed samples:       668800 | elapsed time per iteration (ms): 259.4 | learning rate: 8.117E-05 | global batch size:    32 | lm loss: 5.156939E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.21 | backward-params-all-reduce: 65.18 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 19.79 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 99.14 | batch-generator: 7.95
 iteration    21000/  200000 | consumed samples:       672000 | elapsed time per iteration (ms): 257.0 | learning rate: 8.108E-05 | global batch size:    32 | lm loss: 5.829480E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.13 | backward-params-all-reduce: 63.89 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.14 | optimizer-unscale-and-check-inf: 19.10 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.75 | batch-generator: 7.98
-------------------------------------------------------------------------------------------------
 validation loss at iteration 21000 | lm loss value: 5.415575E-05 | lm loss PPL: 1.000054E+00 | 
 at iteration 21000, match long value: 0.008251873967696517 | match short value: 0.012148238321843538 
-------------------------------------------------------------------------------------------------------
 iteration    21100/  200000 | consumed samples:       675200 | elapsed time per iteration (ms): 271.3 | learning rate: 8.099E-05 | global batch size:    32 | lm loss: 5.339563E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.13 | backward-params-all-reduce: 63.35 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.89 | optimizer-unscale-and-check-inf: 20.84 | optimizer-clip-main-grad: 7.43 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 100.33 | batch-generator: 17.59
 iteration    21200/  200000 | consumed samples:       678400 | elapsed time per iteration (ms): 255.5 | learning rate: 8.090E-05 | global batch size:    32 | lm loss: 4.789048E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.26 | backward-params-all-reduce: 62.51 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.83 | optimizer-unscale-and-check-inf: 20.82 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 100.18 | batch-generator: 8.10
 iteration    21300/  200000 | consumed samples:       681600 | elapsed time per iteration (ms): 254.0 | learning rate: 8.081E-05 | global batch size:    32 | lm loss: 5.455955E-05 | loss scale: 67108864.0 | grad norm: 0.008 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.25 | backward-params-all-reduce: 63.03 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 20.02 | optimizer-clip-main-grad: 7.20 | optimizer-copy-main-to-model-params: 11.94 | optimizer: 98.04 | batch-generator: 8.05
 iteration    21400/  200000 | consumed samples:       684800 | elapsed time per iteration (ms): 267.3 | learning rate: 8.072E-05 | global batch size:    32 | lm loss: 4.961278E-05 | loss scale: 67108864.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.84 | backward-params-all-reduce: 62.95 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.88 | optimizer-unscale-and-check-inf: 20.29 | optimizer-clip-main-grad: 7.39 | optimizer-copy-main-to-model-params: 13.63 | optimizer: 101.18 | batch-generator: 8.04
 iteration    21500/  200000 | consumed samples:       688000 | elapsed time per iteration (ms): 256.8 | learning rate: 8.062E-05 | global batch size:    32 | lm loss: 4.782526E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.23 | backward-params-all-reduce: 63.16 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.04 | optimizer-unscale-and-check-inf: 21.28 | optimizer-clip-main-grad: 7.38 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 100.88 | batch-generator: 8.16
-------------------------------------------------------------------------------------------------
 validation loss at iteration 21500 | lm loss value: 5.800679E-05 | lm loss PPL: 1.000058E+00 | 
 at iteration 21500, match long value: 0.006799287915294638 | match short value: 0.006768384572802991 
-------------------------------------------------------------------------------------------------------
 iteration    21600/  200000 | consumed samples:       691200 | elapsed time per iteration (ms): 269.2 | learning rate: 8.053E-05 | global batch size:    32 | lm loss: 4.710309E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.86 | backward-params-all-reduce: 62.87 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.94 | optimizer-unscale-and-check-inf: 20.67 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 100.09 | batch-generator: 13.88
 iteration    21700/  200000 | consumed samples:       694400 | elapsed time per iteration (ms): 258.9 | learning rate: 8.044E-05 | global batch size:    32 | lm loss: 4.770754E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.07 | backward-params-all-reduce: 63.38 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.19 | optimizer-unscale-and-check-inf: 18.84 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.55 | batch-generator: 7.94
 iteration    21800/  200000 | consumed samples:       697600 | elapsed time per iteration (ms): 255.3 | learning rate: 8.035E-05 | global batch size:    32 | lm loss: 5.159236E-05 | loss scale: 67108864.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.63 | backward-params-all-reduce: 61.68 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.94 | optimizer-unscale-and-check-inf: 20.23 | optimizer-clip-main-grad: 7.38 | optimizer-copy-main-to-model-params: 12.19 | optimizer: 99.75 | batch-generator: 7.99
 iteration    21900/  200000 | consumed samples:       700800 | elapsed time per iteration (ms): 254.8 | learning rate: 8.026E-05 | global batch size:    32 | lm loss: 4.748707E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.15 | backward-params-all-reduce: 62.03 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 20.60 | optimizer-clip-main-grad: 7.39 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 100.00 | batch-generator: 8.11
 iteration    22000/  200000 | consumed samples:       704000 | elapsed time per iteration (ms): 254.9 | learning rate: 8.017E-05 | global batch size:    32 | lm loss: 4.974124E-05 | loss scale: 67108864.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.59 | backward-params-all-reduce: 62.28 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.89 | optimizer-unscale-and-check-inf: 19.80 | optimizer-clip-main-grad: 7.37 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 99.23 | batch-generator: 8.02
-------------------------------------------------------------------------------------------------
 validation loss at iteration 22000 | lm loss value: 5.395243E-05 | lm loss PPL: 1.000054E+00 | 
 at iteration 22000, match long value: 0.008007302817025459 | match short value: -0.01905329649375194 
-------------------------------------------------------------------------------------------------------
 iteration    22100/  200000 | consumed samples:       707200 | elapsed time per iteration (ms): 272.7 | learning rate: 8.008E-05 | global batch size:    32 | lm loss: 5.207122E-05 | loss scale: 67108864.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.42 | backward-params-all-reduce: 62.27 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 19.15 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 13.59 | optimizer: 99.91 | batch-generator: 17.80
 iteration    22200/  200000 | consumed samples:       710400 | elapsed time per iteration (ms): 262.9 | learning rate: 7.999E-05 | global batch size:    32 | lm loss: 6.116937E-05 | loss scale: 67108864.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.50 | backward-params-all-reduce: 63.12 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 19.68 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.99 | batch-generator: 7.97
 iteration    22300/  200000 | consumed samples:       713600 | elapsed time per iteration (ms): 254.0 | learning rate: 7.990E-05 | global batch size:    32 | lm loss: 4.934849E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.24 | backward-params-all-reduce: 62.68 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.89 | optimizer-unscale-and-check-inf: 19.51 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.89 | batch-generator: 7.89
 iteration    22400/  200000 | consumed samples:       716800 | elapsed time per iteration (ms): 257.0 | learning rate: 7.980E-05 | global batch size:    32 | lm loss: 4.686916E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.29 | backward-params-all-reduce: 63.19 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.89 | optimizer-unscale-and-check-inf: 19.12 | optimizer-clip-main-grad: 7.37 | optimizer-copy-main-to-model-params: 12.19 | optimizer: 98.57 | batch-generator: 7.98
 iteration    22500/  200000 | consumed samples:       720000 | elapsed time per iteration (ms): 255.1 | learning rate: 7.971E-05 | global batch size:    32 | lm loss: 4.790605E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.04 | backward-params-all-reduce: 61.96 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 19.25 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.60 | batch-generator: 7.91
-------------------------------------------------------------------------------------------------
 validation loss at iteration 22500 | lm loss value: 5.351189E-05 | lm loss PPL: 1.000054E+00 | 
 at iteration 22500, match long value: 0.0033841231677090504 | match short value: -0.020227410213924244 
---------------------------------------------------------------------------------------------------------
 iteration    22600/  200000 | consumed samples:       723200 | elapsed time per iteration (ms): 268.4 | learning rate: 7.962E-05 | global batch size:    32 | lm loss: 5.382743E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.19 | backward-params-all-reduce: 63.37 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.01 | optimizer-unscale-and-check-inf: 18.60 | optimizer-clip-main-grad: 7.15 | optimizer-copy-main-to-model-params: 11.93 | optimizer: 96.72 | batch-generator: 13.81
 iteration    22700/  200000 | consumed samples:       726400 | elapsed time per iteration (ms): 253.6 | learning rate: 7.953E-05 | global batch size:    32 | lm loss: 4.843843E-05 | loss scale: 67108864.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.05 | backward-params-all-reduce: 62.81 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 17.24 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 96.56 | batch-generator: 8.02
 iteration    22800/  200000 | consumed samples:       729600 | elapsed time per iteration (ms): 256.7 | learning rate: 7.944E-05 | global batch size:    32 | lm loss: 4.904001E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.07 | backward-params-all-reduce: 62.69 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.31 | optimizer-unscale-and-check-inf: 18.95 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.76 | batch-generator: 7.92
 iteration    22900/  200000 | consumed samples:       732800 | elapsed time per iteration (ms): 264.4 | learning rate: 7.935E-05 | global batch size:    32 | lm loss: 5.213238E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.76 | backward-params-all-reduce: 62.04 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.21 | optimizer-unscale-and-check-inf: 18.67 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 13.62 | optimizer: 99.82 | batch-generator: 7.92
 iteration    23000/  200000 | consumed samples:       736000 | elapsed time per iteration (ms): 255.9 | learning rate: 7.926E-05 | global batch size:    32 | lm loss: 4.610702E-05 | loss scale: 67108864.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.04 | backward-params-all-reduce: 62.59 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.90 | optimizer-unscale-and-check-inf: 19.96 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 99.36 | batch-generator: 7.93
-------------------------------------------------------------------------------------------------
 validation loss at iteration 23000 | lm loss value: 5.912409E-05 | lm loss PPL: 1.000059E+00 | 
 at iteration 23000, match long value: -0.004064764637181343 | match short value: -0.03721919826915813 
--------------------------------------------------------------------------------------------------------
 iteration    23100/  200000 | consumed samples:       739200 | elapsed time per iteration (ms): 265.7 | learning rate: 7.917E-05 | global batch size:    32 | lm loss: 4.489732E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.09 | backward-params-all-reduce: 63.01 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.97 | optimizer-unscale-and-check-inf: 17.74 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.15 | batch-generator: 14.27
 iteration    23200/  200000 | consumed samples:       742400 | elapsed time per iteration (ms): 256.6 | learning rate: 7.908E-05 | global batch size:    32 | lm loss: 4.946021E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.03 | backward-params-all-reduce: 63.09 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.97 | optimizer-unscale-and-check-inf: 17.69 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 97.16 | batch-generator: 8.07
 iteration    23300/  200000 | consumed samples:       745600 | elapsed time per iteration (ms): 254.2 | learning rate: 7.899E-05 | global batch size:    32 | lm loss: 4.831385E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.38 | backward-params-all-reduce: 62.52 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.96 | optimizer-unscale-and-check-inf: 17.80 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 97.26 | batch-generator: 8.02
 iteration    23400/  200000 | consumed samples:       748800 | elapsed time per iteration (ms): 254.1 | learning rate: 7.890E-05 | global batch size:    32 | lm loss: 5.311719E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.17 | backward-params-all-reduce: 62.57 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 18.37 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.73 | batch-generator: 7.97
 iteration    23500/  200000 | consumed samples:       752000 | elapsed time per iteration (ms): 251.8 | learning rate: 7.880E-05 | global batch size:    32 | lm loss: 4.951873E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.09 | backward-params-all-reduce: 61.96 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.03 | optimizer-unscale-and-check-inf: 17.25 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 96.80 | batch-generator: 7.98
-------------------------------------------------------------------------------------------------
 validation loss at iteration 23500 | lm loss value: 5.305619E-05 | lm loss PPL: 1.000053E+00 | 
 at iteration 23500, match long value: 0.0037732816221843117 | match short value: 0.00223861246389141 
-------------------------------------------------------------------------------------------------------
 iteration    23600/  200000 | consumed samples:       755200 | elapsed time per iteration (ms): 276.3 | learning rate: 7.871E-05 | global batch size:    32 | lm loss: 4.859340E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.29 | backward-params-all-reduce: 61.92 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.26 | optimizer-unscale-and-check-inf: 17.48 | optimizer-clip-main-grad: 7.16 | optimizer-copy-main-to-model-params: 13.45 | optimizer: 97.38 | batch-generator: 18.41
 iteration    23700/  200000 | consumed samples:       758400 | elapsed time per iteration (ms): 253.7 | learning rate: 7.862E-05 | global batch size:    32 | lm loss: 4.727597E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.18 | backward-params-all-reduce: 62.08 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.91 | optimizer-unscale-and-check-inf: 18.92 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.35 | batch-generator: 7.96
 iteration    23800/  200000 | consumed samples:       761600 | elapsed time per iteration (ms): 253.7 | learning rate: 7.853E-05 | global batch size:    32 | lm loss: 4.252710E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.17 | backward-params-all-reduce: 62.78 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 17.98 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.32 | batch-generator: 7.94
 iteration    23900/  200000 | consumed samples:       764800 | elapsed time per iteration (ms): 256.7 | learning rate: 7.844E-05 | global batch size:    32 | lm loss: 4.404779E-05 | loss scale: 67108864.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.17 | backward-params-all-reduce: 64.29 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.95 | optimizer-unscale-and-check-inf: 18.46 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.93 | batch-generator: 7.90
 iteration    24000/  200000 | consumed samples:       768000 | elapsed time per iteration (ms): 256.2 | learning rate: 7.835E-05 | global batch size:    32 | lm loss: 4.412825E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.02 | backward-params-all-reduce: 63.39 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.63 | optimizer-unscale-and-check-inf: 17.02 | optimizer-clip-main-grad: 7.45 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.26 | batch-generator: 8.07
-------------------------------------------------------------------------------------------------
 validation loss at iteration 24000 | lm loss value: 4.960861E-05 | lm loss PPL: 1.000050E+00 | 
 at iteration 24000, match long value: 0.007723715372129153 | match short value: 0.024699309418844396 
-------------------------------------------------------------------------------------------------------
 iteration    24100/  200000 | consumed samples:       771200 | elapsed time per iteration (ms): 267.7 | learning rate: 7.826E-05 | global batch size:    32 | lm loss: 4.612754E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.22 | backward-params-all-reduce: 64.07 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.97 | optimizer-unscale-and-check-inf: 17.97 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.42 | batch-generator: 15.32
 iteration    24200/  200000 | consumed samples:       774400 | elapsed time per iteration (ms): 254.8 | learning rate: 7.817E-05 | global batch size:    32 | lm loss: 5.519264E-05 | loss scale: 67108864.0 | grad norm: 0.014 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.12 | backward-params-all-reduce: 62.43 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.88 | optimizer-unscale-and-check-inf: 17.49 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 96.84 | batch-generator: 7.82
 iteration    24300/  200000 | consumed samples:       777600 | elapsed time per iteration (ms): 256.4 | learning rate: 7.808E-05 | global batch size:    32 | lm loss: 5.715200E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.10 | backward-params-all-reduce: 63.30 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 16.80 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 13.68 | optimizer: 97.62 | batch-generator: 7.88
 iteration    24400/  200000 | consumed samples:       780800 | elapsed time per iteration (ms): 262.7 | learning rate: 7.799E-05 | global batch size:    32 | lm loss: 4.634574E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.60 | backward-params-all-reduce: 63.56 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.25 | optimizer-unscale-and-check-inf: 19.02 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.73 | batch-generator: 7.89
 iteration    24500/  200000 | consumed samples:       784000 | elapsed time per iteration (ms): 256.3 | learning rate: 7.789E-05 | global batch size:    32 | lm loss: 4.517197E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.56 | backward-params-all-reduce: 61.80 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 13.10 | optimizer-unscale-and-check-inf: 18.44 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 99.01 | batch-generator: 7.91
-------------------------------------------------------------------------------------------------
 validation loss at iteration 24500 | lm loss value: 5.086154E-05 | lm loss PPL: 1.000051E+00 | 
 at iteration 24500, match long value: 0.00169066119626734 | match short value: -0.03088609924594526 
------------------------------------------------------------------------------------------------------
 iteration    24600/  200000 | consumed samples:       787200 | elapsed time per iteration (ms): 273.3 | learning rate: 7.780E-05 | global batch size:    32 | lm loss: 4.418841E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.24 | backward-params-all-reduce: 62.93 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.83 | optimizer-unscale-and-check-inf: 20.11 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 99.43 | batch-generator: 18.75
 iteration    24700/  200000 | consumed samples:       790400 | elapsed time per iteration (ms): 255.1 | learning rate: 7.771E-05 | global batch size:    32 | lm loss: 3.919413E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.11 | backward-params-all-reduce: 62.58 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.88 | optimizer-unscale-and-check-inf: 14.91 | optimizer-clip-main-grad: 7.25 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 94.19 | batch-generator: 7.67
 iteration    24800/  200000 | consumed samples:       793600 | elapsed time per iteration (ms): 256.1 | learning rate: 7.762E-05 | global batch size:    32 | lm loss: 4.069051E-05 | loss scale: 134217728.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.17 | backward-params-all-reduce: 63.14 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.98 | optimizer-unscale-and-check-inf: 18.20 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.63 | batch-generator: 7.90
 iteration    24900/  200000 | consumed samples:       796800 | elapsed time per iteration (ms): 257.1 | learning rate: 7.753E-05 | global batch size:    32 | lm loss: 4.507159E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.02 | backward-params-all-reduce: 64.10 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.89 | optimizer-unscale-and-check-inf: 19.39 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.73 | batch-generator: 7.96
 iteration    25000/  200000 | consumed samples:       800000 | elapsed time per iteration (ms): 257.6 | learning rate: 7.744E-05 | global batch size:    32 | lm loss: 4.180867E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.12 | backward-params-all-reduce: 63.69 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.91 | optimizer-unscale-and-check-inf: 15.65 | optimizer-clip-main-grad: 7.27 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 94.99 | batch-generator: 7.84
-------------------------------------------------------------------------------------------------
 validation loss at iteration 25000 | lm loss value: 4.454450E-05 | lm loss PPL: 1.000045E+00 | 
 at iteration 25000, match long value: 0.006378430558706886 | match short value: 0.012534215135122314 
-------------------------------------------------------------------------------------------------------
 iteration    25100/  200000 | consumed samples:       803200 | elapsed time per iteration (ms): 282.7 | learning rate: 7.735E-05 | global batch size:    32 | lm loss: 4.107833E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.85 | backward-params-all-reduce: 64.91 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.90 | optimizer-unscale-and-check-inf: 18.27 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 13.60 | optimizer: 99.08 | batch-generator: 17.29
 iteration    25200/  200000 | consumed samples:       806400 | elapsed time per iteration (ms): 255.0 | learning rate: 7.726E-05 | global batch size:    32 | lm loss: 4.152207E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.01 | backward-params-all-reduce: 62.76 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.05 | optimizer-unscale-and-check-inf: 18.46 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.99 | batch-generator: 8.00
 iteration    25300/  200000 | consumed samples:       809600 | elapsed time per iteration (ms): 256.2 | learning rate: 7.717E-05 | global batch size:    32 | lm loss: 5.723707E-05 | loss scale: 67108864.0 | grad norm: 0.011 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.99 | backward-params-all-reduce: 65.52 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 18.88 | optimizer-clip-main-grad: 7.20 | optimizer-copy-main-to-model-params: 11.92 | optimizer: 96.87 | batch-generator: 7.97
 iteration    25400/  200000 | consumed samples:       812800 | elapsed time per iteration (ms): 265.0 | learning rate: 7.708E-05 | global batch size:    32 | lm loss: 4.302297E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.20 | backward-params-all-reduce: 63.27 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.99 | optimizer-unscale-and-check-inf: 19.46 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.95 | batch-generator: 7.99
 iteration    25500/  200000 | consumed samples:       816000 | elapsed time per iteration (ms): 262.9 | learning rate: 7.699E-05 | global batch size:    32 | lm loss: 4.055590E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.09 | backward-params-all-reduce: 64.77 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.35 | optimizer-unscale-and-check-inf: 19.12 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.93 | batch-generator: 7.94
-------------------------------------------------------------------------------------------------
 validation loss at iteration 25500 | lm loss value: 4.968767E-05 | lm loss PPL: 1.000050E+00 | 
 at iteration 25500, match long value: 0.0051008316757610575 | match short value: -0.008664879453363698 
---------------------------------------------------------------------------------------------------------
 iteration    25600/  200000 | consumed samples:       819200 | elapsed time per iteration (ms): 265.5 | learning rate: 7.689E-05 | global batch size:    32 | lm loss: 4.538171E-05 | loss scale: 67108864.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.10 | backward-params-all-reduce: 62.86 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.66 | optimizer-unscale-and-check-inf: 16.87 | optimizer-clip-main-grad: 7.26 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 96.93 | batch-generator: 13.97
 iteration    25700/  200000 | consumed samples:       822400 | elapsed time per iteration (ms): 255.2 | learning rate: 7.680E-05 | global batch size:    32 | lm loss: 4.581844E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.05 | backward-params-all-reduce: 62.14 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 18.00 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.31 | batch-generator: 7.93
 iteration    25800/  200000 | consumed samples:       825600 | elapsed time per iteration (ms): 266.5 | learning rate: 7.671E-05 | global batch size:    32 | lm loss: 4.526191E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.63 | backward-params-all-reduce: 62.78 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 17.95 | optimizer-clip-main-grad: 7.27 | optimizer-copy-main-to-model-params: 13.60 | optimizer: 98.65 | batch-generator: 7.92
 iteration    25900/  200000 | consumed samples:       828800 | elapsed time per iteration (ms): 254.5 | learning rate: 7.662E-05 | global batch size:    32 | lm loss: 3.992010E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.14 | backward-params-all-reduce: 62.28 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 18.16 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.46 | batch-generator: 7.88
 iteration    26000/  200000 | consumed samples:       832000 | elapsed time per iteration (ms): 256.0 | learning rate: 7.653E-05 | global batch size:    32 | lm loss: 4.226687E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.05 | backward-params-all-reduce: 62.02 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.90 | optimizer-unscale-and-check-inf: 18.13 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.47 | batch-generator: 7.85
-------------------------------------------------------------------------------------------------
 validation loss at iteration 26000 | lm loss value: 4.447964E-05 | lm loss PPL: 1.000044E+00 | 
 at iteration 26000, match long value: 0.0017543016088920759 | match short value: -0.016394585881187692 
---------------------------------------------------------------------------------------------------------
 iteration    26100/  200000 | consumed samples:       835200 | elapsed time per iteration (ms): 265.8 | learning rate: 7.644E-05 | global batch size:    32 | lm loss: 4.347528E-05 | loss scale: 67108864.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.02 | backward-params-all-reduce: 62.74 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.14 | optimizer-unscale-and-check-inf: 17.81 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.41 | batch-generator: 13.74
 iteration    26200/  200000 | consumed samples:       838400 | elapsed time per iteration (ms): 254.9 | learning rate: 7.635E-05 | global batch size:    32 | lm loss: 4.269677E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.01 | backward-params-all-reduce: 63.37 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 17.36 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 96.68 | batch-generator: 7.97
 iteration    26300/  200000 | consumed samples:       841600 | elapsed time per iteration (ms): 258.8 | learning rate: 7.626E-05 | global batch size:    32 | lm loss: 3.984977E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.97 | backward-params-all-reduce: 62.80 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.07 | optimizer-unscale-and-check-inf: 18.86 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.37 | batch-generator: 7.87
 iteration    26400/  200000 | consumed samples:       844800 | elapsed time per iteration (ms): 256.9 | learning rate: 7.617E-05 | global batch size:    32 | lm loss: 4.266507E-05 | loss scale: 134217728.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.09 | backward-params-all-reduce: 63.73 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 17.80 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.11 | batch-generator: 7.94
 iteration    26500/  200000 | consumed samples:       848000 | elapsed time per iteration (ms): 260.6 | learning rate: 7.608E-05 | global batch size:    32 | lm loss: 4.556951E-05 | loss scale: 67108864.0 | grad norm: 0.010 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.58 | backward-params-all-reduce: 62.48 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.91 | optimizer-unscale-and-check-inf: 16.32 | optimizer-clip-main-grad: 7.13 | optimizer-copy-main-to-model-params: 13.33 | optimizer: 95.74 | batch-generator: 7.89
-------------------------------------------------------------------------------------------------
 validation loss at iteration 26500 | lm loss value: 5.458554E-05 | lm loss PPL: 1.000055E+00 | 
 at iteration 26500, match long value: -0.00011318670336718379 | match short value: -0.01681357988135727 
----------------------------------------------------------------------------------------------------------
 iteration    26600/  200000 | consumed samples:       851200 | elapsed time per iteration (ms): 273.1 | learning rate: 7.599E-05 | global batch size:    32 | lm loss: 4.573400E-05 | loss scale: 67108864.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.34 | backward-params-all-reduce: 63.41 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 16.89 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 96.18 | batch-generator: 17.50
 iteration    26700/  200000 | consumed samples:       854400 | elapsed time per iteration (ms): 255.3 | learning rate: 7.589E-05 | global batch size:    32 | lm loss: 4.167315E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.00 | backward-params-all-reduce: 62.92 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 17.54 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 96.90 | batch-generator: 7.88
 iteration    26800/  200000 | consumed samples:       857600 | elapsed time per iteration (ms): 253.2 | learning rate: 7.580E-05 | global batch size:    32 | lm loss: 4.136033E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.15 | backward-params-all-reduce: 61.86 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.40 | optimizer-unscale-and-check-inf: 15.92 | optimizer-clip-main-grad: 7.26 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 95.76 | batch-generator: 7.85
 iteration    26900/  200000 | consumed samples:       860800 | elapsed time per iteration (ms): 253.7 | learning rate: 7.571E-05 | global batch size:    32 | lm loss: 3.920047E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.08 | backward-params-all-reduce: 63.15 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.91 | optimizer-unscale-and-check-inf: 15.63 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 95.01 | batch-generator: 7.98
 iteration    27000/  200000 | consumed samples:       864000 | elapsed time per iteration (ms): 267.1 | learning rate: 7.562E-05 | global batch size:    32 | lm loss: 4.410219E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.43 | backward-params-all-reduce: 64.10 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.98 | optimizer-unscale-and-check-inf: 17.96 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.45 | batch-generator: 7.93
-------------------------------------------------------------------------------------------------
 validation loss at iteration 27000 | lm loss value: 5.635290E-05 | lm loss PPL: 1.000056E+00 | 
 at iteration 27000, match long value: 0.002041138469605165 | match short value: 0.02232269430617666 
------------------------------------------------------------------------------------------------------
 iteration    27100/  200000 | consumed samples:       867200 | elapsed time per iteration (ms): 266.0 | learning rate: 7.553E-05 | global batch size:    32 | lm loss: 4.201565E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.03 | backward-params-all-reduce: 63.75 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.97 | optimizer-unscale-and-check-inf: 16.07 | optimizer-clip-main-grad: 7.28 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 95.48 | batch-generator: 14.05
 iteration    27200/  200000 | consumed samples:       870400 | elapsed time per iteration (ms): 253.9 | learning rate: 7.544E-05 | global batch size:    32 | lm loss: 4.066287E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.03 | backward-params-all-reduce: 62.27 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 16.90 | optimizer-clip-main-grad: 7.27 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 96.19 | batch-generator: 7.97
 iteration    27300/  200000 | consumed samples:       873600 | elapsed time per iteration (ms): 271.4 | learning rate: 7.535E-05 | global batch size:    32 | lm loss: 3.884232E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.81 | backward-params-all-reduce: 63.63 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.97 | optimizer-unscale-and-check-inf: 18.53 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 13.56 | optimizer: 99.36 | batch-generator: 7.99
 iteration    27400/  200000 | consumed samples:       876800 | elapsed time per iteration (ms): 257.6 | learning rate: 7.526E-05 | global batch size:    32 | lm loss: 4.126361E-05 | loss scale: 67108864.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.99 | backward-params-all-reduce: 63.50 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.96 | optimizer-unscale-and-check-inf: 18.46 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.90 | batch-generator: 7.94
 iteration    27500/  200000 | consumed samples:       880000 | elapsed time per iteration (ms): 260.7 | learning rate: 7.517E-05 | global batch size:    32 | lm loss: 4.057938E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.19 | backward-params-all-reduce: 62.84 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.89 | optimizer-unscale-and-check-inf: 18.60 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.98 | batch-generator: 7.94
-------------------------------------------------------------------------------------------------
 validation loss at iteration 27500 | lm loss value: 4.356258E-05 | lm loss PPL: 1.000044E+00 | 
 at iteration 27500, match long value: 0.008486607467689266 | match short value: -0.07124843077947411 
-------------------------------------------------------------------------------------------------------
 iteration    27600/  200000 | consumed samples:       883200 | elapsed time per iteration (ms): 266.7 | learning rate: 7.507E-05 | global batch size:    32 | lm loss: 4.112982E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.15 | backward-params-all-reduce: 62.30 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.97 | optimizer-unscale-and-check-inf: 18.35 | optimizer-clip-main-grad: 7.37 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.86 | batch-generator: 14.74
 iteration    27700/  200000 | consumed samples:       886400 | elapsed time per iteration (ms): 257.1 | learning rate: 7.498E-05 | global batch size:    32 | lm loss: 4.318501E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.15 | backward-params-all-reduce: 63.14 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.02 | optimizer-unscale-and-check-inf: 17.38 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 96.89 | batch-generator: 8.04
 iteration    27800/  200000 | consumed samples:       889600 | elapsed time per iteration (ms): 258.9 | learning rate: 7.489E-05 | global batch size:    32 | lm loss: 4.028536E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.50 | backward-params-all-reduce: 62.87 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.21 | optimizer-unscale-and-check-inf: 18.54 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.22 | batch-generator: 7.98
 iteration    27900/  200000 | consumed samples:       892800 | elapsed time per iteration (ms): 256.0 | learning rate: 7.480E-05 | global batch size:    32 | lm loss: 3.803995E-05 | loss scale: 134217728.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.02 | backward-params-all-reduce: 63.54 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 17.97 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.31 | batch-generator: 7.95
 iteration    28000/  200000 | consumed samples:       896000 | elapsed time per iteration (ms): 265.4 | learning rate: 7.471E-05 | global batch size:    32 | lm loss: 4.132549E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.70 | backward-params-all-reduce: 64.50 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.92 | optimizer-unscale-and-check-inf: 16.21 | optimizer-clip-main-grad: 7.26 | optimizer-copy-main-to-model-params: 13.59 | optimizer: 96.98 | batch-generator: 8.23
-------------------------------------------------------------------------------------------------
 validation loss at iteration 28000 | lm loss value: 4.427883E-05 | lm loss PPL: 1.000044E+00 | 
 at iteration 28000, match long value: 0.0027032986096586814 | match short value: 0.027805488284142667 
--------------------------------------------------------------------------------------------------------
 iteration    28100/  200000 | consumed samples:       899200 | elapsed time per iteration (ms): 274.9 | learning rate: 7.462E-05 | global batch size:    32 | lm loss: 3.890705E-05 | loss scale: 134217728.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.20 | backward-params-all-reduce: 63.11 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 20.48 | optimizer-clip-main-grad: 7.39 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 99.93 | batch-generator: 22.79
 iteration    28200/  200000 | consumed samples:       902400 | elapsed time per iteration (ms): 253.4 | learning rate: 7.453E-05 | global batch size:    32 | lm loss: 3.814301E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.07 | backward-params-all-reduce: 62.15 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.90 | optimizer-unscale-and-check-inf: 19.18 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.22 | optimizer: 98.65 | batch-generator: 8.13
 iteration    28300/  200000 | consumed samples:       905600 | elapsed time per iteration (ms): 254.0 | learning rate: 7.444E-05 | global batch size:    32 | lm loss: 3.452283E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.15 | backward-params-all-reduce: 61.96 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.02 | optimizer-unscale-and-check-inf: 18.01 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 97.53 | batch-generator: 8.05
 iteration    28400/  200000 | consumed samples:       908800 | elapsed time per iteration (ms): 256.7 | learning rate: 7.435E-05 | global batch size:    32 | lm loss: 3.573254E-05 | loss scale: 134217728.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.17 | backward-params-all-reduce: 64.31 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 19.26 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.19 | optimizer: 98.62 | batch-generator: 8.04
 iteration    28500/  200000 | consumed samples:       912000 | elapsed time per iteration (ms): 252.2 | learning rate: 7.426E-05 | global batch size:    32 | lm loss: 3.800385E-05 | loss scale: 134217728.0 | grad norm: 0.006 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.23 | backward-params-all-reduce: 61.94 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.46 | optimizer-unscale-and-check-inf: 17.94 | optimizer-clip-main-grad: 7.18 | optimizer-copy-main-to-model-params: 11.93 | optimizer: 96.57 | batch-generator: 8.07
-------------------------------------------------------------------------------------------------
 validation loss at iteration 28500 | lm loss value: 4.445969E-05 | lm loss PPL: 1.000044E+00 | 
 at iteration 28500, match long value: 0.0064893749665126 | match short value: 0.01316302669253692 
----------------------------------------------------------------------------------------------------
 iteration    28600/  200000 | consumed samples:       915200 | elapsed time per iteration (ms): 275.0 | learning rate: 7.417E-05 | global batch size:    32 | lm loss: 3.717318E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.38 | backward-params-all-reduce: 65.28 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.28 | optimizer-unscale-and-check-inf: 18.23 | optimizer-clip-main-grad: 7.28 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 97.93 | batch-generator: 14.39
 iteration    28700/  200000 | consumed samples:       918400 | elapsed time per iteration (ms): 267.3 | learning rate: 7.407E-05 | global batch size:    32 | lm loss: 3.956180E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.32 | backward-params-all-reduce: 64.22 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.00 | optimizer-unscale-and-check-inf: 18.06 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 13.59 | optimizer: 98.93 | batch-generator: 7.97
 iteration    28800/  200000 | consumed samples:       921600 | elapsed time per iteration (ms): 257.3 | learning rate: 7.398E-05 | global batch size:    32 | lm loss: 3.853486E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.40 | backward-params-all-reduce: 65.33 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 18.22 | optimizer-clip-main-grad: 7.24 | optimizer-copy-main-to-model-params: 12.05 | optimizer: 96.89 | batch-generator: 7.97
 iteration    28900/  200000 | consumed samples:       924800 | elapsed time per iteration (ms): 255.7 | learning rate: 7.389E-05 | global batch size:    32 | lm loss: 4.087412E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.06 | backward-params-all-reduce: 63.44 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.11 | optimizer-unscale-and-check-inf: 19.30 | optimizer-clip-main-grad: 7.37 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 98.97 | batch-generator: 8.00
 iteration    29000/  200000 | consumed samples:       928000 | elapsed time per iteration (ms): 253.5 | learning rate: 7.380E-05 | global batch size:    32 | lm loss: 3.724194E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.16 | backward-params-all-reduce: 63.14 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.95 | optimizer-unscale-and-check-inf: 18.45 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 97.89 | batch-generator: 8.11
-------------------------------------------------------------------------------------------------
 validation loss at iteration 29000 | lm loss value: 4.319816E-05 | lm loss PPL: 1.000043E+00 | 
 at iteration 29000, match long value: 0.0063117003508792666 | match short value: -0.006408793675016987 
---------------------------------------------------------------------------------------------------------
 iteration    29100/  200000 | consumed samples:       931200 | elapsed time per iteration (ms): 265.7 | learning rate: 7.371E-05 | global batch size:    32 | lm loss: 3.893455E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.49 | backward-params-all-reduce: 64.18 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.88 | optimizer-unscale-and-check-inf: 15.08 | optimizer-clip-main-grad: 7.27 | optimizer-copy-main-to-model-params: 12.19 | optimizer: 94.44 | batch-generator: 14.94
 iteration    29200/  200000 | consumed samples:       934400 | elapsed time per iteration (ms): 255.7 | learning rate: 7.362E-05 | global batch size:    32 | lm loss: 3.572672E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.49 | backward-params-all-reduce: 62.93 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.91 | optimizer-unscale-and-check-inf: 17.96 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.20 | optimizer: 97.42 | batch-generator: 8.04
 iteration    29300/  200000 | consumed samples:       937600 | elapsed time per iteration (ms): 253.9 | learning rate: 7.353E-05 | global batch size:    32 | lm loss: 3.573279E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.09 | backward-params-all-reduce: 61.72 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.53 | optimizer-unscale-and-check-inf: 16.57 | optimizer-clip-main-grad: 7.28 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 96.60 | batch-generator: 7.97
 iteration    29400/  200000 | consumed samples:       940800 | elapsed time per iteration (ms): 255.5 | learning rate: 7.344E-05 | global batch size:    32 | lm loss: 4.214080E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.21 | backward-params-all-reduce: 61.50 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 18.29 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 97.69 | batch-generator: 8.06
 iteration    29500/  200000 | consumed samples:       944000 | elapsed time per iteration (ms): 264.7 | learning rate: 7.335E-05 | global batch size:    32 | lm loss: 3.722520E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.87 | backward-params-all-reduce: 61.83 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.48 | optimizer-unscale-and-check-inf: 17.70 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 13.62 | optimizer: 99.10 | batch-generator: 8.14
-------------------------------------------------------------------------------------------------
 validation loss at iteration 29500 | lm loss value: 4.302030E-05 | lm loss PPL: 1.000043E+00 | 
 at iteration 29500, match long value: 0.0004915401288318183 | match short value: -0.05055275723608059 
--------------------------------------------------------------------------------------------------------
 iteration    29600/  200000 | consumed samples:       947200 | elapsed time per iteration (ms): 271.4 | learning rate: 7.326E-05 | global batch size:    32 | lm loss: 3.462962E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.33 | backward-params-all-reduce: 61.68 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.89 | optimizer-unscale-and-check-inf: 19.06 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.47 | batch-generator: 17.89
 iteration    29700/  200000 | consumed samples:       950400 | elapsed time per iteration (ms): 252.3 | learning rate: 7.316E-05 | global batch size:    32 | lm loss: 3.551946E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.92 | backward-params-all-reduce: 61.86 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 17.91 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 97.27 | batch-generator: 7.96
 iteration    29800/  200000 | consumed samples:       953600 | elapsed time per iteration (ms): 254.9 | learning rate: 7.307E-05 | global batch size:    32 | lm loss: 3.940863E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.05 | backward-params-all-reduce: 62.62 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.95 | optimizer-unscale-and-check-inf: 18.26 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 97.71 | batch-generator: 7.94
 iteration    29900/  200000 | consumed samples:       956800 | elapsed time per iteration (ms): 252.4 | learning rate: 7.298E-05 | global batch size:    32 | lm loss: 3.847142E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.08 | backward-params-all-reduce: 62.40 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.97 | optimizer-unscale-and-check-inf: 17.09 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 96.55 | batch-generator: 7.96
 iteration    30000/  200000 | consumed samples:       960000 | elapsed time per iteration (ms): 254.8 | learning rate: 7.289E-05 | global batch size:    32 | lm loss: 3.643310E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.34 | backward-params-all-reduce: 62.57 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.00 | optimizer-unscale-and-check-inf: 18.57 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.03 | batch-generator: 7.95
-------------------------------------------------------------------------------------------------
 validation loss at iteration 30000 | lm loss value: 4.444430E-05 | lm loss PPL: 1.000044E+00 | 
 at iteration 30000, match long value: -0.004487139401765313 | match short value: 0.03409549700944755 
-------------------------------------------------------------------------------------------------------
 iteration    30100/  200000 | consumed samples:       963200 | elapsed time per iteration (ms): 264.9 | learning rate: 7.280E-05 | global batch size:    32 | lm loss: 3.834319E-05 | loss scale: 134217728.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.31 | backward-params-all-reduce: 62.93 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 18.12 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.19 | optimizer: 97.46 | batch-generator: 13.38
 iteration    30200/  200000 | consumed samples:       966400 | elapsed time per iteration (ms): 262.4 | learning rate: 7.271E-05 | global batch size:    32 | lm loss: 4.030302E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.18 | backward-params-all-reduce: 63.65 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.17 | optimizer-unscale-and-check-inf: 15.25 | optimizer-clip-main-grad: 7.10 | optimizer-copy-main-to-model-params: 13.37 | optimizer: 94.94 | batch-generator: 8.04
 iteration    30300/  200000 | consumed samples:       969600 | elapsed time per iteration (ms): 254.4 | learning rate: 7.262E-05 | global batch size:    32 | lm loss: 3.440072E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.11 | backward-params-all-reduce: 64.26 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.18 | optimizer-unscale-and-check-inf: 16.41 | optimizer-clip-main-grad: 7.27 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 96.05 | batch-generator: 8.07
 iteration    30400/  200000 | consumed samples:       972800 | elapsed time per iteration (ms): 253.3 | learning rate: 7.253E-05 | global batch size:    32 | lm loss: 3.595054E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.10 | backward-params-all-reduce: 62.49 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 17.48 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 96.81 | batch-generator: 8.00
 iteration    30500/  200000 | consumed samples:       976000 | elapsed time per iteration (ms): 255.0 | learning rate: 7.244E-05 | global batch size:    32 | lm loss: 4.148982E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.19 | backward-params-all-reduce: 63.70 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.22 | optimizer-unscale-and-check-inf: 17.07 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 96.78 | batch-generator: 8.01
-------------------------------------------------------------------------------------------------
 validation loss at iteration 30500 | lm loss value: 5.173289E-05 | lm loss PPL: 1.000052E+00 | 
 at iteration 30500, match long value: 0.002517692626743159 | match short value: 0.017959721566599956 
-------------------------------------------------------------------------------------------------------
 iteration    30600/  200000 | consumed samples:       979200 | elapsed time per iteration (ms): 268.1 | learning rate: 7.235E-05 | global batch size:    32 | lm loss: 3.646320E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.43 | backward-params-all-reduce: 64.30 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.95 | optimizer-unscale-and-check-inf: 17.56 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 96.99 | batch-generator: 14.17
 iteration    30700/  200000 | consumed samples:       982400 | elapsed time per iteration (ms): 254.2 | learning rate: 7.226E-05 | global batch size:    32 | lm loss: 3.686936E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.02 | backward-params-all-reduce: 62.66 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.32 | optimizer-unscale-and-check-inf: 16.65 | optimizer-clip-main-grad: 7.28 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 96.44 | batch-generator: 8.01
 iteration    30800/  200000 | consumed samples:       985600 | elapsed time per iteration (ms): 252.6 | learning rate: 7.216E-05 | global batch size:    32 | lm loss: 3.600029E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.12 | backward-params-all-reduce: 62.76 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.07 | optimizer-unscale-and-check-inf: 16.49 | optimizer-clip-main-grad: 7.27 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 96.00 | batch-generator: 8.00
 iteration    30900/  200000 | consumed samples:       988800 | elapsed time per iteration (ms): 264.7 | learning rate: 7.207E-05 | global batch size:    32 | lm loss: 3.801184E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.80 | backward-params-all-reduce: 63.47 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 16.58 | optimizer-clip-main-grad: 7.28 | optimizer-copy-main-to-model-params: 13.58 | optimizer: 97.30 | batch-generator: 7.98
 iteration    31000/  200000 | consumed samples:       992000 | elapsed time per iteration (ms): 256.4 | learning rate: 7.198E-05 | global batch size:    32 | lm loss: 4.098569E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.17 | backward-params-all-reduce: 63.73 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.97 | optimizer-unscale-and-check-inf: 17.31 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 96.74 | batch-generator: 8.02
-------------------------------------------------------------------------------------------------
 validation loss at iteration 31000 | lm loss value: 4.547176E-05 | lm loss PPL: 1.000045E+00 | 
 at iteration 31000, match long value: 0.004422887963325425 | match short value: -0.025151326330330924 
--------------------------------------------------------------------------------------------------------
 iteration    31100/  200000 | consumed samples:       995200 | elapsed time per iteration (ms): 267.2 | learning rate: 7.189E-05 | global batch size:    32 | lm loss: 3.844213E-05 | loss scale: 67108864.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.42 | backward-params-all-reduce: 63.30 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.01 | optimizer-unscale-and-check-inf: 17.48 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.19 | optimizer: 97.00 | batch-generator: 12.95
 iteration    31200/  200000 | consumed samples:       998400 | elapsed time per iteration (ms): 255.4 | learning rate: 7.180E-05 | global batch size:    32 | lm loss: 3.630338E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.15 | backward-params-all-reduce: 62.01 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 17.75 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.08 | batch-generator: 7.98
 iteration    31300/  200000 | consumed samples:      1001600 | elapsed time per iteration (ms): 254.6 | learning rate: 7.171E-05 | global batch size:    32 | lm loss: 3.559090E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.17 | backward-params-all-reduce: 61.89 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.91 | optimizer-unscale-and-check-inf: 17.71 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.11 | batch-generator: 8.03
 iteration    31400/  200000 | consumed samples:      1004800 | elapsed time per iteration (ms): 255.0 | learning rate: 7.162E-05 | global batch size:    32 | lm loss: 3.683938E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.99 | backward-params-all-reduce: 62.10 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.13 | optimizer-unscale-and-check-inf: 16.91 | optimizer-clip-main-grad: 7.27 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 96.49 | batch-generator: 8.06
 iteration    31500/  200000 | consumed samples:      1008000 | elapsed time per iteration (ms): 253.9 | learning rate: 7.153E-05 | global batch size:    32 | lm loss: 3.554221E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.09 | backward-params-all-reduce: 62.76 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.33 | optimizer-unscale-and-check-inf: 16.75 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 96.54 | batch-generator: 8.01
-------------------------------------------------------------------------------------------------
 validation loss at iteration 31500 | lm loss value: 4.092950E-05 | lm loss PPL: 1.000041E+00 | 
 at iteration 31500, match long value: 0.004929102973587837 | match short value: 0.021199103495079596 
-------------------------------------------------------------------------------------------------------
 iteration    31600/  200000 | consumed samples:      1011200 | elapsed time per iteration (ms): 281.1 | learning rate: 7.144E-05 | global batch size:    32 | lm loss: 3.142951E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.36 | backward-params-all-reduce: 63.47 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 20.17 | optimizer-clip-main-grad: 7.43 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 99.58 | batch-generator: 23.39
 iteration    31700/  200000 | consumed samples:      1014400 | elapsed time per iteration (ms): 267.2 | learning rate: 7.134E-05 | global batch size:    32 | lm loss: 3.333776E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.28 | backward-params-all-reduce: 62.92 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.01 | optimizer-unscale-and-check-inf: 19.00 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 13.58 | optimizer: 99.93 | batch-generator: 8.03
 iteration    31800/  200000 | consumed samples:      1017600 | elapsed time per iteration (ms): 260.3 | learning rate: 7.125E-05 | global batch size:    32 | lm loss: 3.277648E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.23 | backward-params-all-reduce: 63.63 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.01 | optimizer-unscale-and-check-inf: 19.58 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 99.09 | batch-generator: 7.98
 iteration    31900/  200000 | consumed samples:      1020800 | elapsed time per iteration (ms): 254.9 | learning rate: 7.116E-05 | global batch size:    32 | lm loss: 3.294502E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.15 | backward-params-all-reduce: 62.37 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.06 | optimizer-unscale-and-check-inf: 18.53 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.09 | batch-generator: 8.02
 iteration    32000/  200000 | consumed samples:      1024000 | elapsed time per iteration (ms): 255.2 | learning rate: 7.107E-05 | global batch size:    32 | lm loss: 3.400393E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.09 | backward-params-all-reduce: 63.30 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.27 | optimizer-unscale-and-check-inf: 17.24 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 96.98 | batch-generator: 7.95
-------------------------------------------------------------------------------------------------
 validation loss at iteration 32000 | lm loss value: 4.127223E-05 | lm loss PPL: 1.000041E+00 | 
 at iteration 32000, match long value: 0.0015459442472887522 | match short value: 0.002560528264935769 
--------------------------------------------------------------------------------------------------------
 iteration    32100/  200000 | consumed samples:      1027200 | elapsed time per iteration (ms): 268.5 | learning rate: 7.098E-05 | global batch size:    32 | lm loss: 3.139248E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.10 | backward-params-all-reduce: 63.92 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.90 | optimizer-unscale-and-check-inf: 18.24 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.62 | batch-generator: 13.60
 iteration    32200/  200000 | consumed samples:      1030400 | elapsed time per iteration (ms): 264.8 | learning rate: 7.089E-05 | global batch size:    32 | lm loss: 3.152473E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.13 | backward-params-all-reduce: 64.06 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.87 | optimizer-unscale-and-check-inf: 18.85 | optimizer-clip-main-grad: 7.21 | optimizer-copy-main-to-model-params: 11.92 | optimizer: 96.89 | batch-generator: 8.02
 iteration    32300/  200000 | consumed samples:      1033600 | elapsed time per iteration (ms): 257.1 | learning rate: 7.080E-05 | global batch size:    32 | lm loss: 3.371204E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.19 | backward-params-all-reduce: 62.89 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.26 | optimizer-unscale-and-check-inf: 19.12 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.87 | batch-generator: 7.96
 iteration    32400/  200000 | consumed samples:      1036800 | elapsed time per iteration (ms): 267.8 | learning rate: 7.071E-05 | global batch size:    32 | lm loss: 3.182555E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.49 | backward-params-all-reduce: 65.01 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.89 | optimizer-unscale-and-check-inf: 16.93 | optimizer-clip-main-grad: 7.26 | optimizer-copy-main-to-model-params: 13.59 | optimizer: 97.68 | batch-generator: 8.02
 iteration    32500/  200000 | consumed samples:      1040000 | elapsed time per iteration (ms): 256.2 | learning rate: 7.062E-05 | global batch size:    32 | lm loss: 3.206884E-05 | loss scale: 134217728.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.01 | backward-params-all-reduce: 63.37 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 19.41 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.77 | batch-generator: 7.99
-------------------------------------------------------------------------------------------------
 validation loss at iteration 32500 | lm loss value: 4.143666E-05 | lm loss PPL: 1.000041E+00 | 
 at iteration 32500, match long value: -0.0002639269952939131 | match short value: -0.025230011048611816 
----------------------------------------------------------------------------------------------------------
 iteration    32600/  200000 | consumed samples:      1043200 | elapsed time per iteration (ms): 266.6 | learning rate: 7.053E-05 | global batch size:    32 | lm loss: 3.220162E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.92 | backward-params-all-reduce: 62.43 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.90 | optimizer-unscale-and-check-inf: 18.75 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.14 | batch-generator: 14.28
 iteration    32700/  200000 | consumed samples:      1046400 | elapsed time per iteration (ms): 255.3 | learning rate: 7.044E-05 | global batch size:    32 | lm loss: 3.348765E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.15 | backward-params-all-reduce: 63.07 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.95 | optimizer-unscale-and-check-inf: 17.24 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 96.71 | batch-generator: 7.95
 iteration    32800/  200000 | consumed samples:      1049600 | elapsed time per iteration (ms): 254.6 | learning rate: 7.035E-05 | global batch size:    32 | lm loss: 3.655143E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.05 | backward-params-all-reduce: 63.22 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.06 | optimizer-unscale-and-check-inf: 17.38 | optimizer-clip-main-grad: 7.19 | optimizer-copy-main-to-model-params: 12.04 | optimizer: 96.22 | batch-generator: 7.90
 iteration    32900/  200000 | consumed samples:      1052800 | elapsed time per iteration (ms): 257.5 | learning rate: 7.025E-05 | global batch size:    32 | lm loss: 3.232338E-05 | loss scale: 67108864.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.92 | backward-params-all-reduce: 63.74 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.07 | optimizer-unscale-and-check-inf: 19.13 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.69 | batch-generator: 7.91
 iteration    33000/  200000 | consumed samples:      1056000 | elapsed time per iteration (ms): 255.7 | learning rate: 7.016E-05 | global batch size:    32 | lm loss: 3.219025E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.44 | backward-params-all-reduce: 62.74 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.01 | optimizer-unscale-and-check-inf: 18.11 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.59 | batch-generator: 7.99
-------------------------------------------------------------------------------------------------
 validation loss at iteration 33000 | lm loss value: 4.260302E-05 | lm loss PPL: 1.000043E+00 | 
 at iteration 33000, match long value: -0.0009070953846630863 | match short value: 0.004038096157541125 
---------------------------------------------------------------------------------------------------------
 iteration    33100/  200000 | consumed samples:      1059200 | elapsed time per iteration (ms): 281.9 | learning rate: 7.007E-05 | global batch size:    32 | lm loss: 3.010757E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.58 | backward-params-all-reduce: 64.25 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.91 | optimizer-unscale-and-check-inf: 16.57 | optimizer-clip-main-grad: 7.28 | optimizer-copy-main-to-model-params: 13.63 | optimizer: 97.40 | batch-generator: 18.68
 iteration    33200/  200000 | consumed samples:      1062400 | elapsed time per iteration (ms): 264.6 | learning rate: 6.998E-05 | global batch size:    32 | lm loss: 3.388016E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.00 | backward-params-all-reduce: 62.96 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 17.28 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 96.58 | batch-generator: 7.90
 iteration    33300/  200000 | consumed samples:      1065600 | elapsed time per iteration (ms): 256.4 | learning rate: 6.989E-05 | global batch size:    32 | lm loss: 3.351020E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.02 | backward-params-all-reduce: 63.43 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.92 | optimizer-unscale-and-check-inf: 17.26 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 96.65 | batch-generator: 7.96
 iteration    33400/  200000 | consumed samples:      1068800 | elapsed time per iteration (ms): 257.1 | learning rate: 6.980E-05 | global batch size:    32 | lm loss: 3.297861E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.01 | backward-params-all-reduce: 63.85 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 17.53 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 96.84 | batch-generator: 7.89
 iteration    33500/  200000 | consumed samples:      1072000 | elapsed time per iteration (ms): 262.1 | learning rate: 6.971E-05 | global batch size:    32 | lm loss: 3.415573E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.54 | backward-params-all-reduce: 63.80 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.90 | optimizer-unscale-and-check-inf: 18.67 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.06 | batch-generator: 7.92
-------------------------------------------------------------------------------------------------
 validation loss at iteration 33500 | lm loss value: 4.065591E-05 | lm loss PPL: 1.000041E+00 | 
 at iteration 33500, match long value: -0.003047295150065658 | match short value: 0.013756657455341461 
--------------------------------------------------------------------------------------------------------
 iteration    33600/  200000 | consumed samples:      1075200 | elapsed time per iteration (ms): 265.3 | learning rate: 6.962E-05 | global batch size:    32 | lm loss: 3.224002E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.17 | backward-params-all-reduce: 63.22 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.87 | optimizer-unscale-and-check-inf: 16.75 | optimizer-clip-main-grad: 7.25 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 96.05 | batch-generator: 13.78
 iteration    33700/  200000 | consumed samples:      1078400 | elapsed time per iteration (ms): 250.9 | learning rate: 6.953E-05 | global batch size:    32 | lm loss: 3.676066E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.20 | backward-params-all-reduce: 63.30 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.89 | optimizer-unscale-and-check-inf: 14.85 | optimizer-clip-main-grad: 7.24 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 94.14 | batch-generator: 8.15
 iteration    33800/  200000 | consumed samples:      1081600 | elapsed time per iteration (ms): 260.2 | learning rate: 6.943E-05 | global batch size:    32 | lm loss: 3.294751E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.62 | backward-params-all-reduce: 63.63 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.98 | optimizer-unscale-and-check-inf: 17.17 | optimizer-clip-main-grad: 7.28 | optimizer-copy-main-to-model-params: 13.63 | optimizer: 98.05 | batch-generator: 7.98
 iteration    33900/  200000 | consumed samples:      1084800 | elapsed time per iteration (ms): 263.9 | learning rate: 6.934E-05 | global batch size:    32 | lm loss: 3.527971E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.59 | backward-params-all-reduce: 63.89 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.90 | optimizer-unscale-and-check-inf: 17.97 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 97.33 | batch-generator: 7.95
 iteration    34000/  200000 | consumed samples:      1088000 | elapsed time per iteration (ms): 253.9 | learning rate: 6.925E-05 | global batch size:    32 | lm loss: 3.764044E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.04 | backward-params-all-reduce: 62.78 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 17.37 | optimizer-clip-main-grad: 7.17 | optimizer-copy-main-to-model-params: 11.92 | optimizer: 95.35 | batch-generator: 7.90
-------------------------------------------------------------------------------------------------
 validation loss at iteration 34000 | lm loss value: 4.275737E-05 | lm loss PPL: 1.000043E+00 | 
 at iteration 34000, match long value: 0.0019668993171689858 | match short value: -0.02137328451137291 
--------------------------------------------------------------------------------------------------------
 iteration    34100/  200000 | consumed samples:      1091200 | elapsed time per iteration (ms): 265.5 | learning rate: 6.916E-05 | global batch size:    32 | lm loss: 3.151519E-05 | loss scale: 67108864.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.06 | backward-params-all-reduce: 62.66 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 16.52 | optimizer-clip-main-grad: 7.28 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 95.82 | batch-generator: 13.71
 iteration    34200/  200000 | consumed samples:      1094400 | elapsed time per iteration (ms): 256.6 | learning rate: 6.907E-05 | global batch size:    32 | lm loss: 3.133670E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.17 | backward-params-all-reduce: 62.45 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 17.31 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 96.63 | batch-generator: 7.99
 iteration    34300/  200000 | consumed samples:      1097600 | elapsed time per iteration (ms): 252.6 | learning rate: 6.898E-05 | global batch size:    32 | lm loss: 3.148426E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.23 | backward-params-all-reduce: 62.77 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 16.25 | optimizer-clip-main-grad: 7.25 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 95.52 | batch-generator: 7.99
 iteration    34400/  200000 | consumed samples:      1100800 | elapsed time per iteration (ms): 255.6 | learning rate: 6.889E-05 | global batch size:    32 | lm loss: 3.434450E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.39 | backward-params-all-reduce: 62.12 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.97 | optimizer-unscale-and-check-inf: 17.48 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 96.91 | batch-generator: 7.91
 iteration    34500/  200000 | consumed samples:      1104000 | elapsed time per iteration (ms): 258.8 | learning rate: 6.880E-05 | global batch size:    32 | lm loss: 3.291955E-05 | loss scale: 67108864.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.14 | backward-params-all-reduce: 64.18 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.01 | optimizer-unscale-and-check-inf: 16.98 | optimizer-clip-main-grad: 7.27 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 96.42 | batch-generator: 7.89
-------------------------------------------------------------------------------------------------
 validation loss at iteration 34500 | lm loss value: 4.259477E-05 | lm loss PPL: 1.000043E+00 | 
 at iteration 34500, match long value: -0.0006036146117127879 | match short value: 0.03609495409376255 
--------------------------------------------------------------------------------------------------------
 iteration    34600/  200000 | consumed samples:      1107200 | elapsed time per iteration (ms): 285.6 | learning rate: 6.871E-05 | global batch size:    32 | lm loss: 3.267798E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.85 | backward-params-all-reduce: 63.61 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.52 | optimizer-unscale-and-check-inf: 16.31 | optimizer-clip-main-grad: 7.25 | optimizer-copy-main-to-model-params: 13.56 | optimizer: 97.63 | batch-generator: 16.06
 iteration    34700/  200000 | consumed samples:      1110400 | elapsed time per iteration (ms): 260.3 | learning rate: 6.862E-05 | global batch size:    32 | lm loss: 3.203739E-05 | loss scale: 67108864.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.96 | backward-params-all-reduce: 62.62 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.10 | optimizer-unscale-and-check-inf: 17.81 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.38 | batch-generator: 7.87
 iteration    34800/  200000 | consumed samples:      1113600 | elapsed time per iteration (ms): 256.0 | learning rate: 6.853E-05 | global batch size:    32 | lm loss: 3.439731E-05 | loss scale: 67108864.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.00 | backward-params-all-reduce: 64.51 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.96 | optimizer-unscale-and-check-inf: 16.71 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 96.10 | batch-generator: 7.86
 iteration    34900/  200000 | consumed samples:      1116800 | elapsed time per iteration (ms): 258.3 | learning rate: 6.843E-05 | global batch size:    32 | lm loss: 3.239527E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.99 | backward-params-all-reduce: 64.03 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.00 | optimizer-unscale-and-check-inf: 17.83 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.32 | batch-generator: 7.92
 iteration    35000/  200000 | consumed samples:      1120000 | elapsed time per iteration (ms): 255.0 | learning rate: 6.834E-05 | global batch size:    32 | lm loss: 3.434856E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.01 | backward-params-all-reduce: 63.80 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.64 | optimizer-unscale-and-check-inf: 16.98 | optimizer-clip-main-grad: 7.27 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.05 | batch-generator: 7.91
-------------------------------------------------------------------------------------------------
 validation loss at iteration 35000 | lm loss value: 4.259397E-05 | lm loss PPL: 1.000043E+00 | 
 at iteration 35000, match long value: 0.0028489981917303167 | match short value: 0.007401311458788863 
--------------------------------------------------------------------------------------------------------
 iteration    35100/  200000 | consumed samples:      1123200 | elapsed time per iteration (ms): 269.7 | learning rate: 6.825E-05 | global batch size:    32 | lm loss: 3.204944E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.12 | backward-params-all-reduce: 63.01 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.96 | optimizer-unscale-and-check-inf: 18.47 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.93 | batch-generator: 18.11
 iteration    35200/  200000 | consumed samples:      1126400 | elapsed time per iteration (ms): 261.0 | learning rate: 6.816E-05 | global batch size:    32 | lm loss: 3.334504E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.57 | backward-params-all-reduce: 63.03 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.04 | optimizer-unscale-and-check-inf: 19.26 | optimizer-clip-main-grad: 7.19 | optimizer-copy-main-to-model-params: 11.93 | optimizer: 97.47 | batch-generator: 8.14
 iteration    35300/  200000 | consumed samples:      1129600 | elapsed time per iteration (ms): 266.5 | learning rate: 6.807E-05 | global batch size:    32 | lm loss: 2.821159E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.94 | backward-params-all-reduce: 62.50 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.01 | optimizer-unscale-and-check-inf: 19.47 | optimizer-clip-main-grad: 7.37 | optimizer-copy-main-to-model-params: 13.66 | optimizer: 100.52 | batch-generator: 8.19
 iteration    35400/  200000 | consumed samples:      1132800 | elapsed time per iteration (ms): 253.0 | learning rate: 6.798E-05 | global batch size:    32 | lm loss: 2.959262E-05 | loss scale: 33554432.0 | grad norm: 0.002 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.11 | backward-params-all-reduce: 62.86 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.92 | optimizer-unscale-and-check-inf: 19.48 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.06 | optimizer: 98.32 | batch-generator: 8.15
 iteration    35500/  200000 | consumed samples:      1136000 | elapsed time per iteration (ms): 256.4 | learning rate: 6.789E-05 | global batch size:    32 | lm loss: 2.736695E-05 | loss scale: 33554432.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.41 | backward-params-all-reduce: 62.95 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.89 | optimizer-unscale-and-check-inf: 20.67 | optimizer-clip-main-grad: 7.44 | optimizer-copy-main-to-model-params: 12.19 | optimizer: 100.20 | batch-generator: 8.22
-------------------------------------------------------------------------------------------------
 validation loss at iteration 35500 | lm loss value: 3.809806E-05 | lm loss PPL: 1.000038E+00 | 
 at iteration 35500, match long value: -0.0036487534877488108 | match short value: -0.02757988935951948 
---------------------------------------------------------------------------------------------------------
 iteration    35600/  200000 | consumed samples:      1139200 | elapsed time per iteration (ms): 264.6 | learning rate: 6.780E-05 | global batch size:    32 | lm loss: 3.003228E-05 | loss scale: 33554432.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.27 | backward-params-all-reduce: 63.45 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.11 | optimizer-unscale-and-check-inf: 18.98 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.59 | batch-generator: 15.38
 iteration    35700/  200000 | consumed samples:      1142400 | elapsed time per iteration (ms): 255.7 | learning rate: 6.771E-05 | global batch size:    32 | lm loss: 2.854665E-05 | loss scale: 33554432.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.74 | backward-params-all-reduce: 63.79 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.41 | optimizer-unscale-and-check-inf: 17.72 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 97.67 | batch-generator: 8.03
 iteration    35800/  200000 | consumed samples:      1145600 | elapsed time per iteration (ms): 253.3 | learning rate: 6.762E-05 | global batch size:    32 | lm loss: 2.866581E-05 | loss scale: 33554432.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.02 | backward-params-all-reduce: 61.96 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 19.12 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.51 | batch-generator: 8.09
 iteration    35900/  200000 | consumed samples:      1148800 | elapsed time per iteration (ms): 258.6 | learning rate: 6.753E-05 | global batch size:    32 | lm loss: 2.845361E-05 | loss scale: 33554432.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.97 | backward-params-all-reduce: 62.75 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.05 | optimizer-unscale-and-check-inf: 19.47 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 99.07 | batch-generator: 8.05
 iteration    36000/  200000 | consumed samples:      1152000 | elapsed time per iteration (ms): 257.7 | learning rate: 6.743E-05 | global batch size:    32 | lm loss: 3.035065E-05 | loss scale: 33554432.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.37 | backward-params-all-reduce: 61.86 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.88 | optimizer-unscale-and-check-inf: 18.41 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 13.63 | optimizer: 99.29 | batch-generator: 8.17
-------------------------------------------------------------------------------------------------
 validation loss at iteration 36000 | lm loss value: 4.440107E-05 | lm loss PPL: 1.000044E+00 | 
 at iteration 36000, match long value: 0.00752077162823667 | match short value: 0.01203310933986626 
-----------------------------------------------------------------------------------------------------
 iteration    36100/  200000 | consumed samples:      1155200 | elapsed time per iteration (ms): 275.0 | learning rate: 6.734E-05 | global batch size:    32 | lm loss: 3.062215E-05 | loss scale: 33554432.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.69 | backward-params-all-reduce: 62.79 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.00 | optimizer-unscale-and-check-inf: 18.48 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 98.02 | batch-generator: 19.97
 iteration    36200/  200000 | consumed samples:      1158400 | elapsed time per iteration (ms): 254.5 | learning rate: 6.725E-05 | global batch size:    32 | lm loss: 2.829214E-05 | loss scale: 33554432.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.97 | backward-params-all-reduce: 62.47 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 18.85 | optimizer-clip-main-grad: 7.37 | optimizer-copy-main-to-model-params: 12.19 | optimizer: 98.27 | batch-generator: 8.11
 iteration    36300/  200000 | consumed samples:      1161600 | elapsed time per iteration (ms): 254.8 | learning rate: 6.716E-05 | global batch size:    32 | lm loss: 2.944786E-05 | loss scale: 33554432.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.03 | backward-params-all-reduce: 63.23 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.18 | optimizer-unscale-and-check-inf: 18.03 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.73 | batch-generator: 8.09
 iteration    36400/  200000 | consumed samples:      1164800 | elapsed time per iteration (ms): 254.0 | learning rate: 6.707E-05 | global batch size:    32 | lm loss: 3.157380E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.94 | backward-params-all-reduce: 62.14 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.03 | optimizer-unscale-and-check-inf: 19.00 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.53 | batch-generator: 8.00
 iteration    36500/  200000 | consumed samples:      1168000 | elapsed time per iteration (ms): 254.2 | learning rate: 6.698E-05 | global batch size:    32 | lm loss: 2.877435E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.97 | backward-params-all-reduce: 62.02 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.01 | optimizer-unscale-and-check-inf: 18.50 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.04 | batch-generator: 8.06
-------------------------------------------------------------------------------------------------
 validation loss at iteration 36500 | lm loss value: 4.012821E-05 | lm loss PPL: 1.000040E+00 | 
 at iteration 36500, match long value: -0.00013849083817188556 | match short value: 0.010475187257337062 
----------------------------------------------------------------------------------------------------------
 iteration    36600/  200000 | consumed samples:      1171200 | elapsed time per iteration (ms): 264.5 | learning rate: 6.689E-05 | global batch size:    32 | lm loss: 2.784973E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.12 | backward-params-all-reduce: 62.59 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.90 | optimizer-unscale-and-check-inf: 18.07 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.47 | batch-generator: 14.25
 iteration    36700/  200000 | consumed samples:      1174400 | elapsed time per iteration (ms): 254.6 | learning rate: 6.680E-05 | global batch size:    32 | lm loss: 2.973009E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.04 | backward-params-all-reduce: 63.01 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.90 | optimizer-unscale-and-check-inf: 18.64 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 98.08 | batch-generator: 8.07
 iteration    36800/  200000 | consumed samples:      1177600 | elapsed time per iteration (ms): 263.5 | learning rate: 6.671E-05 | global batch size:    32 | lm loss: 2.851682E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.70 | backward-params-all-reduce: 62.99 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.89 | optimizer-unscale-and-check-inf: 17.96 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 13.66 | optimizer: 98.83 | batch-generator: 8.11
 iteration    36900/  200000 | consumed samples:      1180800 | elapsed time per iteration (ms): 258.9 | learning rate: 6.662E-05 | global batch size:    32 | lm loss: 2.942163E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.19 | backward-params-all-reduce: 63.42 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.17 | optimizer-unscale-and-check-inf: 19.37 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.19 | optimizer: 99.10 | batch-generator: 8.08
 iteration    37000/  200000 | consumed samples:      1184000 | elapsed time per iteration (ms): 252.6 | learning rate: 6.652E-05 | global batch size:    32 | lm loss: 2.933144E-05 | loss scale: 67108864.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.02 | backward-params-all-reduce: 62.41 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 18.60 | optimizer-clip-main-grad: 7.37 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 98.05 | batch-generator: 8.12
-------------------------------------------------------------------------------------------------
 validation loss at iteration 37000 | lm loss value: 3.812578E-05 | lm loss PPL: 1.000038E+00 | 
 at iteration 37000, match long value: 0.0045061857653554025 | match short value: 0.015071984601083967 
--------------------------------------------------------------------------------------------------------
 iteration    37100/  200000 | consumed samples:      1187200 | elapsed time per iteration (ms): 265.2 | learning rate: 6.643E-05 | global batch size:    32 | lm loss: 2.769348E-05 | loss scale: 67108864.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.16 | backward-params-all-reduce: 63.70 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.08 | optimizer-unscale-and-check-inf: 16.77 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 96.37 | batch-generator: 14.63
 iteration    37200/  200000 | consumed samples:      1190400 | elapsed time per iteration (ms): 256.1 | learning rate: 6.634E-05 | global batch size:    32 | lm loss: 3.136046E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.01 | backward-params-all-reduce: 63.65 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.13 | optimizer-unscale-and-check-inf: 18.50 | optimizer-clip-main-grad: 7.39 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 98.22 | batch-generator: 8.08
 iteration    37300/  200000 | consumed samples:      1193600 | elapsed time per iteration (ms): 256.1 | learning rate: 6.625E-05 | global batch size:    32 | lm loss: 2.924608E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.01 | backward-params-all-reduce: 63.15 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.91 | optimizer-unscale-and-check-inf: 18.41 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 97.84 | batch-generator: 8.05
 iteration    37400/  200000 | consumed samples:      1196800 | elapsed time per iteration (ms): 251.8 | learning rate: 6.616E-05 | global batch size:    32 | lm loss: 3.168648E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.01 | backward-params-all-reduce: 62.25 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.87 | optimizer-unscale-and-check-inf: 17.47 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 96.85 | batch-generator: 8.12
 iteration    37500/  200000 | consumed samples:      1200000 | elapsed time per iteration (ms): 266.3 | learning rate: 6.607E-05 | global batch size:    32 | lm loss: 2.820507E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.79 | backward-params-all-reduce: 62.13 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.98 | optimizer-unscale-and-check-inf: 17.71 | optimizer-clip-main-grad: 7.38 | optimizer-copy-main-to-model-params: 13.60 | optimizer: 98.69 | batch-generator: 8.16
-------------------------------------------------------------------------------------------------
 validation loss at iteration 37500 | lm loss value: 3.615103E-05 | lm loss PPL: 1.000036E+00 | 
 at iteration 37500, match long value: -0.00021214729783175688 | match short value: -0.003056605530043139 
-----------------------------------------------------------------------------------------------------------
 iteration    37600/  200000 | consumed samples:      1203200 | elapsed time per iteration (ms): 272.1 | learning rate: 6.598E-05 | global batch size:    32 | lm loss: 2.764961E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.06 | backward-params-all-reduce: 64.07 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.17 | optimizer-unscale-and-check-inf: 17.91 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 97.65 | batch-generator: 18.16
 iteration    37700/  200000 | consumed samples:      1206400 | elapsed time per iteration (ms): 255.5 | learning rate: 6.589E-05 | global batch size:    32 | lm loss: 2.841383E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.77 | backward-params-all-reduce: 63.73 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.04 | optimizer-unscale-and-check-inf: 18.14 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 97.69 | batch-generator: 8.07
 iteration    37800/  200000 | consumed samples:      1209600 | elapsed time per iteration (ms): 256.3 | learning rate: 6.580E-05 | global batch size:    32 | lm loss: 2.871501E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.96 | backward-params-all-reduce: 64.07 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 18.41 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.19 | optimizer: 97.79 | batch-generator: 7.99
 iteration    37900/  200000 | consumed samples:      1212800 | elapsed time per iteration (ms): 255.9 | learning rate: 6.570E-05 | global batch size:    32 | lm loss: 2.792990E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.74 | backward-params-all-reduce: 64.08 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.01 | optimizer-unscale-and-check-inf: 17.69 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.17 | batch-generator: 7.87
 iteration    38000/  200000 | consumed samples:      1216000 | elapsed time per iteration (ms): 256.5 | learning rate: 6.561E-05 | global batch size:    32 | lm loss: 3.030587E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.74 | backward-params-all-reduce: 63.05 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 18.88 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 98.24 | batch-generator: 8.02
-------------------------------------------------------------------------------------------------
 validation loss at iteration 38000 | lm loss value: 4.069739E-05 | lm loss PPL: 1.000041E+00 | 
 at iteration 38000, match long value: -0.0021961330510732145 | match short value: 0.018092463888081143 
---------------------------------------------------------------------------------------------------------
 iteration    38100/  200000 | consumed samples:      1219200 | elapsed time per iteration (ms): 267.6 | learning rate: 6.552E-05 | global batch size:    32 | lm loss: 3.230189E-05 | loss scale: 134217728.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.38 | backward-params-all-reduce: 64.78 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.91 | optimizer-unscale-and-check-inf: 18.24 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 97.62 | batch-generator: 13.84
 iteration    38200/  200000 | consumed samples:      1222400 | elapsed time per iteration (ms): 263.0 | learning rate: 6.543E-05 | global batch size:    32 | lm loss: 3.155618E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.34 | backward-params-all-reduce: 64.99 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.97 | optimizer-unscale-and-check-inf: 17.33 | optimizer-clip-main-grad: 7.27 | optimizer-copy-main-to-model-params: 13.59 | optimizer: 98.17 | batch-generator: 7.96
 iteration    38300/  200000 | consumed samples:      1225600 | elapsed time per iteration (ms): 259.5 | learning rate: 6.534E-05 | global batch size:    32 | lm loss: 2.806135E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.31 | backward-params-all-reduce: 62.95 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.88 | optimizer-unscale-and-check-inf: 18.19 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.55 | batch-generator: 8.00
 iteration    38400/  200000 | consumed samples:      1228800 | elapsed time per iteration (ms): 254.4 | learning rate: 6.525E-05 | global batch size:    32 | lm loss: 2.695119E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.04 | backward-params-all-reduce: 63.07 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.91 | optimizer-unscale-and-check-inf: 18.43 | optimizer-clip-main-grad: 7.19 | optimizer-copy-main-to-model-params: 11.93 | optimizer: 96.50 | batch-generator: 8.03
 iteration    38500/  200000 | consumed samples:      1232000 | elapsed time per iteration (ms): 255.4 | learning rate: 6.516E-05 | global batch size:    32 | lm loss: 2.846983E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.13 | backward-params-all-reduce: 63.29 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 18.39 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.77 | batch-generator: 8.00
-------------------------------------------------------------------------------------------------
 validation loss at iteration 38500 | lm loss value: 3.891420E-05 | lm loss PPL: 1.000039E+00 | 
 at iteration 38500, match long value: 0.0041227360574253805 | match short value: 0.012693922200946344 
--------------------------------------------------------------------------------------------------------
 iteration    38600/  200000 | consumed samples:      1235200 | elapsed time per iteration (ms): 272.0 | learning rate: 6.507E-05 | global batch size:    32 | lm loss: 2.823740E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.22 | backward-params-all-reduce: 62.44 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.94 | optimizer-unscale-and-check-inf: 19.63 | optimizer-clip-main-grad: 7.39 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 99.12 | batch-generator: 18.21
 iteration    38700/  200000 | consumed samples:      1238400 | elapsed time per iteration (ms): 252.5 | learning rate: 6.498E-05 | global batch size:    32 | lm loss: 2.669383E-05 | loss scale: 67108864.0 | grad norm: 0.002 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.26 | backward-params-all-reduce: 61.62 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.33 | optimizer-unscale-and-check-inf: 19.58 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.06 | optimizer: 98.82 | batch-generator: 8.21
 iteration    38800/  200000 | consumed samples:      1241600 | elapsed time per iteration (ms): 256.2 | learning rate: 6.489E-05 | global batch size:    32 | lm loss: 2.642192E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.18 | backward-params-all-reduce: 63.19 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.20 | optimizer-unscale-and-check-inf: 20.36 | optimizer-clip-main-grad: 7.39 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 100.15 | batch-generator: 8.10
 iteration    38900/  200000 | consumed samples:      1244800 | elapsed time per iteration (ms): 254.0 | learning rate: 6.480E-05 | global batch size:    32 | lm loss: 2.538231E-05 | loss scale: 67108864.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.15 | backward-params-all-reduce: 61.55 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.21 | optimizer-unscale-and-check-inf: 20.05 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 99.79 | batch-generator: 8.15
 iteration    39000/  200000 | consumed samples:      1248000 | elapsed time per iteration (ms): 268.6 | learning rate: 6.470E-05 | global batch size:    32 | lm loss: 2.641573E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.65 | backward-params-all-reduce: 64.29 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.89 | optimizer-unscale-and-check-inf: 18.29 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 13.62 | optimizer: 99.13 | batch-generator: 7.99
-------------------------------------------------------------------------------------------------
 validation loss at iteration 39000 | lm loss value: 4.245459E-05 | lm loss PPL: 1.000042E+00 | 
 at iteration 39000, match long value: 0.006843075794085381 | match short value: 0.017316071843765296 
-------------------------------------------------------------------------------------------------------
 iteration    39100/  200000 | consumed samples:      1251200 | elapsed time per iteration (ms): 272.4 | learning rate: 6.461E-05 | global batch size:    32 | lm loss: 2.586770E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.27 | backward-params-all-reduce: 63.20 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 20.13 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 99.52 | batch-generator: 17.89
 iteration    39200/  200000 | consumed samples:      1254400 | elapsed time per iteration (ms): 254.8 | learning rate: 6.452E-05 | global batch size:    32 | lm loss: 2.534346E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.58 | backward-params-all-reduce: 62.05 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.73 | optimizer-unscale-and-check-inf: 17.61 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.86 | batch-generator: 8.00
 iteration    39300/  200000 | consumed samples:      1257600 | elapsed time per iteration (ms): 255.1 | learning rate: 6.443E-05 | global batch size:    32 | lm loss: 2.804387E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.38 | backward-params-all-reduce: 62.05 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.16 | optimizer-unscale-and-check-inf: 20.02 | optimizer-clip-main-grad: 7.39 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 99.76 | batch-generator: 8.04
 iteration    39400/  200000 | consumed samples:      1260800 | elapsed time per iteration (ms): 255.0 | learning rate: 6.434E-05 | global batch size:    32 | lm loss: 2.587690E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.17 | backward-params-all-reduce: 62.68 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.95 | optimizer-unscale-and-check-inf: 19.61 | optimizer-clip-main-grad: 7.38 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 99.13 | batch-generator: 8.14
 iteration    39500/  200000 | consumed samples:      1264000 | elapsed time per iteration (ms): 255.1 | learning rate: 6.425E-05 | global batch size:    32 | lm loss: 2.748725E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.04 | backward-params-all-reduce: 62.06 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.90 | optimizer-unscale-and-check-inf: 19.66 | optimizer-clip-main-grad: 7.37 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 99.12 | batch-generator: 8.13
-------------------------------------------------------------------------------------------------
 validation loss at iteration 39500 | lm loss value: 3.644488E-05 | lm loss PPL: 1.000036E+00 | 
 at iteration 39500, match long value: 0.009986657675700193 | match short value: 0.04223328403342333 
------------------------------------------------------------------------------------------------------
 iteration    39600/  200000 | consumed samples:      1267200 | elapsed time per iteration (ms): 266.6 | learning rate: 6.416E-05 | global batch size:    32 | lm loss: 2.548719E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.71 | backward-params-all-reduce: 63.21 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.01 | optimizer-unscale-and-check-inf: 19.17 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.71 | batch-generator: 14.92
 iteration    39700/  200000 | consumed samples:      1270400 | elapsed time per iteration (ms): 267.3 | learning rate: 6.407E-05 | global batch size:    32 | lm loss: 2.740388E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.61 | backward-params-all-reduce: 63.32 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 18.87 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 13.62 | optimizer: 99.68 | batch-generator: 8.19
 iteration    39800/  200000 | consumed samples:      1273600 | elapsed time per iteration (ms): 257.0 | learning rate: 6.398E-05 | global batch size:    32 | lm loss: 2.560274E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.15 | backward-params-all-reduce: 63.04 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.99 | optimizer-unscale-and-check-inf: 19.54 | optimizer-clip-main-grad: 7.28 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.98 | batch-generator: 8.09
 iteration    39900/  200000 | consumed samples:      1276800 | elapsed time per iteration (ms): 256.5 | learning rate: 6.389E-05 | global batch size:    32 | lm loss: 2.660245E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.19 | backward-params-all-reduce: 63.34 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.90 | optimizer-unscale-and-check-inf: 19.20 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 98.61 | batch-generator: 8.09
 iteration    40000/  200000 | consumed samples:      1280000 | elapsed time per iteration (ms): 254.2 | learning rate: 6.379E-05 | global batch size:    32 | lm loss: 2.546391E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.07 | backward-params-all-reduce: 63.47 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.06 | optimizer-unscale-and-check-inf: 17.72 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 97.26 | batch-generator: 8.00
-------------------------------------------------------------------------------------------------
 validation loss at iteration 40000 | lm loss value: 3.906448E-05 | lm loss PPL: 1.000039E+00 | 
 at iteration 40000, match long value: 0.003201357793136429 | match short value: 0.034763293738524076 
-------------------------------------------------------------------------------------------------------
 iteration    40100/  200000 | consumed samples:      1283200 | elapsed time per iteration (ms): 267.1 | learning rate: 6.370E-05 | global batch size:    32 | lm loss: 2.449156E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.43 | backward-params-all-reduce: 62.56 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 19.65 | optimizer-clip-main-grad: 7.38 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 99.05 | batch-generator: 14.12
 iteration    40200/  200000 | consumed samples:      1286400 | elapsed time per iteration (ms): 256.6 | learning rate: 6.361E-05 | global batch size:    32 | lm loss: 2.777966E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.14 | backward-params-all-reduce: 62.33 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.95 | optimizer-unscale-and-check-inf: 19.28 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.74 | batch-generator: 7.95
 iteration    40300/  200000 | consumed samples:      1289600 | elapsed time per iteration (ms): 253.8 | learning rate: 6.352E-05 | global batch size:    32 | lm loss: 2.678000E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.27 | backward-params-all-reduce: 63.67 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 18.49 | optimizer-clip-main-grad: 7.21 | optimizer-copy-main-to-model-params: 11.94 | optimizer: 96.56 | batch-generator: 8.17
 iteration    40400/  200000 | consumed samples:      1292800 | elapsed time per iteration (ms): 262.7 | learning rate: 6.343E-05 | global batch size:    32 | lm loss: 2.685711E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.63 | backward-params-all-reduce: 62.61 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.90 | optimizer-unscale-and-check-inf: 18.39 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 13.63 | optimizer: 99.26 | batch-generator: 8.14
 iteration    40500/  200000 | consumed samples:      1296000 | elapsed time per iteration (ms): 262.7 | learning rate: 6.334E-05 | global batch size:    32 | lm loss: 2.612896E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.44 | backward-params-all-reduce: 64.26 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.25 | optimizer-unscale-and-check-inf: 18.01 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 97.73 | batch-generator: 8.01
-------------------------------------------------------------------------------------------------
 validation loss at iteration 40500 | lm loss value: 3.768957E-05 | lm loss PPL: 1.000038E+00 | 
 at iteration 40500, match long value: -0.0016035846453054468 | match short value: -0.04575879611198539 
---------------------------------------------------------------------------------------------------------
 iteration    40600/  200000 | consumed samples:      1299200 | elapsed time per iteration (ms): 270.8 | learning rate: 6.325E-05 | global batch size:    32 | lm loss: 2.481213E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.09 | backward-params-all-reduce: 63.95 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 18.95 | optimizer-clip-main-grad: 7.37 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.34 | batch-generator: 17.30
 iteration    40700/  200000 | consumed samples:      1302400 | elapsed time per iteration (ms): 253.8 | learning rate: 6.316E-05 | global batch size:    32 | lm loss: 2.576379E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.03 | backward-params-all-reduce: 63.61 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 18.33 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 97.73 | batch-generator: 8.11
 iteration    40800/  200000 | consumed samples:      1305600 | elapsed time per iteration (ms): 254.3 | learning rate: 6.307E-05 | global batch size:    32 | lm loss: 2.630770E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.01 | backward-params-all-reduce: 63.20 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.87 | optimizer-unscale-and-check-inf: 17.98 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 97.34 | batch-generator: 8.05
 iteration    40900/  200000 | consumed samples:      1308800 | elapsed time per iteration (ms): 255.5 | learning rate: 6.298E-05 | global batch size:    32 | lm loss: 2.466213E-05 | loss scale: 67108864.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.21 | backward-params-all-reduce: 62.77 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.96 | optimizer-unscale-and-check-inf: 18.32 | optimizer-clip-main-grad: 7.38 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 97.85 | batch-generator: 8.18
 iteration    41000/  200000 | consumed samples:      1312000 | elapsed time per iteration (ms): 258.5 | learning rate: 6.288E-05 | global batch size:    32 | lm loss: 2.877675E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.05 | backward-params-all-reduce: 63.95 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.02 | optimizer-unscale-and-check-inf: 18.85 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.39 | batch-generator: 8.06
-------------------------------------------------------------------------------------------------
 validation loss at iteration 41000 | lm loss value: 3.772340E-05 | lm loss PPL: 1.000038E+00 | 
 at iteration 41000, match long value: -0.0020503944133692235 | match short value: 0.0014777613896196677 
----------------------------------------------------------------------------------------------------------
 iteration    41100/  200000 | consumed samples:      1315200 | elapsed time per iteration (ms): 266.8 | learning rate: 6.279E-05 | global batch size:    32 | lm loss: 2.596077E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.21 | backward-params-all-reduce: 62.91 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 18.74 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 12.20 | optimizer: 98.19 | batch-generator: 15.21
 iteration    41200/  200000 | consumed samples:      1318400 | elapsed time per iteration (ms): 265.7 | learning rate: 6.270E-05 | global batch size:    32 | lm loss: 2.553734E-05 | loss scale: 67108864.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.49 | backward-params-all-reduce: 63.55 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.88 | optimizer-unscale-and-check-inf: 17.44 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 13.73 | optimizer: 98.40 | batch-generator: 8.21
 iteration    41300/  200000 | consumed samples:      1321600 | elapsed time per iteration (ms): 255.1 | learning rate: 6.261E-05 | global batch size:    32 | lm loss: 2.662404E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.43 | backward-params-all-reduce: 63.39 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.91 | optimizer-unscale-and-check-inf: 18.03 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.46 | batch-generator: 8.01
 iteration    41400/  200000 | consumed samples:      1324800 | elapsed time per iteration (ms): 257.3 | learning rate: 6.252E-05 | global batch size:    32 | lm loss: 2.447345E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.14 | backward-params-all-reduce: 62.92 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.91 | optimizer-unscale-and-check-inf: 18.66 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.20 | optimizer: 98.12 | batch-generator: 8.01
 iteration    41500/  200000 | consumed samples:      1328000 | elapsed time per iteration (ms): 255.8 | learning rate: 6.243E-05 | global batch size:    32 | lm loss: 2.434246E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.07 | backward-params-all-reduce: 61.71 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.06 | optimizer-unscale-and-check-inf: 19.63 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 99.21 | batch-generator: 8.04
-------------------------------------------------------------------------------------------------
 validation loss at iteration 41500 | lm loss value: 3.589313E-05 | lm loss PPL: 1.000036E+00 | 
 at iteration 41500, match long value: -0.004300155808991446 | match short value: -0.003164561504171367 
---------------------------------------------------------------------------------------------------------
 iteration    41600/  200000 | consumed samples:      1331200 | elapsed time per iteration (ms): 267.4 | learning rate: 6.234E-05 | global batch size:    32 | lm loss: 2.507734E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.11 | backward-params-all-reduce: 62.75 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 17.45 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 96.77 | batch-generator: 15.00
 iteration    41700/  200000 | consumed samples:      1334400 | elapsed time per iteration (ms): 253.1 | learning rate: 6.225E-05 | global batch size:    32 | lm loss: 2.493916E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.06 | backward-params-all-reduce: 61.94 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 17.18 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 96.54 | batch-generator: 8.03
 iteration    41800/  200000 | consumed samples:      1337600 | elapsed time per iteration (ms): 254.6 | learning rate: 6.216E-05 | global batch size:    32 | lm loss: 2.477608E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.07 | backward-params-all-reduce: 62.48 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.88 | optimizer-unscale-and-check-inf: 18.00 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.35 | batch-generator: 7.92
 iteration    41900/  200000 | consumed samples:      1340800 | elapsed time per iteration (ms): 265.4 | learning rate: 6.207E-05 | global batch size:    32 | lm loss: 2.540930E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.98 | backward-params-all-reduce: 62.76 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.95 | optimizer-unscale-and-check-inf: 17.43 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 13.69 | optimizer: 98.37 | batch-generator: 8.05
 iteration    42000/  200000 | consumed samples:      1344000 | elapsed time per iteration (ms): 253.9 | learning rate: 6.197E-05 | global batch size:    32 | lm loss: 2.594449E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.36 | backward-params-all-reduce: 63.60 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 17.44 | optimizer-clip-main-grad: 7.26 | optimizer-copy-main-to-model-params: 12.04 | optimizer: 96.11 | batch-generator: 7.99
-------------------------------------------------------------------------------------------------
 validation loss at iteration 42000 | lm loss value: 4.662359E-05 | lm loss PPL: 1.000047E+00 | 
 at iteration 42000, match long value: -0.001480545448295924 | match short value: -0.013809550169659148 
---------------------------------------------------------------------------------------------------------
 iteration    42100/  200000 | consumed samples:      1347200 | elapsed time per iteration (ms): 277.1 | learning rate: 6.188E-05 | global batch size:    32 | lm loss: 2.523232E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.50 | backward-params-all-reduce: 64.37 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.83 | optimizer-unscale-and-check-inf: 18.30 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 97.61 | batch-generator: 22.15
 iteration    42200/  200000 | consumed samples:      1350400 | elapsed time per iteration (ms): 259.4 | learning rate: 6.179E-05 | global batch size:    32 | lm loss: 2.612299E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.06 | backward-params-all-reduce: 65.83 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.83 | optimizer-unscale-and-check-inf: 19.13 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.46 | batch-generator: 7.87
 iteration    42300/  200000 | consumed samples:      1353600 | elapsed time per iteration (ms): 265.9 | learning rate: 6.170E-05 | global batch size:    32 | lm loss: 2.291540E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.86 | backward-params-all-reduce: 64.44 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 19.34 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.68 | batch-generator: 7.97
 iteration    42400/  200000 | consumed samples:      1356800 | elapsed time per iteration (ms): 258.9 | learning rate: 6.161E-05 | global batch size:    32 | lm loss: 2.214965E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.16 | backward-params-all-reduce: 63.86 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.94 | optimizer-unscale-and-check-inf: 19.48 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.90 | batch-generator: 8.02
 iteration    42500/  200000 | consumed samples:      1360000 | elapsed time per iteration (ms): 257.4 | learning rate: 6.152E-05 | global batch size:    32 | lm loss: 2.458867E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.06 | backward-params-all-reduce: 63.46 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 19.82 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 99.18 | batch-generator: 7.94
-------------------------------------------------------------------------------------------------
 validation loss at iteration 42500 | lm loss value: 3.878217E-05 | lm loss PPL: 1.000039E+00 | 
 at iteration 42500, match long value: 0.0014867746131755825 | match short value: -0.001041868362456484 
---------------------------------------------------------------------------------------------------------
 iteration    42600/  200000 | consumed samples:      1363200 | elapsed time per iteration (ms): 274.7 | learning rate: 6.143E-05 | global batch size:    32 | lm loss: 2.986703E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.56 | backward-params-all-reduce: 63.80 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.00 | optimizer-unscale-and-check-inf: 18.05 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 13.71 | optimizer: 99.07 | batch-generator: 14.62
 iteration    42700/  200000 | consumed samples:      1366400 | elapsed time per iteration (ms): 257.0 | learning rate: 6.134E-05 | global batch size:    32 | lm loss: 2.413318E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.30 | backward-params-all-reduce: 64.57 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.95 | optimizer-unscale-and-check-inf: 18.88 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.37 | batch-generator: 8.00
 iteration    42800/  200000 | consumed samples:      1369600 | elapsed time per iteration (ms): 255.4 | learning rate: 6.125E-05 | global batch size:    32 | lm loss: 2.202203E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.12 | backward-params-all-reduce: 65.51 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 18.58 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.91 | batch-generator: 8.05
 iteration    42900/  200000 | consumed samples:      1372800 | elapsed time per iteration (ms): 255.0 | learning rate: 6.116E-05 | global batch size:    32 | lm loss: 2.257679E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.10 | backward-params-all-reduce: 64.81 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 16.55 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 95.88 | batch-generator: 8.15
 iteration    43000/  200000 | consumed samples:      1376000 | elapsed time per iteration (ms): 257.1 | learning rate: 6.106E-05 | global batch size:    32 | lm loss: 2.280255E-05 | loss scale: 268435456.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.03 | backward-params-all-reduce: 64.29 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 18.72 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.10 | batch-generator: 8.02
-------------------------------------------------------------------------------------------------
 validation loss at iteration 43000 | lm loss value: 3.546453E-05 | lm loss PPL: 1.000035E+00 | 
 at iteration 43000, match long value: 0.0026870720072811713 | match short value: 0.015408733024811123 
--------------------------------------------------------------------------------------------------------
 iteration    43100/  200000 | consumed samples:      1379200 | elapsed time per iteration (ms): 268.0 | learning rate: 6.097E-05 | global batch size:    32 | lm loss: 2.254189E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.65 | backward-params-all-reduce: 65.14 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.20 | optimizer-unscale-and-check-inf: 17.33 | optimizer-clip-main-grad: 7.46 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 97.20 | batch-generator: 14.12
 iteration    43200/  200000 | consumed samples:      1382400 | elapsed time per iteration (ms): 258.3 | learning rate: 6.088E-05 | global batch size:    32 | lm loss: 2.515436E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.88 | backward-params-all-reduce: 64.98 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 18.82 | optimizer-clip-main-grad: 7.18 | optimizer-copy-main-to-model-params: 11.92 | optimizer: 96.81 | batch-generator: 8.03
 iteration    43300/  200000 | consumed samples:      1385600 | elapsed time per iteration (ms): 258.9 | learning rate: 6.079E-05 | global batch size:    32 | lm loss: 2.379380E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.95 | backward-params-all-reduce: 64.15 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.72 | optimizer-unscale-and-check-inf: 16.54 | optimizer-clip-main-grad: 7.49 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 96.94 | batch-generator: 8.06
 iteration    43400/  200000 | consumed samples:      1388800 | elapsed time per iteration (ms): 270.6 | learning rate: 6.070E-05 | global batch size:    32 | lm loss: 2.385991E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.54 | backward-params-all-reduce: 63.59 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.83 | optimizer-unscale-and-check-inf: 18.23 | optimizer-clip-main-grad: 7.27 | optimizer-copy-main-to-model-params: 13.72 | optimizer: 99.05 | batch-generator: 8.07
 iteration    43500/  200000 | consumed samples:      1392000 | elapsed time per iteration (ms): 264.4 | learning rate: 6.061E-05 | global batch size:    32 | lm loss: 2.388820E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.05 | backward-params-all-reduce: 62.17 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.82 | optimizer-unscale-and-check-inf: 19.80 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 99.10 | batch-generator: 8.00
-------------------------------------------------------------------------------------------------
 validation loss at iteration 43500 | lm loss value: 3.497689E-05 | lm loss PPL: 1.000035E+00 | 
 at iteration 43500, match long value: 0.012459874403985088 | match short value: 0.029922181933292973 
-------------------------------------------------------------------------------------------------------
 iteration    43600/  200000 | consumed samples:      1395200 | elapsed time per iteration (ms): 266.8 | learning rate: 6.052E-05 | global batch size:    32 | lm loss: 2.312388E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.05 | backward-params-all-reduce: 61.51 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 18.02 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.35 | batch-generator: 18.10
 iteration    43700/  200000 | consumed samples:      1398400 | elapsed time per iteration (ms): 254.7 | learning rate: 6.043E-05 | global batch size:    32 | lm loss: 2.331275E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.35 | backward-params-all-reduce: 61.98 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 18.40 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.76 | batch-generator: 8.02
 iteration    43800/  200000 | consumed samples:      1401600 | elapsed time per iteration (ms): 254.4 | learning rate: 6.034E-05 | global batch size:    32 | lm loss: 2.264417E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.14 | backward-params-all-reduce: 62.07 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.92 | optimizer-unscale-and-check-inf: 16.67 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 96.05 | batch-generator: 8.03
 iteration    43900/  200000 | consumed samples:      1404800 | elapsed time per iteration (ms): 252.1 | learning rate: 6.025E-05 | global batch size:    32 | lm loss: 2.255295E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.19 | backward-params-all-reduce: 61.30 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.05 | optimizer-unscale-and-check-inf: 16.59 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 96.12 | batch-generator: 8.00
 iteration    44000/  200000 | consumed samples:      1408000 | elapsed time per iteration (ms): 255.7 | learning rate: 6.015E-05 | global batch size:    32 | lm loss: 2.328490E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.00 | backward-params-all-reduce: 63.50 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 17.71 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.08 | batch-generator: 8.03
-------------------------------------------------------------------------------------------------
 validation loss at iteration 44000 | lm loss value: 3.871615E-05 | lm loss PPL: 1.000039E+00 | 
 at iteration 44000, match long value: 0.00034533400297155875 | match short value: -0.008397721532142435 
----------------------------------------------------------------------------------------------------------
 iteration    44100/  200000 | consumed samples:      1411200 | elapsed time per iteration (ms): 273.3 | learning rate: 6.006E-05 | global batch size:    32 | lm loss: 2.732294E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.20 | backward-params-all-reduce: 63.66 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 16.44 | optimizer-clip-main-grad: 7.26 | optimizer-copy-main-to-model-params: 13.69 | optimizer: 97.26 | batch-generator: 14.00
 iteration    44200/  200000 | consumed samples:      1414400 | elapsed time per iteration (ms): 255.5 | learning rate: 5.997E-05 | global batch size:    32 | lm loss: 2.459832E-05 | loss scale: 268435456.0 | grad norm: 0.006 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.05 | backward-params-all-reduce: 63.39 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.05 | optimizer-unscale-and-check-inf: 18.31 | optimizer-clip-main-grad: 7.25 | optimizer-copy-main-to-model-params: 12.05 | optimizer: 97.20 | batch-generator: 7.99
 iteration    44300/  200000 | consumed samples:      1417600 | elapsed time per iteration (ms): 254.9 | learning rate: 5.988E-05 | global batch size:    32 | lm loss: 2.550799E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.23 | backward-params-all-reduce: 63.22 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.19 | optimizer-unscale-and-check-inf: 17.80 | optimizer-clip-main-grad: 7.24 | optimizer-copy-main-to-model-params: 12.05 | optimizer: 96.80 | batch-generator: 7.95
 iteration    44400/  200000 | consumed samples:      1420800 | elapsed time per iteration (ms): 254.0 | learning rate: 5.979E-05 | global batch size:    32 | lm loss: 2.636926E-05 | loss scale: 134217728.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.06 | backward-params-all-reduce: 62.18 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.93 | optimizer-unscale-and-check-inf: 17.60 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.01 | batch-generator: 8.05
 iteration    44500/  200000 | consumed samples:      1424000 | elapsed time per iteration (ms): 254.1 | learning rate: 5.970E-05 | global batch size:    32 | lm loss: 2.320636E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.05 | backward-params-all-reduce: 62.52 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.96 | optimizer-unscale-and-check-inf: 16.67 | optimizer-clip-main-grad: 7.26 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 96.06 | batch-generator: 7.97
-------------------------------------------------------------------------------------------------
 validation loss at iteration 44500 | lm loss value: 3.362543E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 44500, match long value: -0.0001928571549108663 | match short value: -0.00014566486681710686 
------------------------------------------------------------------------------------------------------------
 iteration    44600/  200000 | consumed samples:      1427200 | elapsed time per iteration (ms): 266.5 | learning rate: 5.961E-05 | global batch size:    32 | lm loss: 2.195895E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.11 | backward-params-all-reduce: 64.04 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 17.59 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 96.92 | batch-generator: 13.97
 iteration    44700/  200000 | consumed samples:      1430400 | elapsed time per iteration (ms): 253.1 | learning rate: 5.952E-05 | global batch size:    32 | lm loss: 2.157882E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.21 | backward-params-all-reduce: 62.65 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.88 | optimizer-unscale-and-check-inf: 14.91 | optimizer-clip-main-grad: 7.24 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 94.21 | batch-generator: 7.94
 iteration    44800/  200000 | consumed samples:      1433600 | elapsed time per iteration (ms): 263.9 | learning rate: 5.943E-05 | global batch size:    32 | lm loss: 2.365279E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.55 | backward-params-all-reduce: 63.08 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.04 | optimizer-unscale-and-check-inf: 17.54 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 13.70 | optimizer: 98.58 | batch-generator: 8.14
 iteration    44900/  200000 | consumed samples:      1436800 | elapsed time per iteration (ms): 257.4 | learning rate: 5.934E-05 | global batch size:    32 | lm loss: 2.398243E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.10 | backward-params-all-reduce: 63.15 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.96 | optimizer-unscale-and-check-inf: 17.68 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.13 | batch-generator: 8.05
 iteration    45000/  200000 | consumed samples:      1440000 | elapsed time per iteration (ms): 255.4 | learning rate: 5.925E-05 | global batch size:    32 | lm loss: 2.204650E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.11 | backward-params-all-reduce: 63.20 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.19 | optimizer-unscale-and-check-inf: 18.16 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 97.86 | batch-generator: 8.07
-------------------------------------------------------------------------------------------------
 validation loss at iteration 45000 | lm loss value: 3.524201E-05 | lm loss PPL: 1.000035E+00 | 
 at iteration 45000, match long value: 0.001463953450137807 | match short value: -0.01882259654235106 
-------------------------------------------------------------------------------------------------------
 iteration    45100/  200000 | consumed samples:      1443200 | elapsed time per iteration (ms): 270.9 | learning rate: 5.915E-05 | global batch size:    32 | lm loss: 2.132077E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.63 | backward-params-all-reduce: 63.11 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.90 | optimizer-unscale-and-check-inf: 17.75 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 97.09 | batch-generator: 17.78
 iteration    45200/  200000 | consumed samples:      1446400 | elapsed time per iteration (ms): 254.4 | learning rate: 5.906E-05 | global batch size:    32 | lm loss: 2.377024E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.00 | backward-params-all-reduce: 63.00 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.90 | optimizer-unscale-and-check-inf: 17.80 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.15 | batch-generator: 7.96
 iteration    45300/  200000 | consumed samples:      1449600 | elapsed time per iteration (ms): 252.6 | learning rate: 5.897E-05 | global batch size:    32 | lm loss: 2.672298E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.03 | backward-params-all-reduce: 62.08 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.96 | optimizer-unscale-and-check-inf: 17.99 | optimizer-clip-main-grad: 7.19 | optimizer-copy-main-to-model-params: 11.94 | optimizer: 96.13 | batch-generator: 8.05
 iteration    45400/  200000 | consumed samples:      1452800 | elapsed time per iteration (ms): 255.0 | learning rate: 5.888E-05 | global batch size:    32 | lm loss: 2.263797E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.28 | backward-params-all-reduce: 62.21 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.01 | optimizer-unscale-and-check-inf: 18.50 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 98.01 | batch-generator: 8.02
 iteration    45500/  200000 | consumed samples:      1456000 | elapsed time per iteration (ms): 257.5 | learning rate: 5.879E-05 | global batch size:    32 | lm loss: 2.297540E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.48 | backward-params-all-reduce: 61.56 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.20 | optimizer-unscale-and-check-inf: 18.39 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.07 | batch-generator: 8.05
-------------------------------------------------------------------------------------------------
 validation loss at iteration 45500 | lm loss value: 3.614054E-05 | lm loss PPL: 1.000036E+00 | 
 at iteration 45500, match long value: 0.0024470644819214325 | match short value: 0.012257786446010258 
--------------------------------------------------------------------------------------------------------
 iteration    45600/  200000 | consumed samples:      1459200 | elapsed time per iteration (ms): 283.0 | learning rate: 5.870E-05 | global batch size:    32 | lm loss: 2.258286E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.82 | backward-params-all-reduce: 62.07 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.89 | optimizer-unscale-and-check-inf: 18.50 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 13.66 | optimizer: 99.41 | batch-generator: 19.15
 iteration    45700/  200000 | consumed samples:      1462400 | elapsed time per iteration (ms): 259.5 | learning rate: 5.861E-05 | global batch size:    32 | lm loss: 2.114647E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.89 | backward-params-all-reduce: 62.44 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.15 | optimizer-unscale-and-check-inf: 18.74 | optimizer-clip-main-grad: 7.37 | optimizer-copy-main-to-model-params: 12.20 | optimizer: 98.48 | batch-generator: 8.10
 iteration    45800/  200000 | consumed samples:      1465600 | elapsed time per iteration (ms): 256.0 | learning rate: 5.852E-05 | global batch size:    32 | lm loss: 2.158233E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.92 | backward-params-all-reduce: 63.54 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.42 | optimizer-unscale-and-check-inf: 19.82 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.22 | optimizer: 99.83 | batch-generator: 8.16
 iteration    45900/  200000 | consumed samples:      1468800 | elapsed time per iteration (ms): 257.3 | learning rate: 5.843E-05 | global batch size:    32 | lm loss: 2.141194E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.26 | backward-params-all-reduce: 64.27 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.89 | optimizer-unscale-and-check-inf: 21.10 | optimizer-clip-main-grad: 7.39 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 100.56 | batch-generator: 8.19
 iteration    46000/  200000 | consumed samples:      1472000 | elapsed time per iteration (ms): 262.5 | learning rate: 5.834E-05 | global batch size:    32 | lm loss: 2.059853E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.97 | backward-params-all-reduce: 62.94 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.00 | optimizer-unscale-and-check-inf: 21.04 | optimizer-clip-main-grad: 7.38 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 100.63 | batch-generator: 8.18
-------------------------------------------------------------------------------------------------
 validation loss at iteration 46000 | lm loss value: 3.423478E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 46000, match long value: -0.0023106939764735517 | match short value: 0.010782038822789467 
---------------------------------------------------------------------------------------------------------
 iteration    46100/  200000 | consumed samples:      1475200 | elapsed time per iteration (ms): 267.6 | learning rate: 5.825E-05 | global batch size:    32 | lm loss: 2.075890E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.57 | backward-params-all-reduce: 64.13 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 20.50 | optimizer-clip-main-grad: 7.40 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 99.95 | batch-generator: 13.31
 iteration    46200/  200000 | consumed samples:      1478400 | elapsed time per iteration (ms): 256.9 | learning rate: 5.815E-05 | global batch size:    32 | lm loss: 2.322544E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.74 | backward-params-all-reduce: 66.08 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.91 | optimizer-unscale-and-check-inf: 18.94 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 98.36 | batch-generator: 8.13
 iteration    46300/  200000 | consumed samples:      1481600 | elapsed time per iteration (ms): 267.0 | learning rate: 5.806E-05 | global batch size:    32 | lm loss: 2.104132E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.49 | backward-params-all-reduce: 62.51 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.98 | optimizer-unscale-and-check-inf: 19.37 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 13.73 | optimizer: 100.46 | batch-generator: 8.10
 iteration    46400/  200000 | consumed samples:      1484800 | elapsed time per iteration (ms): 255.4 | learning rate: 5.797E-05 | global batch size:    32 | lm loss: 2.098212E-05 | loss scale: 268435456.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.92 | backward-params-all-reduce: 62.49 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.99 | optimizer-unscale-and-check-inf: 20.23 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 99.78 | batch-generator: 8.11
 iteration    46500/  200000 | consumed samples:      1488000 | elapsed time per iteration (ms): 256.5 | learning rate: 5.788E-05 | global batch size:    32 | lm loss: 2.105780E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.41 | backward-params-all-reduce: 62.12 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.50 | optimizer-unscale-and-check-inf: 20.34 | optimizer-clip-main-grad: 7.27 | optimizer-copy-main-to-model-params: 11.95 | optimizer: 99.13 | batch-generator: 8.36
-------------------------------------------------------------------------------------------------
 validation loss at iteration 46500 | lm loss value: 3.475032E-05 | lm loss PPL: 1.000035E+00 | 
 at iteration 46500, match long value: 0.003398456468123483 | match short value: -0.018142850540267735 
--------------------------------------------------------------------------------------------------------
 iteration    46600/  200000 | consumed samples:      1491200 | elapsed time per iteration (ms): 265.2 | learning rate: 5.779E-05 | global batch size:    32 | lm loss: 2.101477E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.34 | backward-params-all-reduce: 62.15 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.94 | optimizer-unscale-and-check-inf: 20.32 | optimizer-clip-main-grad: 7.37 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 99.82 | batch-generator: 14.90
 iteration    46700/  200000 | consumed samples:      1494400 | elapsed time per iteration (ms): 256.2 | learning rate: 5.770E-05 | global batch size:    32 | lm loss: 2.300920E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.41 | backward-params-all-reduce: 62.62 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.02 | optimizer-unscale-and-check-inf: 20.13 | optimizer-clip-main-grad: 7.41 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 99.75 | batch-generator: 8.14
 iteration    46800/  200000 | consumed samples:      1497600 | elapsed time per iteration (ms): 256.0 | learning rate: 5.761E-05 | global batch size:    32 | lm loss: 2.091091E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.23 | backward-params-all-reduce: 63.02 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 20.49 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 99.87 | batch-generator: 8.09
 iteration    46900/  200000 | consumed samples:      1500800 | elapsed time per iteration (ms): 254.9 | learning rate: 5.752E-05 | global batch size:    32 | lm loss: 1.981506E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.68 | backward-params-all-reduce: 62.15 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.19 | optimizer-unscale-and-check-inf: 19.68 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.19 | optimizer: 99.42 | batch-generator: 8.28
 iteration    47000/  200000 | consumed samples:      1504000 | elapsed time per iteration (ms): 262.8 | learning rate: 5.743E-05 | global batch size:    32 | lm loss: 2.428154E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.72 | backward-params-all-reduce: 62.68 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.96 | optimizer-unscale-and-check-inf: 18.94 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 13.75 | optimizer: 99.98 | batch-generator: 8.10
-------------------------------------------------------------------------------------------------
 validation loss at iteration 47000 | lm loss value: 3.491460E-05 | lm loss PPL: 1.000035E+00 | 
 at iteration 47000, match long value: -0.008080759829430512 | match short value: -0.03884378872530988 
--------------------------------------------------------------------------------------------------------
 iteration    47100/  200000 | consumed samples:      1507200 | elapsed time per iteration (ms): 274.1 | learning rate: 5.734E-05 | global batch size:    32 | lm loss: 2.205726E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.20 | backward-params-all-reduce: 63.20 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.90 | optimizer-unscale-and-check-inf: 20.00 | optimizer-clip-main-grad: 7.39 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 99.48 | batch-generator: 20.78
 iteration    47200/  200000 | consumed samples:      1510400 | elapsed time per iteration (ms): 255.7 | learning rate: 5.725E-05 | global batch size:    32 | lm loss: 2.170036E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.77 | backward-params-all-reduce: 62.23 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.07 | optimizer-unscale-and-check-inf: 18.69 | optimizer-clip-main-grad: 7.38 | optimizer-copy-main-to-model-params: 12.22 | optimizer: 98.39 | batch-generator: 8.19
 iteration    47300/  200000 | consumed samples:      1513600 | elapsed time per iteration (ms): 254.9 | learning rate: 5.715E-05 | global batch size:    32 | lm loss: 2.212700E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.30 | backward-params-all-reduce: 61.86 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.02 | optimizer-unscale-and-check-inf: 19.90 | optimizer-clip-main-grad: 7.38 | optimizer-copy-main-to-model-params: 12.20 | optimizer: 99.52 | batch-generator: 8.19
 iteration    47400/  200000 | consumed samples:      1516800 | elapsed time per iteration (ms): 254.1 | learning rate: 5.706E-05 | global batch size:    32 | lm loss: 2.010640E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.17 | backward-params-all-reduce: 63.42 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.02 | optimizer-unscale-and-check-inf: 18.24 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.23 | optimizer: 97.83 | batch-generator: 8.10
 iteration    47500/  200000 | consumed samples:      1520000 | elapsed time per iteration (ms): 253.2 | learning rate: 5.697E-05 | global batch size:    32 | lm loss: 2.354332E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.14 | backward-params-all-reduce: 63.09 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.98 | optimizer-unscale-and-check-inf: 18.95 | optimizer-clip-main-grad: 7.22 | optimizer-copy-main-to-model-params: 11.95 | optimizer: 97.17 | batch-generator: 8.16
-------------------------------------------------------------------------------------------------
 validation loss at iteration 47500 | lm loss value: 3.448757E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 47500, match long value: -0.005098971211545408 | match short value: -0.02079815363985986 
--------------------------------------------------------------------------------------------------------
 iteration    47600/  200000 | consumed samples:      1523200 | elapsed time per iteration (ms): 267.8 | learning rate: 5.688E-05 | global batch size:    32 | lm loss: 2.142789E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.09 | backward-params-all-reduce: 63.56 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 19.31 | optimizer-clip-main-grad: 7.43 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 98.80 | batch-generator: 15.12
 iteration    47700/  200000 | consumed samples:      1526400 | elapsed time per iteration (ms): 254.6 | learning rate: 5.679E-05 | global batch size:    32 | lm loss: 2.027735E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.34 | backward-params-all-reduce: 62.39 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.87 | optimizer-unscale-and-check-inf: 18.29 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 12.20 | optimizer: 97.80 | batch-generator: 8.17
 iteration    47800/  200000 | consumed samples:      1529600 | elapsed time per iteration (ms): 264.6 | learning rate: 5.670E-05 | global batch size:    32 | lm loss: 2.051182E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.91 | backward-params-all-reduce: 62.49 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.95 | optimizer-unscale-and-check-inf: 18.87 | optimizer-clip-main-grad: 7.39 | optimizer-copy-main-to-model-params: 13.67 | optimizer: 99.91 | batch-generator: 8.24
 iteration    47900/  200000 | consumed samples:      1532800 | elapsed time per iteration (ms): 253.3 | learning rate: 5.661E-05 | global batch size:    32 | lm loss: 2.069709E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.10 | backward-params-all-reduce: 61.89 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.04 | optimizer-unscale-and-check-inf: 18.67 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.19 | optimizer: 98.27 | batch-generator: 8.29
 iteration    48000/  200000 | consumed samples:      1536000 | elapsed time per iteration (ms): 254.2 | learning rate: 5.652E-05 | global batch size:    32 | lm loss: 2.064754E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.17 | backward-params-all-reduce: 63.17 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.96 | optimizer-unscale-and-check-inf: 18.88 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 98.39 | batch-generator: 8.10
-------------------------------------------------------------------------------------------------
 validation loss at iteration 48000 | lm loss value: 3.450515E-05 | lm loss PPL: 1.000035E+00 | 
 at iteration 48000, match long value: 0.007325933966142038 | match short value: -0.0025793266819103193 
---------------------------------------------------------------------------------------------------------
 iteration    48100/  200000 | consumed samples:      1539200 | elapsed time per iteration (ms): 267.3 | learning rate: 5.643E-05 | global batch size:    32 | lm loss: 2.130372E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.23 | backward-params-all-reduce: 63.48 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.22 | optimizer-unscale-and-check-inf: 18.37 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.11 | batch-generator: 13.67
 iteration    48200/  200000 | consumed samples:      1542400 | elapsed time per iteration (ms): 254.2 | learning rate: 5.634E-05 | global batch size:    32 | lm loss: 2.109694E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.14 | backward-params-all-reduce: 62.97 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.87 | optimizer-unscale-and-check-inf: 19.37 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 98.80 | batch-generator: 8.12
 iteration    48300/  200000 | consumed samples:      1545600 | elapsed time per iteration (ms): 254.5 | learning rate: 5.625E-05 | global batch size:    32 | lm loss: 2.225612E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.08 | backward-params-all-reduce: 64.22 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 18.91 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 98.32 | batch-generator: 8.18
 iteration    48400/  200000 | consumed samples:      1548800 | elapsed time per iteration (ms): 259.8 | learning rate: 5.615E-05 | global batch size:    32 | lm loss: 2.112374E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.08 | backward-params-all-reduce: 62.19 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.93 | optimizer-unscale-and-check-inf: 19.10 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 98.60 | batch-generator: 8.21
 iteration    48500/  200000 | consumed samples:      1552000 | elapsed time per iteration (ms): 267.5 | learning rate: 5.606E-05 | global batch size:    32 | lm loss: 2.171626E-05 | loss scale: 268435456.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.71 | backward-params-all-reduce: 62.98 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 18.56 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 13.64 | optimizer: 99.42 | batch-generator: 8.11
-------------------------------------------------------------------------------------------------
 validation loss at iteration 48500 | lm loss value: 3.736361E-05 | lm loss PPL: 1.000037E+00 | 
 at iteration 48500, match long value: 0.0013129640244584858 | match short value: 0.06770846275027555 
-------------------------------------------------------------------------------------------------------
 iteration    48600/  200000 | consumed samples:      1555200 | elapsed time per iteration (ms): 269.8 | learning rate: 5.597E-05 | global batch size:    32 | lm loss: 2.172049E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.24 | backward-params-all-reduce: 62.34 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 19.38 | optimizer-clip-main-grad: 7.23 | optimizer-copy-main-to-model-params: 11.92 | optimizer: 97.45 | batch-generator: 18.34
 iteration    48700/  200000 | consumed samples:      1558400 | elapsed time per iteration (ms): 256.4 | learning rate: 5.588E-05 | global batch size:    32 | lm loss: 2.098429E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.44 | backward-params-all-reduce: 62.61 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.91 | optimizer-unscale-and-check-inf: 18.50 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.19 | optimizer: 97.97 | batch-generator: 8.17
 iteration    48800/  200000 | consumed samples:      1561600 | elapsed time per iteration (ms): 255.9 | learning rate: 5.579E-05 | global batch size:    32 | lm loss: 2.034380E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.00 | backward-params-all-reduce: 63.68 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.91 | optimizer-unscale-and-check-inf: 18.83 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 98.29 | batch-generator: 8.16
 iteration    48900/  200000 | consumed samples:      1564800 | elapsed time per iteration (ms): 254.9 | learning rate: 5.570E-05 | global batch size:    32 | lm loss: 2.123879E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.12 | backward-params-all-reduce: 63.55 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.94 | optimizer-unscale-and-check-inf: 16.92 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.19 | optimizer: 96.37 | batch-generator: 8.05
 iteration    49000/  200000 | consumed samples:      1568000 | elapsed time per iteration (ms): 253.6 | learning rate: 5.561E-05 | global batch size:    32 | lm loss: 2.200196E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.03 | backward-params-all-reduce: 62.86 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.93 | optimizer-unscale-and-check-inf: 17.91 | optimizer-clip-main-grad: 7.26 | optimizer-copy-main-to-model-params: 12.06 | optimizer: 96.69 | batch-generator: 8.16
-------------------------------------------------------------------------------------------------
 validation loss at iteration 49000 | lm loss value: 3.657652E-05 | lm loss PPL: 1.000037E+00 | 
 at iteration 49000, match long value: -0.0011543379874221962 | match short value: 0.0008203097837905812 
----------------------------------------------------------------------------------------------------------
 iteration    49100/  200000 | consumed samples:      1571200 | elapsed time per iteration (ms): 271.2 | learning rate: 5.552E-05 | global batch size:    32 | lm loss: 2.046403E-05 | loss scale: 67108864.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.23 | backward-params-all-reduce: 62.84 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 20.27 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 99.64 | batch-generator: 18.97
 iteration    49200/  200000 | consumed samples:      1574400 | elapsed time per iteration (ms): 268.0 | learning rate: 5.543E-05 | global batch size:    32 | lm loss: 2.062490E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.72 | backward-params-all-reduce: 64.02 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.83 | optimizer-unscale-and-check-inf: 19.59 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 13.62 | optimizer: 100.39 | batch-generator: 8.16
 iteration    49300/  200000 | consumed samples:      1577600 | elapsed time per iteration (ms): 256.2 | learning rate: 5.534E-05 | global batch size:    32 | lm loss: 2.163338E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.14 | backward-params-all-reduce: 64.12 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.89 | optimizer-unscale-and-check-inf: 19.38 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.81 | batch-generator: 8.07
 iteration    49400/  200000 | consumed samples:      1580800 | elapsed time per iteration (ms): 254.6 | learning rate: 5.525E-05 | global batch size:    32 | lm loss: 1.904020E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.01 | backward-params-all-reduce: 62.45 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.03 | optimizer-unscale-and-check-inf: 19.08 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 98.65 | batch-generator: 8.09
 iteration    49500/  200000 | consumed samples:      1584000 | elapsed time per iteration (ms): 256.7 | learning rate: 5.515E-05 | global batch size:    32 | lm loss: 1.980742E-05 | loss scale: 67108864.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.00 | backward-params-all-reduce: 64.06 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 18.43 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 97.79 | batch-generator: 8.10
-------------------------------------------------------------------------------------------------
 validation loss at iteration 49500 | lm loss value: 3.517785E-05 | lm loss PPL: 1.000035E+00 | 
 at iteration 49500, match long value: 0.00252681399036391 | match short value: 0.029545521436273512 
------------------------------------------------------------------------------------------------------
 iteration    49600/  200000 | consumed samples:      1587200 | elapsed time per iteration (ms): 265.6 | learning rate: 5.506E-05 | global batch size:    32 | lm loss: 2.024957E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.14 | backward-params-all-reduce: 62.90 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.83 | optimizer-unscale-and-check-inf: 18.83 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.17 | batch-generator: 14.05
 iteration    49700/  200000 | consumed samples:      1590400 | elapsed time per iteration (ms): 262.2 | learning rate: 5.497E-05 | global batch size:    32 | lm loss: 1.906841E-05 | loss scale: 67108864.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.12 | backward-params-all-reduce: 63.15 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 19.83 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 99.20 | batch-generator: 8.03
 iteration    49800/  200000 | consumed samples:      1593600 | elapsed time per iteration (ms): 254.7 | learning rate: 5.488E-05 | global batch size:    32 | lm loss: 1.932356E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.17 | backward-params-all-reduce: 62.28 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 20.01 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 99.37 | batch-generator: 8.11
 iteration    49900/  200000 | consumed samples:      1596800 | elapsed time per iteration (ms): 259.2 | learning rate: 5.479E-05 | global batch size:    32 | lm loss: 2.023299E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.21 | backward-params-all-reduce: 62.86 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.89 | optimizer-unscale-and-check-inf: 19.24 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.64 | batch-generator: 8.02
 iteration    50000/  200000 | consumed samples:      1600000 | elapsed time per iteration (ms): 268.3 | learning rate: 5.470E-05 | global batch size:    32 | lm loss: 2.107168E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.59 | backward-params-all-reduce: 63.32 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 18.47 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 13.60 | optimizer: 99.24 | batch-generator: 8.04
-------------------------------------------------------------------------------------------------
 validation loss at iteration 50000 | lm loss value: 3.685014E-05 | lm loss PPL: 1.000037E+00 | 
 at iteration 50000, match long value: 0.0013170634898610409 | match short value: 0.020088949526502586 
--------------------------------------------------------------------------------------------------------
saving checkpoint at iteration   50000 to verify1
  successfully saved checkpoint at iteration   50000 to verify1
time (ms) | save-checkpoint: 32686.98
 iteration    50100/  200000 | consumed samples:      1603200 | elapsed time per iteration (ms): 596.6 | learning rate: 5.461E-05 | global batch size:    32 | lm loss: 1.918080E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.91 | backward-params-all-reduce: 62.81 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 18.06 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.41 | batch-generator: 16.84
 iteration    50200/  200000 | consumed samples:      1606400 | elapsed time per iteration (ms): 253.3 | learning rate: 5.452E-05 | global batch size:    32 | lm loss: 1.869492E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.93 | backward-params-all-reduce: 61.81 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.95 | optimizer-unscale-and-check-inf: 18.06 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 97.52 | batch-generator: 8.11
 iteration    50300/  200000 | consumed samples:      1609600 | elapsed time per iteration (ms): 254.5 | learning rate: 5.443E-05 | global batch size:    32 | lm loss: 1.972165E-05 | loss scale: 67108864.0 | grad norm: 0.002 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.99 | backward-params-all-reduce: 61.81 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.00 | optimizer-unscale-and-check-inf: 17.93 | optimizer-clip-main-grad: 7.14 | optimizer-copy-main-to-model-params: 11.93 | optimizer: 96.06 | batch-generator: 7.95
 iteration    50400/  200000 | consumed samples:      1612800 | elapsed time per iteration (ms): 254.7 | learning rate: 5.434E-05 | global batch size:    32 | lm loss: 1.956498E-05 | loss scale: 67108864.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.29 | backward-params-all-reduce: 62.36 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.10 | optimizer-unscale-and-check-inf: 17.91 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.49 | batch-generator: 8.07
 iteration    50500/  200000 | consumed samples:      1616000 | elapsed time per iteration (ms): 254.8 | learning rate: 5.425E-05 | global batch size:    32 | lm loss: 2.049542E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.65 | backward-params-all-reduce: 61.59 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.15 | optimizer-unscale-and-check-inf: 18.77 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.40 | batch-generator: 8.07
-------------------------------------------------------------------------------------------------
 validation loss at iteration 50500 | lm loss value: 3.678679E-05 | lm loss PPL: 1.000037E+00 | 
 at iteration 50500, match long value: 0.0025093155967070148 | match short value: 0.016467716671095617 
--------------------------------------------------------------------------------------------------------
 iteration    50600/  200000 | consumed samples:      1619200 | elapsed time per iteration (ms): 266.1 | learning rate: 5.415E-05 | global batch size:    32 | lm loss: 1.966597E-05 | loss scale: 67108864.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.21 | backward-params-all-reduce: 62.18 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.96 | optimizer-unscale-and-check-inf: 17.44 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.19 | optimizer: 96.96 | batch-generator: 14.86
 iteration    50700/  200000 | consumed samples:      1622400 | elapsed time per iteration (ms): 264.6 | learning rate: 5.406E-05 | global batch size:    32 | lm loss: 1.940590E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.34 | backward-params-all-reduce: 62.69 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.32 | optimizer-unscale-and-check-inf: 17.49 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 13.67 | optimizer: 98.80 | batch-generator: 8.81
 iteration    50800/  200000 | consumed samples:      1625600 | elapsed time per iteration (ms): 255.4 | learning rate: 5.397E-05 | global batch size:    32 | lm loss: 1.966077E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 54.66 | backward-params-all-reduce: 60.62 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 13.20 | optimizer-unscale-and-check-inf: 17.32 | optimizer-clip-main-grad: 7.27 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.97 | batch-generator: 8.45
examples/tmp.sh: line 54: 96521 Killed                  python -m torch.distributed.launch $DISTRIBUTED_ARGS pretrain_gw.py --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 24 --hidden-size 2048 --num-attention-heads 32 --micro-batch-size 4 --global-batch-size 32 --segment-length 2048 --seq-length 127 --max-position-embeddings 127 --train-iters 200000 --save $CHECKPOINT_PATH --load $CHECKPOINT_PATH --data-path $DATA_PATH --vocab-file $VOCAB_FILE --data-impl mmap --split 949,50,1 --distributed-backend nccl --lr 0.0001 --lr-decay-style linear --min-lr 1.0e-5 --lr-decay-iters 99000 --weight-decay 1e-2 --clip-grad 1.0 --lr-warmup-fraction .002 --log-interval 100 --save-interval 50000 --eval-interval 500 --eval-iters 10 --dataloader-type cyclic --fp16 --bert-no-binary-head
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:177: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torch.distributed.run.
Note that --use_env is set by default in torch.distributed.run.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
Traceback (most recent call last):
  File "/opt/conda/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py", line 191, in <module>
    main()
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py", line 187, in main
    launch(args)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py", line 173, in launch
    run(args)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/run.py", line 688, in run
    elastic_launch(
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 252, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 700, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 822, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 670, in _initialize_workers
    self._rendezvous(worker_group)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 530, in _rendezvous
    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 55, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: Address already in use
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:177: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torch.distributed.run.
Note that --use_env is set by default in torch.distributed.run.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
Traceback (most recent call last):
  File "/opt/conda/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py", line 191, in <module>
    main()
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py", line 187, in main
    launch(args)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py", line 173, in launch
    run(args)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/run.py", line 688, in run
    elastic_launch(
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 252, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 700, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 822, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 670, in _initialize_workers
    self._rendezvous(worker_group)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 530, in _rendezvous
    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 55, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: Address already in use
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:177: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torch.distributed.run.
Note that --use_env is set by default in torch.distributed.run.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
Traceback (most recent call last):
  File "/opt/conda/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py", line 191, in <module>
    main()
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py", line 187, in main
    launch(args)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py", line 173, in launch
    run(args)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/run.py", line 688, in run
    elastic_launch(
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 252, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 700, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 822, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 670, in _initialize_workers
    self._rendezvous(worker_group)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 530, in _rendezvous
    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 55, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: Address already in use
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:177: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torch.distributed.run.
Note that --use_env is set by default in torch.distributed.run.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
Traceback (most recent call last):
  File "/opt/conda/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py", line 191, in <module>
    main()
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py", line 187, in main
    launch(args)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py", line 173, in launch
    run(args)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/run.py", line 688, in run
    elastic_launch(
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 252, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 700, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 822, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 670, in _initialize_workers
    self._rendezvous(worker_group)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 530, in _rendezvous
    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 55, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: Address already in use
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:177: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torch.distributed.run.
Note that --use_env is set by default in torch.distributed.run.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
Traceback (most recent call last):
  File "/opt/conda/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py", line 191, in <module>
    main()
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py", line 187, in main
    launch(args)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py", line 173, in launch
    run(args)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/run.py", line 688, in run
    elastic_launch(
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 252, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 700, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 822, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 670, in _initialize_workers
    self._rendezvous(worker_group)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 530, in _rendezvous
    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 55, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: Address already in use
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:177: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torch.distributed.run.
Note that --use_env is set by default in torch.distributed.run.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
Traceback (most recent call last):
  File "/opt/conda/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py", line 191, in <module>
    main()
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py", line 187, in main
    launch(args)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py", line 173, in launch
    run(args)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/run.py", line 688, in run
    elastic_launch(
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 252, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 700, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 822, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 670, in _initialize_workers
    self._rendezvous(worker_group)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 530, in _rendezvous
    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 55, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: Address already in use
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:177: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torch.distributed.run.
Note that --use_env is set by default in torch.distributed.run.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
Traceback (most recent call last):
  File "/opt/conda/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py", line 191, in <module>
    main()
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py", line 187, in main
    launch(args)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py", line 173, in launch
    run(args)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/run.py", line 688, in run
    elastic_launch(
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 252, in launch_agent
    result = agent.run()
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 700, in run
    result = self._invoke_run(role)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 822, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 670, in _initialize_workers
    self._rendezvous(worker_group)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 530, in _rendezvous
    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 55, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: connect() timed out. Original timeout was 900000 ms.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
using world size: 8, data-parallel-size: 8, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.999
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ False
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  data_impl ....................................... mmap
  data_parallel_size .............................. 8
  data_path ....................................... ['../bigdata/']
  dataloader_type ................................. cyclic
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  embedding_path .................................. None
  encoder_seq_length .............................. 127
  eod_mask_loss ................................... False
  eval_interval ................................... 500
  eval_iters ...................................... 10
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  ffn_hidden_size ................................. 8192
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 32
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 2048
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  init_method_std ................................. 0.02
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  kv_channels ..................................... 64
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ verify1
  local_rank ...................................... 0
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 100
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0001
  lr_decay_iters .................................. 99000
  lr_decay_samples ................................ None
  lr_decay_style .................................. linear
  lr_warmup_fraction .............................. 0.002
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 127
  merge_file ...................................... None
  micro_batch_size ................................ 4
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mmap_warmup ..................................... False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 32
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_layers ...................................... 24
  num_layers_per_virtual_pipeline_stage ........... None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  sample_rate ..................................... 1.0
  save ............................................ verify1
  save_interval ................................... 20000
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  segment_length .................................. 2048
  seq_length ...................................... 127
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 949,50,1
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. None
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  titles_data_path ................................ None
  tokenizer_type .................................. BertWordPieceLowerCase
  train_iters ..................................... 200000
  train_samples ................................... None
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... ./bert_vocab_files/bert-base-uncased-vocab.txt
  weight_decay .................................... 0.01
  world_size ...................................... 8
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 1
> building BertWordPieceLowerCase tokenizer ...
 > padded vocab (size: 30524) with 68 dummy tokens (new size: 30592)
> initializing torch distributed ...
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /workspace/zhouy/megatron-gw/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /workspace/zhouy/megatron-gw/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /workspace/zhouy/megatron-gw/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_mix_prec_layer_norm_cuda...
>>> done with compiling and loading fused kernels. Compilation time: 7.531 seconds
time to initialize megatron (seconds): 27.343
[after megatron is initialized] datetime: 2021-12-23 21:43:55 
building BERT model ...
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1216997376
> learning rate decay style: linear
 loading checkpoint from verify1 at iteration 50000
 checkpoint version 3.0
 > using checkpoint value 0.0001 for learning rate
 > using checkpoint value 1e-05 for minimum learning rate
 > using checkpoint value 6336.0 for warmup iterations
 > using checkpoint value 3168000 for total number of iterations
 > using checkpoint value linear for decay style
  successfully loaded checkpoint from verify1 at iteration 50000
time (ms) | load-checkpoint: 28947.06
[after model, optimizer, and learning rate scheduler are built] datetime: 2021-12-23 21:44:25 
> building train, validation, and test datasets ...
> building train, validation, and test datasets for BERT ...
> finished creating BERT datasets ...
[after dataloaders are built] datetime: 2021-12-23 21:44:34 
time (ms) | model-and-optimizer-setup: 29751.27 | train/valid/test-data-iterators-setup: 8740.03done with setup ...

training ...
[before the start of training step] datetime: 2021-12-23 21:44:34 
[Rank 0] (after 50100 iterations) memory (MB) | allocated: 23212.4814453125 | max allocated: 23213.79052734375 | reserved: 26498.0 | max reserved: 26498.0
 iteration    50100/  200000 | consumed samples:      1603200 | elapsed time per iteration (ms): 318.3 | learning rate: 5.461E-05 | global batch size:    32 | lm loss: 8.123324E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 58.91 | backward-params-all-reduce: 66.79 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.86 | optimizer-unscale-and-check-inf: 15.80 | optimizer-clip-main-grad: 7.46 | optimizer-copy-main-to-model-params: 14.02 | optimizer: 99.40 | batch-generator: 15.29
 iteration    50200/  200000 | consumed samples:      1606400 | elapsed time per iteration (ms): 263.5 | learning rate: 5.452E-05 | global batch size:    32 | lm loss: 7.488849E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 54.70 | backward-params-all-reduce: 65.73 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.69 | optimizer-unscale-and-check-inf: 15.12 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 97.06 | batch-generator: 8.64
 iteration    50300/  200000 | consumed samples:      1609600 | elapsed time per iteration (ms): 261.0 | learning rate: 5.443E-05 | global batch size:    32 | lm loss: 6.787899E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.00 | backward-params-all-reduce: 65.77 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.39 | optimizer-unscale-and-check-inf: 16.67 | optimizer-clip-main-grad: 7.54 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 98.21 | batch-generator: 8.49
 iteration    50400/  200000 | consumed samples:      1612800 | elapsed time per iteration (ms): 262.3 | learning rate: 5.434E-05 | global batch size:    32 | lm loss: 6.213984E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.97 | backward-params-all-reduce: 67.11 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.46 | optimizer-unscale-and-check-inf: 16.84 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.55 | batch-generator: 8.47
 iteration    50500/  200000 | consumed samples:      1616000 | elapsed time per iteration (ms): 263.6 | learning rate: 5.425E-05 | global batch size:    32 | lm loss: 5.875033E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.76 | backward-params-all-reduce: 68.08 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.47 | optimizer-unscale-and-check-inf: 16.43 | optimizer-clip-main-grad: 7.53 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 98.04 | batch-generator: 8.47
-------------------------------------------------------------------------------------------------
 validation loss at iteration 50500 | lm loss value: 3.931357E-05 | lm loss PPL: 1.000039E+00 | 
 at iteration 50500, match long value: 0.0007611334828888072 | match short value: -0.033295250379607615 
---------------------------------------------------------------------------------------------------------
 iteration    50600/  200000 | consumed samples:      1619200 | elapsed time per iteration (ms): 272.4 | learning rate: 5.415E-05 | global batch size:    32 | lm loss: 3.472134E-05 | loss scale: 67108864.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.56 | backward-params-all-reduce: 66.01 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.52 | optimizer-unscale-and-check-inf: 16.12 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 97.81 | batch-generator: 15.86
 iteration    50700/  200000 | consumed samples:      1622400 | elapsed time per iteration (ms): 261.1 | learning rate: 5.406E-05 | global batch size:    32 | lm loss: 3.325512E-05 | loss scale: 67108864.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.90 | backward-params-all-reduce: 65.85 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.51 | optimizer-unscale-and-check-inf: 15.66 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 97.43 | batch-generator: 8.54
 iteration    50800/  200000 | consumed samples:      1625600 | elapsed time per iteration (ms): 257.4 | learning rate: 5.397E-05 | global batch size:    32 | lm loss: 3.433150E-05 | loss scale: 67108864.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.19 | backward-params-all-reduce: 63.62 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.86 | optimizer-unscale-and-check-inf: 17.15 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 99.13 | batch-generator: 8.41
 iteration    50900/  200000 | consumed samples:      1628800 | elapsed time per iteration (ms): 261.5 | learning rate: 5.388E-05 | global batch size:    32 | lm loss: 3.347510E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.17 | backward-params-all-reduce: 64.66 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.64 | optimizer-unscale-and-check-inf: 16.61 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 14.27 | optimizer: 100.22 | batch-generator: 8.56
 iteration    51000/  200000 | consumed samples:      1632000 | elapsed time per iteration (ms): 257.2 | learning rate: 5.379E-05 | global batch size:    32 | lm loss: 3.251721E-05 | loss scale: 67108864.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.09 | backward-params-all-reduce: 62.67 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.22 | optimizer-unscale-and-check-inf: 16.52 | optimizer-clip-main-grad: 7.54 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 98.90 | batch-generator: 8.46
-------------------------------------------------------------------------------------------------
 validation loss at iteration 51000 | lm loss value: 3.630639E-05 | lm loss PPL: 1.000036E+00 | 
 at iteration 51000, match long value: -0.0022877324263111004 | match short value: -0.012294488694367925 
----------------------------------------------------------------------------------------------------------
 iteration    51100/  200000 | consumed samples:      1635200 | elapsed time per iteration (ms): 271.5 | learning rate: 5.370E-05 | global batch size:    32 | lm loss: 3.246310E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.48 | backward-params-all-reduce: 64.11 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.24 | optimizer-unscale-and-check-inf: 16.18 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 97.69 | batch-generator: 15.27
 iteration    51200/  200000 | consumed samples:      1638400 | elapsed time per iteration (ms): 260.5 | learning rate: 5.361E-05 | global batch size:    32 | lm loss: 3.263063E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.57 | backward-params-all-reduce: 64.67 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.50 | optimizer-unscale-and-check-inf: 15.64 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 97.32 | batch-generator: 8.49
 iteration    51300/  200000 | consumed samples:      1641600 | elapsed time per iteration (ms): 258.6 | learning rate: 5.352E-05 | global batch size:    32 | lm loss: 3.215081E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.30 | backward-params-all-reduce: 63.19 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.78 | optimizer-unscale-and-check-inf: 16.17 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 98.13 | batch-generator: 8.44
 iteration    51400/  200000 | consumed samples:      1644800 | elapsed time per iteration (ms): 258.5 | learning rate: 5.343E-05 | global batch size:    32 | lm loss: 3.282721E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.17 | backward-params-all-reduce: 63.83 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.51 | optimizer-unscale-and-check-inf: 15.16 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 96.87 | batch-generator: 8.59
 iteration    51500/  200000 | consumed samples:      1648000 | elapsed time per iteration (ms): 258.6 | learning rate: 5.333E-05 | global batch size:    32 | lm loss: 3.243678E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.24 | backward-params-all-reduce: 63.91 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.15 | optimizer-unscale-and-check-inf: 15.74 | optimizer-clip-main-grad: 7.54 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 97.07 | batch-generator: 8.50
-------------------------------------------------------------------------------------------------
 validation loss at iteration 51500 | lm loss value: 3.631311E-05 | lm loss PPL: 1.000036E+00 | 
 at iteration 51500, match long value: -0.00593802933459562 | match short value: 0.0037645204906484177 
--------------------------------------------------------------------------------------------------------
 iteration    51600/  200000 | consumed samples:      1651200 | elapsed time per iteration (ms): 280.6 | learning rate: 5.324E-05 | global batch size:    32 | lm loss: 3.231509E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.49 | backward-params-all-reduce: 65.52 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.18 | optimizer-unscale-and-check-inf: 15.95 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 14.26 | optimizer: 99.16 | batch-generator: 18.44
 iteration    51700/  200000 | consumed samples:      1654400 | elapsed time per iteration (ms): 260.5 | learning rate: 5.315E-05 | global batch size:    32 | lm loss: 3.162740E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.73 | backward-params-all-reduce: 63.66 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.57 | optimizer-unscale-and-check-inf: 15.70 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.45 | optimizer: 97.43 | batch-generator: 8.41
 iteration    51800/  200000 | consumed samples:      1657600 | elapsed time per iteration (ms): 259.4 | learning rate: 5.306E-05 | global batch size:    32 | lm loss: 3.268739E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.16 | backward-params-all-reduce: 63.33 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.27 | optimizer-unscale-and-check-inf: 15.14 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 96.66 | batch-generator: 8.48
 iteration    51900/  200000 | consumed samples:      1660800 | elapsed time per iteration (ms): 259.0 | learning rate: 5.297E-05 | global batch size:    32 | lm loss: 3.162472E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.22 | backward-params-all-reduce: 63.04 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.26 | optimizer-unscale-and-check-inf: 15.68 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 97.14 | batch-generator: 8.49
 iteration    52000/  200000 | consumed samples:      1664000 | elapsed time per iteration (ms): 257.8 | learning rate: 5.288E-05 | global batch size:    32 | lm loss: 3.314148E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.00 | backward-params-all-reduce: 63.60 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.61 | optimizer-unscale-and-check-inf: 15.58 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 97.41 | batch-generator: 8.56
-------------------------------------------------------------------------------------------------
 validation loss at iteration 52000 | lm loss value: 3.423032E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 52000, match long value: 0.0016959624485344091 | match short value: 0.0016801670540078553 
---------------------------------------------------------------------------------------------------------
 iteration    52100/  200000 | consumed samples:      1667200 | elapsed time per iteration (ms): 272.0 | learning rate: 5.279E-05 | global batch size:    32 | lm loss: 3.178449E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.21 | backward-params-all-reduce: 65.31 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.00 | optimizer-unscale-and-check-inf: 15.12 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.45 | optimizer: 97.30 | batch-generator: 14.59
 iteration    52200/  200000 | consumed samples:      1670400 | elapsed time per iteration (ms): 260.4 | learning rate: 5.270E-05 | global batch size:    32 | lm loss: 3.119408E-05 | loss scale: 268435456.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.47 | backward-params-all-reduce: 62.89 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.48 | optimizer-unscale-and-check-inf: 16.18 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 97.93 | batch-generator: 8.52
 iteration    52300/  200000 | consumed samples:      1673600 | elapsed time per iteration (ms): 265.9 | learning rate: 5.261E-05 | global batch size:    32 | lm loss: 3.109957E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.72 | backward-params-all-reduce: 62.60 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.21 | optimizer-unscale-and-check-inf: 16.56 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 14.25 | optimizer: 99.76 | batch-generator: 8.55
 iteration    52400/  200000 | consumed samples:      1676800 | elapsed time per iteration (ms): 259.4 | learning rate: 5.252E-05 | global batch size:    32 | lm loss: 3.117760E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.93 | backward-params-all-reduce: 64.13 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.54 | optimizer-unscale-and-check-inf: 15.18 | optimizer-clip-main-grad: 7.53 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 96.90 | batch-generator: 8.60
 iteration    52500/  200000 | consumed samples:      1680000 | elapsed time per iteration (ms): 260.1 | learning rate: 5.242E-05 | global batch size:    32 | lm loss: 3.263976E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.13 | backward-params-all-reduce: 64.20 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.57 | optimizer-unscale-and-check-inf: 16.15 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.45 | optimizer: 97.93 | batch-generator: 8.57
-------------------------------------------------------------------------------------------------
 validation loss at iteration 52500 | lm loss value: 3.611017E-05 | lm loss PPL: 1.000036E+00 | 
 at iteration 52500, match long value: 0.003346531602909427 | match short value: -0.002355925544866697 
--------------------------------------------------------------------------------------------------------
 iteration    52600/  200000 | consumed samples:      1683200 | elapsed time per iteration (ms): 272.2 | learning rate: 5.233E-05 | global batch size:    32 | lm loss: 3.230300E-05 | loss scale: 268435456.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.59 | backward-params-all-reduce: 64.46 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.24 | optimizer-unscale-and-check-inf: 16.63 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.09 | batch-generator: 14.22
 iteration    52700/  200000 | consumed samples:      1686400 | elapsed time per iteration (ms): 256.3 | learning rate: 5.224E-05 | global batch size:    32 | lm loss: 3.140176E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.45 | backward-params-all-reduce: 62.42 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.55 | optimizer-unscale-and-check-inf: 15.41 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 97.13 | batch-generator: 8.55
 iteration    52800/  200000 | consumed samples:      1689600 | elapsed time per iteration (ms): 259.1 | learning rate: 5.215E-05 | global batch size:    32 | lm loss: 3.099062E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.32 | backward-params-all-reduce: 62.89 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.37 | optimizer-unscale-and-check-inf: 16.61 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.28 | batch-generator: 8.53
 iteration    52900/  200000 | consumed samples:      1692800 | elapsed time per iteration (ms): 256.2 | learning rate: 5.206E-05 | global batch size:    32 | lm loss: 3.143043E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.35 | backward-params-all-reduce: 62.74 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.23 | optimizer-unscale-and-check-inf: 15.45 | optimizer-clip-main-grad: 7.51 | optimizer-copy-main-to-model-params: 12.44 | optimizer: 96.76 | batch-generator: 8.49
 iteration    53000/  200000 | consumed samples:      1696000 | elapsed time per iteration (ms): 259.6 | learning rate: 5.197E-05 | global batch size:    32 | lm loss: 3.130414E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.24 | backward-params-all-reduce: 65.00 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.71 | optimizer-unscale-and-check-inf: 14.73 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.62 | optimizer: 96.80 | batch-generator: 8.61
-------------------------------------------------------------------------------------------------
 validation loss at iteration 53000 | lm loss value: 3.211614E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 53000, match long value: 0.014250332917911048 | match short value: -0.03429803780140067 
-------------------------------------------------------------------------------------------------------
 iteration    53100/  200000 | consumed samples:      1699200 | elapsed time per iteration (ms): 280.9 | learning rate: 5.188E-05 | global batch size:    32 | lm loss: 3.465336E-05 | loss scale: 134217728.0 | grad norm: 0.010 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.77 | backward-params-all-reduce: 63.46 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.55 | optimizer-unscale-and-check-inf: 15.53 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.23 | optimizer: 97.38 | batch-generator: 19.39
 iteration    53200/  200000 | consumed samples:      1702400 | elapsed time per iteration (ms): 266.2 | learning rate: 5.179E-05 | global batch size:    32 | lm loss: 2.903751E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.29 | backward-params-all-reduce: 64.64 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.67 | optimizer-unscale-and-check-inf: 18.40 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 100.40 | batch-generator: 14.32
 iteration    53300/  200000 | consumed samples:      1705600 | elapsed time per iteration (ms): 261.9 | learning rate: 5.170E-05 | global batch size:    32 | lm loss: 2.816673E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.20 | backward-params-all-reduce: 64.96 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.50 | optimizer-unscale-and-check-inf: 18.85 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 100.60 | batch-generator: 8.52
 iteration    53400/  200000 | consumed samples:      1708800 | elapsed time per iteration (ms): 260.5 | learning rate: 5.161E-05 | global batch size:    32 | lm loss: 2.789237E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.34 | backward-params-all-reduce: 64.14 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.30 | optimizer-unscale-and-check-inf: 18.47 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.54 | optimizer: 100.16 | batch-generator: 8.69
 iteration    53500/  200000 | consumed samples:      1712000 | elapsed time per iteration (ms): 260.4 | learning rate: 5.151E-05 | global batch size:    32 | lm loss: 2.757740E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.00 | backward-params-all-reduce: 64.92 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.21 | optimizer-unscale-and-check-inf: 18.54 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.45 | optimizer: 100.02 | batch-generator: 8.63
-------------------------------------------------------------------------------------------------
 validation loss at iteration 53500 | lm loss value: 3.455305E-05 | lm loss PPL: 1.000035E+00 | 
 at iteration 53500, match long value: 0.009129554536225914 | match short value: 0.09633142989393086 
------------------------------------------------------------------------------------------------------
 iteration    53600/  200000 | consumed samples:      1715200 | elapsed time per iteration (ms): 273.4 | learning rate: 5.142E-05 | global batch size:    32 | lm loss: 2.838440E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.49 | backward-params-all-reduce: 65.01 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.55 | optimizer-unscale-and-check-inf: 17.55 | optimizer-clip-main-grad: 7.74 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.57 | batch-generator: 15.46
 iteration    53700/  200000 | consumed samples:      1718400 | elapsed time per iteration (ms): 261.0 | learning rate: 5.133E-05 | global batch size:    32 | lm loss: 2.802727E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.22 | backward-params-all-reduce: 64.73 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.52 | optimizer-unscale-and-check-inf: 18.49 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 100.27 | batch-generator: 8.58
 iteration    53800/  200000 | consumed samples:      1721600 | elapsed time per iteration (ms): 268.1 | learning rate: 5.124E-05 | global batch size:    32 | lm loss: 2.865051E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.63 | backward-params-all-reduce: 64.30 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.08 | optimizer-unscale-and-check-inf: 18.17 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 14.25 | optimizer: 101.24 | batch-generator: 8.62
 iteration    53900/  200000 | consumed samples:      1724800 | elapsed time per iteration (ms): 263.1 | learning rate: 5.115E-05 | global batch size:    32 | lm loss: 2.813182E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.84 | backward-params-all-reduce: 64.54 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.90 | optimizer-unscale-and-check-inf: 17.10 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.37 | batch-generator: 8.78
 iteration    54000/  200000 | consumed samples:      1728000 | elapsed time per iteration (ms): 261.8 | learning rate: 5.106E-05 | global batch size:    32 | lm loss: 2.888146E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.01 | backward-params-all-reduce: 65.44 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.55 | optimizer-unscale-and-check-inf: 17.93 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.55 | optimizer: 99.86 | batch-generator: 8.65
-------------------------------------------------------------------------------------------------
 validation loss at iteration 54000 | lm loss value: 3.552485E-05 | lm loss PPL: 1.000036E+00 | 
 at iteration 54000, match long value: 2.5609448568392983e-05 | match short value: -0.02529387945458304 
---------------------------------------------------------------------------------------------------------
 iteration    54100/  200000 | consumed samples:      1731200 | elapsed time per iteration (ms): 274.9 | learning rate: 5.097E-05 | global batch size:    32 | lm loss: 2.894122E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.31 | backward-params-all-reduce: 66.10 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.04 | optimizer-unscale-and-check-inf: 17.12 | optimizer-clip-main-grad: 7.48 | optimizer-copy-main-to-model-params: 12.23 | optimizer: 98.08 | batch-generator: 15.24
 iteration    54200/  200000 | consumed samples:      1734400 | elapsed time per iteration (ms): 259.9 | learning rate: 5.088E-05 | global batch size:    32 | lm loss: 2.813402E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.69 | backward-params-all-reduce: 64.75 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 11.98 | optimizer-unscale-and-check-inf: 17.95 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 99.16 | batch-generator: 8.52
 iteration    54300/  200000 | consumed samples:      1737600 | elapsed time per iteration (ms): 261.6 | learning rate: 5.079E-05 | global batch size:    32 | lm loss: 2.825975E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.37 | backward-params-all-reduce: 64.95 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.14 | optimizer-unscale-and-check-inf: 18.99 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 100.37 | batch-generator: 8.60
 iteration    54400/  200000 | consumed samples:      1740800 | elapsed time per iteration (ms): 260.7 | learning rate: 5.070E-05 | global batch size:    32 | lm loss: 2.808478E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.23 | backward-params-all-reduce: 64.57 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.34 | optimizer-unscale-and-check-inf: 18.62 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 100.32 | batch-generator: 8.69
 iteration    54500/  200000 | consumed samples:      1744000 | elapsed time per iteration (ms): 270.5 | learning rate: 5.061E-05 | global batch size:    32 | lm loss: 2.943445E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.63 | backward-params-all-reduce: 64.93 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.95 | optimizer-unscale-and-check-inf: 17.06 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 14.40 | optimizer: 101.30 | batch-generator: 8.74
-------------------------------------------------------------------------------------------------
 validation loss at iteration 54500 | lm loss value: 3.297637E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 54500, match long value: 0.0021682210839962385 | match short value: 0.003251178282490509 
--------------------------------------------------------------------------------------------------------
 iteration    54600/  200000 | consumed samples:      1747200 | elapsed time per iteration (ms): 279.3 | learning rate: 5.051E-05 | global batch size:    32 | lm loss: 2.820653E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.22 | backward-params-all-reduce: 66.37 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.28 | optimizer-unscale-and-check-inf: 18.20 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.75 | batch-generator: 21.73
 iteration    54700/  200000 | consumed samples:      1750400 | elapsed time per iteration (ms): 260.8 | learning rate: 5.042E-05 | global batch size:    32 | lm loss: 2.779802E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.14 | backward-params-all-reduce: 64.44 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.68 | optimizer-unscale-and-check-inf: 16.85 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.88 | batch-generator: 8.65
 iteration    54800/  200000 | consumed samples:      1753600 | elapsed time per iteration (ms): 262.3 | learning rate: 5.033E-05 | global batch size:    32 | lm loss: 2.797994E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.85 | backward-params-all-reduce: 64.97 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.26 | optimizer-unscale-and-check-inf: 18.67 | optimizer-clip-main-grad: 7.71 | optimizer-copy-main-to-model-params: 12.56 | optimizer: 100.41 | batch-generator: 8.70
 iteration    54900/  200000 | consumed samples:      1756800 | elapsed time per iteration (ms): 260.0 | learning rate: 5.024E-05 | global batch size:    32 | lm loss: 2.805470E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.02 | backward-params-all-reduce: 63.75 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.50 | optimizer-unscale-and-check-inf: 17.65 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 99.46 | batch-generator: 8.49
 iteration    55000/  200000 | consumed samples:      1760000 | elapsed time per iteration (ms): 259.6 | learning rate: 5.015E-05 | global batch size:    32 | lm loss: 2.834374E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.52 | backward-params-all-reduce: 62.99 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.64 | optimizer-unscale-and-check-inf: 17.20 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 99.24 | batch-generator: 8.64
-------------------------------------------------------------------------------------------------
 validation loss at iteration 55000 | lm loss value: 3.428183E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 55000, match long value: -0.00022276614794808895 | match short value: -0.04489625693901334 
----------------------------------------------------------------------------------------------------------
 iteration    55100/  200000 | consumed samples:      1763200 | elapsed time per iteration (ms): 273.7 | learning rate: 5.006E-05 | global batch size:    32 | lm loss: 2.962947E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.44 | backward-params-all-reduce: 64.42 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.43 | optimizer-unscale-and-check-inf: 17.79 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.42 | batch-generator: 14.82
 iteration    55200/  200000 | consumed samples:      1766400 | elapsed time per iteration (ms): 260.9 | learning rate: 4.997E-05 | global batch size:    32 | lm loss: 2.918822E-05 | loss scale: 268435456.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.51 | backward-params-all-reduce: 63.45 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.62 | optimizer-unscale-and-check-inf: 18.22 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.56 | optimizer: 100.24 | batch-generator: 8.65
 iteration    55300/  200000 | consumed samples:      1769600 | elapsed time per iteration (ms): 270.1 | learning rate: 4.988E-05 | global batch size:    32 | lm loss: 2.923745E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.79 | backward-params-all-reduce: 63.58 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.27 | optimizer-unscale-and-check-inf: 16.64 | optimizer-clip-main-grad: 7.74 | optimizer-copy-main-to-model-params: 14.32 | optimizer: 101.15 | batch-generator: 8.87
 iteration    55400/  200000 | consumed samples:      1772800 | elapsed time per iteration (ms): 259.7 | learning rate: 4.979E-05 | global batch size:    32 | lm loss: 2.844932E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.03 | backward-params-all-reduce: 63.60 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.43 | optimizer-unscale-and-check-inf: 17.19 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 98.89 | batch-generator: 8.61
 iteration    55500/  200000 | consumed samples:      1776000 | elapsed time per iteration (ms): 261.4 | learning rate: 4.969E-05 | global batch size:    32 | lm loss: 2.862107E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.33 | backward-params-all-reduce: 64.47 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.24 | optimizer-unscale-and-check-inf: 17.77 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 99.39 | batch-generator: 8.55
-------------------------------------------------------------------------------------------------
 validation loss at iteration 55500 | lm loss value: 3.360387E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 55500, match long value: 0.0015557827960836518 | match short value: -0.02070587101137078 
--------------------------------------------------------------------------------------------------------
 iteration    55600/  200000 | consumed samples:      1779200 | elapsed time per iteration (ms): 273.5 | learning rate: 4.960E-05 | global batch size:    32 | lm loss: 2.725533E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.24 | backward-params-all-reduce: 63.18 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.21 | optimizer-unscale-and-check-inf: 20.36 | optimizer-clip-main-grad: 7.71 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 101.98 | batch-generator: 15.36
 iteration    55700/  200000 | consumed samples:      1782400 | elapsed time per iteration (ms): 262.2 | learning rate: 4.951E-05 | global batch size:    32 | lm loss: 2.792501E-05 | loss scale: 268435456.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.71 | backward-params-all-reduce: 65.62 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.16 | optimizer-unscale-and-check-inf: 20.21 | optimizer-clip-main-grad: 7.76 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 101.80 | batch-generator: 8.54
 iteration    55800/  200000 | consumed samples:      1785600 | elapsed time per iteration (ms): 318.0 | learning rate: 4.942E-05 | global batch size:    32 | lm loss: 2.780663E-05 | loss scale: 268435456.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.98 | backward-params-all-reduce: 64.99 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.15 | optimizer-unscale-and-check-inf: 19.31 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 100.26 | batch-generator: 63.84
 iteration    55900/  200000 | consumed samples:      1788800 | elapsed time per iteration (ms): 259.8 | learning rate: 4.933E-05 | global batch size:    32 | lm loss: 2.977312E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.50 | backward-params-all-reduce: 63.05 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.93 | optimizer-unscale-and-check-inf: 18.85 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.40 | optimizer: 100.50 | batch-generator: 8.67
 iteration    56000/  200000 | consumed samples:      1792000 | elapsed time per iteration (ms): 267.4 | learning rate: 4.924E-05 | global batch size:    32 | lm loss: 2.764676E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.53 | backward-params-all-reduce: 64.13 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.34 | optimizer-unscale-and-check-inf: 18.12 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 14.22 | optimizer: 101.45 | batch-generator: 8.58
-------------------------------------------------------------------------------------------------
 validation loss at iteration 56000 | lm loss value: 3.346371E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 56000, match long value: -8.005774184331682e-05 | match short value: -0.0002739234378224051 
-----------------------------------------------------------------------------------------------------------
 iteration    56100/  200000 | consumed samples:      1795200 | elapsed time per iteration (ms): 276.1 | learning rate: 4.915E-05 | global batch size:    32 | lm loss: 2.993342E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.42 | backward-params-all-reduce: 63.60 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.94 | optimizer-unscale-and-check-inf: 18.18 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.55 | optimizer: 100.51 | batch-generator: 19.90
 iteration    56200/  200000 | consumed samples:      1798400 | elapsed time per iteration (ms): 258.2 | learning rate: 4.906E-05 | global batch size:    32 | lm loss: 2.982242E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.42 | backward-params-all-reduce: 63.05 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.22 | optimizer-unscale-and-check-inf: 16.40 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.96 | batch-generator: 8.64
 iteration    56300/  200000 | consumed samples:      1801600 | elapsed time per iteration (ms): 265.5 | learning rate: 4.897E-05 | global batch size:    32 | lm loss: 2.596006E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.66 | backward-params-all-reduce: 64.95 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.55 | optimizer-unscale-and-check-inf: 16.64 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 99.47 | batch-generator: 14.28
 iteration    56400/  200000 | consumed samples:      1804800 | elapsed time per iteration (ms): 261.8 | learning rate: 4.888E-05 | global batch size:    32 | lm loss: 2.569772E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.88 | backward-params-all-reduce: 66.72 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.17 | optimizer-unscale-and-check-inf: 17.10 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 99.62 | batch-generator: 8.58
 iteration    56500/  200000 | consumed samples:      1808000 | elapsed time per iteration (ms): 259.0 | learning rate: 4.879E-05 | global batch size:    32 | lm loss: 2.591957E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.50 | backward-params-all-reduce: 65.03 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.49 | optimizer-unscale-and-check-inf: 16.97 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.77 | batch-generator: 8.61
-------------------------------------------------------------------------------------------------
 validation loss at iteration 56500 | lm loss value: 3.329663E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 56500, match long value: 0.006349142887351076 | match short value: -0.008005798417374473 
--------------------------------------------------------------------------------------------------------
 iteration    56600/  200000 | consumed samples:      1811200 | elapsed time per iteration (ms): 271.9 | learning rate: 4.869E-05 | global batch size:    32 | lm loss: 2.482409E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.51 | backward-params-all-reduce: 65.87 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.37 | optimizer-unscale-and-check-inf: 16.13 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.77 | batch-generator: 15.24
 iteration    56700/  200000 | consumed samples:      1814400 | elapsed time per iteration (ms): 275.0 | learning rate: 4.860E-05 | global batch size:    32 | lm loss: 2.498831E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.11 | backward-params-all-reduce: 66.70 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.28 | optimizer-unscale-and-check-inf: 16.16 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 14.54 | optimizer: 100.72 | batch-generator: 8.60
 iteration    56800/  200000 | consumed samples:      1817600 | elapsed time per iteration (ms): 260.5 | learning rate: 4.851E-05 | global batch size:    32 | lm loss: 2.493582E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.43 | backward-params-all-reduce: 65.60 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.50 | optimizer-unscale-and-check-inf: 16.63 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.41 | batch-generator: 8.56
 iteration    56900/  200000 | consumed samples:      1820800 | elapsed time per iteration (ms): 260.0 | learning rate: 4.842E-05 | global batch size:    32 | lm loss: 2.512801E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.27 | backward-params-all-reduce: 65.25 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.26 | optimizer-unscale-and-check-inf: 16.61 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 99.30 | batch-generator: 8.71
 iteration    57000/  200000 | consumed samples:      1824000 | elapsed time per iteration (ms): 260.0 | learning rate: 4.833E-05 | global batch size:    32 | lm loss: 2.585618E-05 | loss scale: 268435456.0 | grad norm: 0.003 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.23 | backward-params-all-reduce: 65.73 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 14.12 | optimizer-unscale-and-check-inf: 16.17 | optimizer-clip-main-grad: 7.71 | optimizer-copy-main-to-model-params: 12.38 | optimizer: 99.10 | batch-generator: 8.82
-------------------------------------------------------------------------------------------------
 validation loss at iteration 57000 | lm loss value: 3.210598E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 57000, match long value: 0.006423102972635553 | match short value: 0.00735749275531589 
------------------------------------------------------------------------------------------------------
 iteration    57100/  200000 | consumed samples:      1827200 | elapsed time per iteration (ms): 272.8 | learning rate: 4.824E-05 | global batch size:    32 | lm loss: 2.661275E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.73 | backward-params-all-reduce: 65.26 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.22 | optimizer-unscale-and-check-inf: 16.64 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.06 | batch-generator: 15.72
 iteration    57200/  200000 | consumed samples:      1830400 | elapsed time per iteration (ms): 261.4 | learning rate: 4.815E-05 | global batch size:    32 | lm loss: 2.510416E-05 | loss scale: 268435456.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.49 | backward-params-all-reduce: 65.26 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 14.10 | optimizer-unscale-and-check-inf: 15.90 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 99.38 | batch-generator: 8.75
 iteration    57300/  200000 | consumed samples:      1833600 | elapsed time per iteration (ms): 259.2 | learning rate: 4.806E-05 | global batch size:    32 | lm loss: 2.559251E-05 | loss scale: 268435456.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.16 | backward-params-all-reduce: 64.20 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.84 | optimizer-unscale-and-check-inf: 16.18 | optimizer-clip-main-grad: 7.71 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.37 | batch-generator: 8.83
 iteration    57400/  200000 | consumed samples:      1836800 | elapsed time per iteration (ms): 266.8 | learning rate: 4.797E-05 | global batch size:    32 | lm loss: 2.581820E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.82 | backward-params-all-reduce: 64.80 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.04 | optimizer-unscale-and-check-inf: 16.63 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 14.28 | optimizer: 100.69 | batch-generator: 8.57
 iteration    57500/  200000 | consumed samples:      1840000 | elapsed time per iteration (ms): 264.6 | learning rate: 4.788E-05 | global batch size:    32 | lm loss: 2.932741E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.03 | backward-params-all-reduce: 63.86 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.18 | optimizer-unscale-and-check-inf: 16.29 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.45 | optimizer: 98.63 | batch-generator: 8.64
-------------------------------------------------------------------------------------------------
 validation loss at iteration 57500 | lm loss value: 3.334847E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 57500, match long value: 0.004609199425954653 | match short value: 0.004273795101956451 
-------------------------------------------------------------------------------------------------------
 iteration    57600/  200000 | consumed samples:      1843200 | elapsed time per iteration (ms): 274.3 | learning rate: 4.778E-05 | global batch size:    32 | lm loss: 2.491346E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.42 | backward-params-all-reduce: 64.94 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.61 | optimizer-unscale-and-check-inf: 15.86 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 98.67 | batch-generator: 19.27
 iteration    57700/  200000 | consumed samples:      1846400 | elapsed time per iteration (ms): 261.1 | learning rate: 4.769E-05 | global batch size:    32 | lm loss: 2.517662E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.39 | backward-params-all-reduce: 63.93 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.63 | optimizer-unscale-and-check-inf: 15.86 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 98.84 | batch-generator: 8.68
 iteration    57800/  200000 | consumed samples:      1849600 | elapsed time per iteration (ms): 259.9 | learning rate: 4.760E-05 | global batch size:    32 | lm loss: 2.551745E-05 | loss scale: 67108864.0 | grad norm: 0.002 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.17 | backward-params-all-reduce: 65.11 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.33 | optimizer-unscale-and-check-inf: 18.18 | optimizer-clip-main-grad: 7.51 | optimizer-copy-main-to-model-params: 12.25 | optimizer: 98.47 | batch-generator: 8.55
 iteration    57900/  200000 | consumed samples:      1852800 | elapsed time per iteration (ms): 260.2 | learning rate: 4.751E-05 | global batch size:    32 | lm loss: 2.634928E-05 | loss scale: 67108864.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.17 | backward-params-all-reduce: 64.64 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.67 | optimizer-unscale-and-check-inf: 17.96 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.98 | batch-generator: 8.52
 iteration    58000/  200000 | consumed samples:      1856000 | elapsed time per iteration (ms): 259.9 | learning rate: 4.742E-05 | global batch size:    32 | lm loss: 2.571857E-05 | loss scale: 67108864.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.24 | backward-params-all-reduce: 63.86 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.26 | optimizer-unscale-and-check-inf: 18.84 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 100.42 | batch-generator: 8.57
-------------------------------------------------------------------------------------------------
 validation loss at iteration 58000 | lm loss value: 3.217166E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 58000, match long value: 0.004916563279321938 | match short value: 0.0006264619625131976 
--------------------------------------------------------------------------------------------------------
 iteration    58100/  200000 | consumed samples:      1859200 | elapsed time per iteration (ms): 272.1 | learning rate: 4.733E-05 | global batch size:    32 | lm loss: 2.542329E-05 | loss scale: 67108864.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.80 | backward-params-all-reduce: 65.92 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.01 | optimizer-unscale-and-check-inf: 16.78 | optimizer-clip-main-grad: 7.53 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 97.93 | batch-generator: 16.58
 iteration    58200/  200000 | consumed samples:      1862400 | elapsed time per iteration (ms): 272.6 | learning rate: 4.724E-05 | global batch size:    32 | lm loss: 2.507678E-05 | loss scale: 67108864.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.95 | backward-params-all-reduce: 63.46 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.33 | optimizer-unscale-and-check-inf: 17.21 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 14.26 | optimizer: 100.56 | batch-generator: 8.59
 iteration    58300/  200000 | consumed samples:      1865600 | elapsed time per iteration (ms): 261.8 | learning rate: 4.715E-05 | global batch size:    32 | lm loss: 2.495010E-05 | loss scale: 67108864.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.40 | backward-params-all-reduce: 64.30 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.46 | optimizer-unscale-and-check-inf: 18.32 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 100.08 | batch-generator: 8.62
 iteration    58400/  200000 | consumed samples:      1868800 | elapsed time per iteration (ms): 259.4 | learning rate: 4.706E-05 | global batch size:    32 | lm loss: 2.536243E-05 | loss scale: 67108864.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.35 | backward-params-all-reduce: 64.39 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 11.99 | optimizer-unscale-and-check-inf: 17.21 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.47 | batch-generator: 8.66
 iteration    58500/  200000 | consumed samples:      1872000 | elapsed time per iteration (ms): 257.9 | learning rate: 4.697E-05 | global batch size:    32 | lm loss: 2.535489E-05 | loss scale: 67108864.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.02 | backward-params-all-reduce: 63.68 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 11.95 | optimizer-unscale-and-check-inf: 17.12 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.37 | batch-generator: 8.62
-------------------------------------------------------------------------------------------------
 validation loss at iteration 58500 | lm loss value: 3.362263E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 58500, match long value: -1.4533982792721607e-05 | match short value: -0.00372303059171135 
----------------------------------------------------------------------------------------------------------
 iteration    58600/  200000 | consumed samples:      1875200 | elapsed time per iteration (ms): 276.2 | learning rate: 4.688E-05 | global batch size:    32 | lm loss: 2.491993E-05 | loss scale: 67108864.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.49 | backward-params-all-reduce: 65.06 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.45 | optimizer-unscale-and-check-inf: 17.63 | optimizer-clip-main-grad: 7.71 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.47 | batch-generator: 16.37
 iteration    58700/  200000 | consumed samples:      1878400 | elapsed time per iteration (ms): 258.8 | learning rate: 4.678E-05 | global batch size:    32 | lm loss: 2.577455E-05 | loss scale: 67108864.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.23 | backward-params-all-reduce: 63.94 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.03 | optimizer-unscale-and-check-inf: 16.75 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.07 | batch-generator: 8.72
 iteration    58800/  200000 | consumed samples:      1881600 | elapsed time per iteration (ms): 258.4 | learning rate: 4.669E-05 | global batch size:    32 | lm loss: 2.729534E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.04 | backward-params-all-reduce: 63.58 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.57 | optimizer-unscale-and-check-inf: 16.47 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 98.42 | batch-generator: 8.69
 iteration    58900/  200000 | consumed samples:      1884800 | elapsed time per iteration (ms): 274.3 | learning rate: 4.660E-05 | global batch size:    32 | lm loss: 2.709678E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.75 | backward-params-all-reduce: 63.98 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.56 | optimizer-unscale-and-check-inf: 16.53 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 14.20 | optimizer: 100.13 | batch-generator: 8.70
 iteration    59000/  200000 | consumed samples:      1888000 | elapsed time per iteration (ms): 257.2 | learning rate: 4.651E-05 | global batch size:    32 | lm loss: 2.667556E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.21 | backward-params-all-reduce: 63.03 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.23 | optimizer-unscale-and-check-inf: 15.64 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 97.17 | batch-generator: 8.77
-------------------------------------------------------------------------------------------------
 validation loss at iteration 59000 | lm loss value: 3.578262E-05 | lm loss PPL: 1.000036E+00 | 
 at iteration 59000, match long value: 0.0043775422426334445 | match short value: -0.0006861279072046473 
----------------------------------------------------------------------------------------------------------
 iteration    59100/  200000 | consumed samples:      1891200 | elapsed time per iteration (ms): 276.6 | learning rate: 4.642E-05 | global batch size:    32 | lm loss: 2.617151E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.98 | backward-params-all-reduce: 63.09 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.17 | optimizer-unscale-and-check-inf: 16.54 | optimizer-clip-main-grad: 7.70 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.10 | batch-generator: 23.01
 iteration    59200/  200000 | consumed samples:      1894400 | elapsed time per iteration (ms): 260.0 | learning rate: 4.633E-05 | global batch size:    32 | lm loss: 2.564677E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.11 | backward-params-all-reduce: 63.43 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.40 | optimizer-unscale-and-check-inf: 17.20 | optimizer-clip-main-grad: 7.74 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 99.06 | batch-generator: 8.73
 iteration    59300/  200000 | consumed samples:      1897600 | elapsed time per iteration (ms): 262.3 | learning rate: 4.624E-05 | global batch size:    32 | lm loss: 2.581307E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.62 | backward-params-all-reduce: 63.94 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.61 | optimizer-unscale-and-check-inf: 17.10 | optimizer-clip-main-grad: 7.79 | optimizer-copy-main-to-model-params: 12.56 | optimizer: 99.24 | batch-generator: 8.68
 iteration    59400/  200000 | consumed samples:      1900800 | elapsed time per iteration (ms): 266.1 | learning rate: 4.615E-05 | global batch size:    32 | lm loss: 2.512352E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.40 | backward-params-all-reduce: 63.34 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.58 | optimizer-unscale-and-check-inf: 16.69 | optimizer-clip-main-grad: 7.73 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.65 | batch-generator: 14.48
 iteration    59500/  200000 | consumed samples:      1904000 | elapsed time per iteration (ms): 261.4 | learning rate: 4.606E-05 | global batch size:    32 | lm loss: 2.361006E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.31 | backward-params-all-reduce: 65.48 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.69 | optimizer-unscale-and-check-inf: 16.97 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.94 | batch-generator: 8.59
-------------------------------------------------------------------------------------------------
 validation loss at iteration 59500 | lm loss value: 3.305962E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 59500, match long value: -0.006318824580767095 | match short value: -0.04167884248687526 
--------------------------------------------------------------------------------------------------------
 iteration    59600/  200000 | consumed samples:      1907200 | elapsed time per iteration (ms): 285.8 | learning rate: 4.596E-05 | global batch size:    32 | lm loss: 2.354023E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.16 | backward-params-all-reduce: 64.71 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.39 | optimizer-unscale-and-check-inf: 16.60 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 14.37 | optimizer: 100.16 | batch-generator: 16.41
 iteration    59700/  200000 | consumed samples:      1910400 | elapsed time per iteration (ms): 259.8 | learning rate: 4.587E-05 | global batch size:    32 | lm loss: 2.315369E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.39 | backward-params-all-reduce: 63.38 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.26 | optimizer-unscale-and-check-inf: 18.05 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.63 | batch-generator: 8.57
 iteration    59800/  200000 | consumed samples:      1913600 | elapsed time per iteration (ms): 260.9 | learning rate: 4.578E-05 | global batch size:    32 | lm loss: 2.310093E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.56 | backward-params-all-reduce: 65.07 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.18 | optimizer-unscale-and-check-inf: 16.97 | optimizer-clip-main-grad: 7.74 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 98.59 | batch-generator: 8.71
 iteration    59900/  200000 | consumed samples:      1916800 | elapsed time per iteration (ms): 262.3 | learning rate: 4.569E-05 | global batch size:    32 | lm loss: 2.292813E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.12 | backward-params-all-reduce: 65.40 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.24 | optimizer-unscale-and-check-inf: 17.94 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 99.43 | batch-generator: 8.63
 iteration    60000/  200000 | consumed samples:      1920000 | elapsed time per iteration (ms): 261.1 | learning rate: 4.560E-05 | global batch size:    32 | lm loss: 2.340478E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.20 | backward-params-all-reduce: 64.05 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.92 | optimizer-unscale-and-check-inf: 17.66 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 12.55 | optimizer: 99.96 | batch-generator: 8.61
-------------------------------------------------------------------------------------------------
 validation loss at iteration 60000 | lm loss value: 3.354006E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 60000, match long value: -0.0019124117881839783 | match short value: -0.07975414148747834 
---------------------------------------------------------------------------------------------------------
saving checkpoint at iteration   60000 to verify1
  successfully saved checkpoint at iteration   60000 to verify1
time (ms) | save-checkpoint: 24535.56
 iteration    60100/  200000 | consumed samples:      1923200 | elapsed time per iteration (ms): 518.6 | learning rate: 4.551E-05 | global batch size:    32 | lm loss: 2.325891E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.15 | backward-params-all-reduce: 64.19 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.74 | optimizer-unscale-and-check-inf: 17.61 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.61 | batch-generator: 16.25
 iteration    60200/  200000 | consumed samples:      1926400 | elapsed time per iteration (ms): 260.3 | learning rate: 4.542E-05 | global batch size:    32 | lm loss: 2.323302E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.38 | backward-params-all-reduce: 64.64 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.54 | optimizer-unscale-and-check-inf: 16.88 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 98.60 | batch-generator: 8.56
 iteration    60300/  200000 | consumed samples:      1929600 | elapsed time per iteration (ms): 260.0 | learning rate: 4.533E-05 | global batch size:    32 | lm loss: 2.350875E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.74 | backward-params-all-reduce: 64.13 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.80 | optimizer-unscale-and-check-inf: 17.12 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.29 | optimizer: 97.98 | batch-generator: 8.63
 iteration    60400/  200000 | consumed samples:      1932800 | elapsed time per iteration (ms): 270.5 | learning rate: 4.524E-05 | global batch size:    32 | lm loss: 2.389142E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.66 | backward-params-all-reduce: 63.84 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.04 | optimizer-unscale-and-check-inf: 17.68 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 14.23 | optimizer: 100.71 | batch-generator: 8.68
 iteration    60500/  200000 | consumed samples:      1936000 | elapsed time per iteration (ms): 260.3 | learning rate: 4.515E-05 | global batch size:    32 | lm loss: 2.482510E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.26 | backward-params-all-reduce: 63.56 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.97 | optimizer-unscale-and-check-inf: 17.56 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.90 | batch-generator: 8.59
-------------------------------------------------------------------------------------------------
 validation loss at iteration 60500 | lm loss value: 3.224312E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 60500, match long value: -0.0015868224732846052 | match short value: -0.028589354066260342 
----------------------------------------------------------------------------------------------------------
 iteration    60600/  200000 | consumed samples:      1939200 | elapsed time per iteration (ms): 277.6 | learning rate: 4.506E-05 | global batch size:    32 | lm loss: 2.641748E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.32 | backward-params-all-reduce: 63.61 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.62 | optimizer-unscale-and-check-inf: 16.47 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 98.27 | batch-generator: 21.08
 iteration    60700/  200000 | consumed samples:      1942400 | elapsed time per iteration (ms): 260.6 | learning rate: 4.496E-05 | global batch size:    32 | lm loss: 2.294442E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.18 | backward-params-all-reduce: 63.71 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.51 | optimizer-unscale-and-check-inf: 17.09 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.86 | batch-generator: 8.64
 iteration    60800/  200000 | consumed samples:      1945600 | elapsed time per iteration (ms): 262.3 | learning rate: 4.487E-05 | global batch size:    32 | lm loss: 2.296686E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.12 | backward-params-all-reduce: 64.34 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.63 | optimizer-unscale-and-check-inf: 15.98 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 97.96 | batch-generator: 8.71
 iteration    60900/  200000 | consumed samples:      1948800 | elapsed time per iteration (ms): 259.7 | learning rate: 4.478E-05 | global batch size:    32 | lm loss: 2.342934E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.76 | backward-params-all-reduce: 64.35 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.16 | optimizer-unscale-and-check-inf: 15.89 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 97.23 | batch-generator: 8.52
 iteration    61000/  200000 | consumed samples:      1952000 | elapsed time per iteration (ms): 258.3 | learning rate: 4.469E-05 | global batch size:    32 | lm loss: 2.308835E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.17 | backward-params-all-reduce: 64.21 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.34 | optimizer-unscale-and-check-inf: 15.58 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 97.24 | batch-generator: 8.66
-------------------------------------------------------------------------------------------------
 validation loss at iteration 61000 | lm loss value: 3.234196E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 61000, match long value: 0.004519019739276598 | match short value: -0.06388128956424448 
-------------------------------------------------------------------------------------------------------
 iteration    61100/  200000 | consumed samples:      1955200 | elapsed time per iteration (ms): 282.9 | learning rate: 4.460E-05 | global batch size:    32 | lm loss: 2.367272E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.14 | backward-params-all-reduce: 64.29 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.46 | optimizer-unscale-and-check-inf: 16.01 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 14.33 | optimizer: 99.50 | batch-generator: 14.08
 iteration    61200/  200000 | consumed samples:      1958400 | elapsed time per iteration (ms): 261.1 | learning rate: 4.451E-05 | global batch size:    32 | lm loss: 2.322854E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.03 | backward-params-all-reduce: 63.53 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.80 | optimizer-unscale-and-check-inf: 16.53 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.56 | batch-generator: 8.52
 iteration    61300/  200000 | consumed samples:      1961600 | elapsed time per iteration (ms): 260.3 | learning rate: 4.442E-05 | global batch size:    32 | lm loss: 2.465101E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.18 | backward-params-all-reduce: 64.92 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.17 | optimizer-unscale-and-check-inf: 16.44 | optimizer-clip-main-grad: 7.46 | optimizer-copy-main-to-model-params: 12.26 | optimizer: 96.50 | batch-generator: 8.55
 iteration    61400/  200000 | consumed samples:      1964800 | elapsed time per iteration (ms): 260.1 | learning rate: 4.433E-05 | global batch size:    32 | lm loss: 2.337756E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.29 | backward-params-all-reduce: 63.51 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.32 | optimizer-unscale-and-check-inf: 17.11 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.62 | batch-generator: 8.56
 iteration    61500/  200000 | consumed samples:      1968000 | elapsed time per iteration (ms): 260.3 | learning rate: 4.424E-05 | global batch size:    32 | lm loss: 2.322491E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.16 | backward-params-all-reduce: 64.56 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.11 | optimizer-unscale-and-check-inf: 16.46 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 97.80 | batch-generator: 8.56
-------------------------------------------------------------------------------------------------
 validation loss at iteration 61500 | lm loss value: 3.204876E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 61500, match long value: 0.006671687448975671 | match short value: 0.0034148451361432617 
--------------------------------------------------------------------------------------------------------
 iteration    61600/  200000 | consumed samples:      1971200 | elapsed time per iteration (ms): 273.8 | learning rate: 4.415E-05 | global batch size:    32 | lm loss: 2.327957E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.46 | backward-params-all-reduce: 64.24 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.77 | optimizer-unscale-and-check-inf: 15.86 | optimizer-clip-main-grad: 7.74 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.13 | batch-generator: 15.92
 iteration    61700/  200000 | consumed samples:      1974400 | elapsed time per iteration (ms): 259.5 | learning rate: 4.406E-05 | global batch size:    32 | lm loss: 2.383987E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.89 | backward-params-all-reduce: 63.89 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.30 | optimizer-unscale-and-check-inf: 16.25 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.55 | optimizer: 97.82 | batch-generator: 8.57
 iteration    61800/  200000 | consumed samples:      1977600 | elapsed time per iteration (ms): 269.9 | learning rate: 4.396E-05 | global batch size:    32 | lm loss: 2.620535E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.66 | backward-params-all-reduce: 63.49 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.22 | optimizer-unscale-and-check-inf: 16.58 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 14.16 | optimizer: 99.77 | batch-generator: 8.60
 iteration    61900/  200000 | consumed samples:      1980800 | elapsed time per iteration (ms): 259.1 | learning rate: 4.387E-05 | global batch size:    32 | lm loss: 2.322281E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.54 | backward-params-all-reduce: 63.19 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.82 | optimizer-unscale-and-check-inf: 16.08 | optimizer-clip-main-grad: 7.78 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 98.40 | batch-generator: 8.71
 iteration    62000/  200000 | consumed samples:      1984000 | elapsed time per iteration (ms): 258.3 | learning rate: 4.378E-05 | global batch size:    32 | lm loss: 2.327476E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.21 | backward-params-all-reduce: 63.29 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.42 | optimizer-unscale-and-check-inf: 16.06 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 97.81 | batch-generator: 8.71
-------------------------------------------------------------------------------------------------
 validation loss at iteration 62000 | lm loss value: 3.372205E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 62000, match long value: 0.0014345506354109226 | match short value: -0.003000859605293811 
---------------------------------------------------------------------------------------------------------
 iteration    62100/  200000 | consumed samples:      1987200 | elapsed time per iteration (ms): 271.5 | learning rate: 4.369E-05 | global batch size:    32 | lm loss: 2.342999E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.20 | backward-params-all-reduce: 64.42 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.35 | optimizer-unscale-and-check-inf: 16.79 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 98.48 | batch-generator: 16.16
 iteration    62200/  200000 | consumed samples:      1990400 | elapsed time per iteration (ms): 262.7 | learning rate: 4.360E-05 | global batch size:    32 | lm loss: 2.389905E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.36 | backward-params-all-reduce: 64.51 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.51 | optimizer-unscale-and-check-inf: 17.48 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.24 | batch-generator: 8.55
 iteration    62300/  200000 | consumed samples:      1993600 | elapsed time per iteration (ms): 258.8 | learning rate: 4.351E-05 | global batch size:    32 | lm loss: 2.363089E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.19 | backward-params-all-reduce: 63.59 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.63 | optimizer-unscale-and-check-inf: 16.15 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 97.96 | batch-generator: 8.56
 iteration    62400/  200000 | consumed samples:      1996800 | elapsed time per iteration (ms): 261.3 | learning rate: 4.342E-05 | global batch size:    32 | lm loss: 2.343121E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.06 | backward-params-all-reduce: 63.95 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.59 | optimizer-unscale-and-check-inf: 16.87 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.74 | batch-generator: 8.59
 iteration    62500/  200000 | consumed samples:      2000000 | elapsed time per iteration (ms): 259.9 | learning rate: 4.333E-05 | global batch size:    32 | lm loss: 2.342746E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.94 | backward-params-all-reduce: 63.58 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.56 | optimizer-unscale-and-check-inf: 16.12 | optimizer-clip-main-grad: 7.72 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 98.11 | batch-generator: 8.72
-------------------------------------------------------------------------------------------------
 validation loss at iteration 62500 | lm loss value: 3.330557E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 62500, match long value: -0.0007760548846742385 | match short value: -0.005035308115312173 
----------------------------------------------------------------------------------------------------------
 iteration    62600/  200000 | consumed samples:      2003200 | elapsed time per iteration (ms): 291.5 | learning rate: 4.324E-05 | global batch size:    32 | lm loss: 2.086816E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.48 | backward-params-all-reduce: 64.59 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.54 | optimizer-unscale-and-check-inf: 16.52 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.45 | optimizer: 99.60 | batch-generator: 24.43
 iteration    62700/  200000 | consumed samples:      2006400 | elapsed time per iteration (ms): 256.4 | learning rate: 4.315E-05 | global batch size:    32 | lm loss: 2.096928E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.42 | backward-params-all-reduce: 62.43 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.63 | optimizer-unscale-and-check-inf: 17.01 | optimizer-clip-main-grad: 7.42 | optimizer-copy-main-to-model-params: 12.24 | optimizer: 97.46 | batch-generator: 8.51
 iteration    62800/  200000 | consumed samples:      2009600 | elapsed time per iteration (ms): 260.6 | learning rate: 4.306E-05 | global batch size:    32 | lm loss: 2.249669E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.21 | backward-params-all-reduce: 64.78 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.17 | optimizer-unscale-and-check-inf: 17.34 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.80 | batch-generator: 8.47
 iteration    62900/  200000 | consumed samples:      2012800 | elapsed time per iteration (ms): 261.3 | learning rate: 4.296E-05 | global batch size:    32 | lm loss: 2.136679E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.88 | backward-params-all-reduce: 64.38 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.43 | optimizer-unscale-and-check-inf: 17.60 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.30 | batch-generator: 8.43
 iteration    63000/  200000 | consumed samples:      2016000 | elapsed time per iteration (ms): 261.2 | learning rate: 4.287E-05 | global batch size:    32 | lm loss: 2.119223E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.35 | backward-params-all-reduce: 64.84 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.41 | optimizer-unscale-and-check-inf: 17.07 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 98.82 | batch-generator: 8.56
-------------------------------------------------------------------------------------------------
 validation loss at iteration 63000 | lm loss value: 3.216812E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 63000, match long value: 0.0019904131510708432 | match short value: -0.050940162589276056 
---------------------------------------------------------------------------------------------------------
 iteration    63100/  200000 | consumed samples:      2019200 | elapsed time per iteration (ms): 272.7 | learning rate: 4.278E-05 | global batch size:    32 | lm loss: 2.109770E-05 | loss scale: 67108864.0 | grad norm: 0.002 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.04 | backward-params-all-reduce: 64.12 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.45 | optimizer-unscale-and-check-inf: 15.74 | optimizer-clip-main-grad: 7.51 | optimizer-copy-main-to-model-params: 12.42 | optimizer: 96.80 | batch-generator: 14.75
 iteration    63200/  200000 | consumed samples:      2022400 | elapsed time per iteration (ms): 260.9 | learning rate: 4.269E-05 | global batch size:    32 | lm loss: 2.125927E-05 | loss scale: 67108864.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.24 | backward-params-all-reduce: 64.47 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.14 | optimizer-unscale-and-check-inf: 16.07 | optimizer-clip-main-grad: 7.93 | optimizer-copy-main-to-model-params: 12.58 | optimizer: 98.88 | batch-generator: 8.56
 iteration    63300/  200000 | consumed samples:      2025600 | elapsed time per iteration (ms): 267.6 | learning rate: 4.260E-05 | global batch size:    32 | lm loss: 2.105846E-05 | loss scale: 67108864.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.53 | backward-params-all-reduce: 63.64 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.59 | optimizer-unscale-and-check-inf: 15.42 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 14.31 | optimizer: 99.10 | batch-generator: 8.51
 iteration    63400/  200000 | consumed samples:      2028800 | elapsed time per iteration (ms): 259.4 | learning rate: 4.251E-05 | global batch size:    32 | lm loss: 2.216850E-05 | loss scale: 67108864.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.06 | backward-params-all-reduce: 63.31 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.51 | optimizer-unscale-and-check-inf: 16.54 | optimizer-clip-main-grad: 7.79 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 98.48 | batch-generator: 8.39
 iteration    63500/  200000 | consumed samples:      2032000 | elapsed time per iteration (ms): 258.7 | learning rate: 4.242E-05 | global batch size:    32 | lm loss: 3.045109E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.22 | backward-params-all-reduce: 63.21 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.53 | optimizer-unscale-and-check-inf: 16.58 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 98.50 | batch-generator: 8.45
-------------------------------------------------------------------------------------------------
 validation loss at iteration 63500 | lm loss value: 3.474240E-05 | lm loss PPL: 1.000035E+00 | 
 at iteration 63500, match long value: -0.000704062993888806 | match short value: -0.004634979083116302 
---------------------------------------------------------------------------------------------------------
 iteration    63600/  200000 | consumed samples:      2035200 | elapsed time per iteration (ms): 271.3 | learning rate: 4.233E-05 | global batch size:    32 | lm loss: 2.186522E-05 | loss scale: 67108864.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.40 | backward-params-all-reduce: 63.17 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.53 | optimizer-unscale-and-check-inf: 16.17 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 97.95 | batch-generator: 16.05
 iteration    63700/  200000 | consumed samples:      2038400 | elapsed time per iteration (ms): 259.0 | learning rate: 4.224E-05 | global batch size:    32 | lm loss: 2.096179E-05 | loss scale: 67108864.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.14 | backward-params-all-reduce: 63.21 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.49 | optimizer-unscale-and-check-inf: 17.52 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 99.20 | batch-generator: 8.50
 iteration    63800/  200000 | consumed samples:      2041600 | elapsed time per iteration (ms): 261.7 | learning rate: 4.215E-05 | global batch size:    32 | lm loss: 2.105513E-05 | loss scale: 67108864.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.22 | backward-params-all-reduce: 64.20 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.33 | optimizer-unscale-and-check-inf: 19.31 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 100.98 | batch-generator: 8.43
 iteration    63900/  200000 | consumed samples:      2044800 | elapsed time per iteration (ms): 260.9 | learning rate: 4.205E-05 | global batch size:    32 | lm loss: 2.109443E-05 | loss scale: 67108864.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.59 | backward-params-all-reduce: 63.26 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.29 | optimizer-unscale-and-check-inf: 19.57 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 101.16 | batch-generator: 8.47
 iteration    64000/  200000 | consumed samples:      2048000 | elapsed time per iteration (ms): 270.6 | learning rate: 4.196E-05 | global batch size:    32 | lm loss: 2.108086E-05 | loss scale: 67108864.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.36 | backward-params-all-reduce: 65.37 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.20 | optimizer-unscale-and-check-inf: 18.66 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 14.37 | optimizer: 101.97 | batch-generator: 8.42
-------------------------------------------------------------------------------------------------
 validation loss at iteration 64000 | lm loss value: 3.322900E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 64000, match long value: -0.0010683049036189116 | match short value: -0.03872766543722509 
---------------------------------------------------------------------------------------------------------
 iteration    64100/  200000 | consumed samples:      2051200 | elapsed time per iteration (ms): 278.0 | learning rate: 4.187E-05 | global batch size:    32 | lm loss: 2.133687E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.23 | backward-params-all-reduce: 64.93 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.21 | optimizer-unscale-and-check-inf: 19.43 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 100.91 | batch-generator: 19.14
 iteration    64200/  200000 | consumed samples:      2054400 | elapsed time per iteration (ms): 316.0 | learning rate: 4.178E-05 | global batch size:    32 | lm loss: 2.152659E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.26 | backward-params-all-reduce: 63.03 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.47 | optimizer-unscale-and-check-inf: 18.92 | optimizer-clip-main-grad: 7.70 | optimizer-copy-main-to-model-params: 12.55 | optimizer: 100.80 | batch-generator: 64.70
 iteration    64300/  200000 | consumed samples:      2057600 | elapsed time per iteration (ms): 261.1 | learning rate: 4.169E-05 | global batch size:    32 | lm loss: 2.138970E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.26 | backward-params-all-reduce: 63.66 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.20 | optimizer-unscale-and-check-inf: 18.38 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.86 | batch-generator: 8.58
 iteration    64400/  200000 | consumed samples:      2060800 | elapsed time per iteration (ms): 263.0 | learning rate: 4.160E-05 | global batch size:    32 | lm loss: 2.131275E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.50 | backward-params-all-reduce: 63.83 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.37 | optimizer-unscale-and-check-inf: 18.79 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 100.43 | batch-generator: 8.59
 iteration    64500/  200000 | consumed samples:      2064000 | elapsed time per iteration (ms): 261.5 | learning rate: 4.151E-05 | global batch size:    32 | lm loss: 2.125519E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.87 | backward-params-all-reduce: 64.01 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.92 | optimizer-unscale-and-check-inf: 19.42 | optimizer-clip-main-grad: 7.74 | optimizer-copy-main-to-model-params: 12.54 | optimizer: 101.79 | batch-generator: 8.71
-------------------------------------------------------------------------------------------------
 validation loss at iteration 64500 | lm loss value: 3.419181E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 64500, match long value: -0.010384261314706425 | match short value: -0.0795926079365682 
-------------------------------------------------------------------------------------------------------
 iteration    64600/  200000 | consumed samples:      2067200 | elapsed time per iteration (ms): 270.0 | learning rate: 4.142E-05 | global batch size:    32 | lm loss: 2.172010E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.32 | backward-params-all-reduce: 65.10 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.57 | optimizer-unscale-and-check-inf: 17.96 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.54 | optimizer: 99.78 | batch-generator: 14.35
 iteration    64700/  200000 | consumed samples:      2070400 | elapsed time per iteration (ms): 259.3 | learning rate: 4.133E-05 | global batch size:    32 | lm loss: 2.215667E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.15 | backward-params-all-reduce: 63.52 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.47 | optimizer-unscale-and-check-inf: 18.90 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 12.54 | optimizer: 100.79 | batch-generator: 8.54
 iteration    64800/  200000 | consumed samples:      2073600 | elapsed time per iteration (ms): 266.8 | learning rate: 4.123E-05 | global batch size:    32 | lm loss: 2.149902E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.47 | backward-params-all-reduce: 63.01 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.34 | optimizer-unscale-and-check-inf: 18.76 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 14.28 | optimizer: 102.20 | batch-generator: 8.61
 iteration    64900/  200000 | consumed samples:      2076800 | elapsed time per iteration (ms): 258.9 | learning rate: 4.114E-05 | global batch size:    32 | lm loss: 2.179006E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.96 | backward-params-all-reduce: 63.53 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.17 | optimizer-unscale-and-check-inf: 18.90 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 100.31 | batch-generator: 8.47
 iteration    65000/  200000 | consumed samples:      2080000 | elapsed time per iteration (ms): 259.8 | learning rate: 4.105E-05 | global batch size:    32 | lm loss: 2.188279E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.14 | backward-params-all-reduce: 62.92 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.43 | optimizer-unscale-and-check-inf: 19.19 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 100.84 | batch-generator: 8.58
-------------------------------------------------------------------------------------------------
 validation loss at iteration 65000 | lm loss value: 3.142094E-05 | lm loss PPL: 1.000031E+00 | 
 at iteration 65000, match long value: 0.0077709730568934375 | match short value: -0.032360593408969116 
---------------------------------------------------------------------------------------------------------
 iteration    65100/  200000 | consumed samples:      2083200 | elapsed time per iteration (ms): 270.0 | learning rate: 4.096E-05 | global batch size:    32 | lm loss: 2.209616E-05 | loss scale: 268435456.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.14 | backward-params-all-reduce: 63.39 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.21 | optimizer-unscale-and-check-inf: 19.23 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 100.63 | batch-generator: 14.55
 iteration    65200/  200000 | consumed samples:      2086400 | elapsed time per iteration (ms): 258.7 | learning rate: 4.087E-05 | global batch size:    32 | lm loss: 2.238227E-05 | loss scale: 268435456.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.14 | backward-params-all-reduce: 63.15 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.20 | optimizer-unscale-and-check-inf: 19.25 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.38 | optimizer: 100.12 | batch-generator: 8.76
 iteration    65300/  200000 | consumed samples:      2089600 | elapsed time per iteration (ms): 258.5 | learning rate: 4.078E-05 | global batch size:    32 | lm loss: 2.224753E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.11 | backward-params-all-reduce: 64.66 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.01 | optimizer-unscale-and-check-inf: 17.86 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.37 | optimizer: 99.46 | batch-generator: 8.59
 iteration    65400/  200000 | consumed samples:      2092800 | elapsed time per iteration (ms): 257.4 | learning rate: 4.069E-05 | global batch size:    32 | lm loss: 2.175291E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.15 | backward-params-all-reduce: 62.49 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.06 | optimizer-unscale-and-check-inf: 17.56 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.89 | batch-generator: 8.68
 iteration    65500/  200000 | consumed samples:      2096000 | elapsed time per iteration (ms): 266.1 | learning rate: 4.060E-05 | global batch size:    32 | lm loss: 2.172250E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.82 | backward-params-all-reduce: 63.02 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.55 | optimizer-unscale-and-check-inf: 17.51 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 14.26 | optimizer: 101.11 | batch-generator: 8.62
-------------------------------------------------------------------------------------------------
 validation loss at iteration 65500 | lm loss value: 3.101272E-05 | lm loss PPL: 1.000031E+00 | 
 at iteration 65500, match long value: 0.007373052256737187 | match short value: -0.0361693759448513 
------------------------------------------------------------------------------------------------------
 iteration    65600/  200000 | consumed samples:      2099200 | elapsed time per iteration (ms): 274.3 | learning rate: 4.051E-05 | global batch size:    32 | lm loss: 2.172253E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.18 | backward-params-all-reduce: 62.72 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.67 | optimizer-unscale-and-check-inf: 17.56 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.57 | optimizer: 99.58 | batch-generator: 19.69
 iteration    65700/  200000 | consumed samples:      2102400 | elapsed time per iteration (ms): 266.2 | learning rate: 4.042E-05 | global batch size:    32 | lm loss: 1.998038E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.17 | backward-params-all-reduce: 64.68 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.05 | optimizer-unscale-and-check-inf: 17.21 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.68 | batch-generator: 13.99
 iteration    65800/  200000 | consumed samples:      2105600 | elapsed time per iteration (ms): 262.5 | learning rate: 4.033E-05 | global batch size:    32 | lm loss: 1.945800E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.72 | backward-params-all-reduce: 64.22 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.45 | optimizer-unscale-and-check-inf: 17.96 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.69 | batch-generator: 8.65
 iteration    65900/  200000 | consumed samples:      2108800 | elapsed time per iteration (ms): 260.0 | learning rate: 4.023E-05 | global batch size:    32 | lm loss: 1.989139E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.29 | backward-params-all-reduce: 64.91 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.82 | optimizer-unscale-and-check-inf: 18.44 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 100.47 | batch-generator: 8.60
 iteration    66000/  200000 | consumed samples:      2112000 | elapsed time per iteration (ms): 262.9 | learning rate: 4.014E-05 | global batch size:    32 | lm loss: 1.971383E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.33 | backward-params-all-reduce: 65.93 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.96 | optimizer-unscale-and-check-inf: 17.95 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 100.23 | batch-generator: 8.64
-------------------------------------------------------------------------------------------------
 validation loss at iteration 66000 | lm loss value: 3.270182E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 66000, match long value: 0.0021846459193774217 | match short value: 0.0004899162737464535 
---------------------------------------------------------------------------------------------------------
 iteration    66100/  200000 | consumed samples:      2115200 | elapsed time per iteration (ms): 273.6 | learning rate: 4.005E-05 | global batch size:    32 | lm loss: 1.929818E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.35 | backward-params-all-reduce: 65.54 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.98 | optimizer-unscale-and-check-inf: 18.21 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 100.51 | batch-generator: 14.81
 iteration    66200/  200000 | consumed samples:      2118400 | elapsed time per iteration (ms): 271.5 | learning rate: 3.996E-05 | global batch size:    32 | lm loss: 1.960383E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.01 | backward-params-all-reduce: 64.82 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.54 | optimizer-unscale-and-check-inf: 16.18 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 14.73 | optimizer: 101.29 | batch-generator: 8.71
 iteration    66300/  200000 | consumed samples:      2121600 | elapsed time per iteration (ms): 259.9 | learning rate: 3.987E-05 | global batch size:    32 | lm loss: 1.955840E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.49 | backward-params-all-reduce: 64.51 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.26 | optimizer-unscale-and-check-inf: 16.86 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.38 | batch-generator: 8.58
 iteration    66400/  200000 | consumed samples:      2124800 | elapsed time per iteration (ms): 260.8 | learning rate: 3.978E-05 | global batch size:    32 | lm loss: 1.932245E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.34 | backward-params-all-reduce: 64.19 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.94 | optimizer-unscale-and-check-inf: 18.48 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 100.66 | batch-generator: 8.49
 iteration    66500/  200000 | consumed samples:      2128000 | elapsed time per iteration (ms): 259.8 | learning rate: 3.969E-05 | global batch size:    32 | lm loss: 1.991286E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.54 | backward-params-all-reduce: 64.07 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.25 | optimizer-unscale-and-check-inf: 17.29 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.83 | batch-generator: 8.65
-------------------------------------------------------------------------------------------------
 validation loss at iteration 66500 | lm loss value: 3.350411E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 66500, match long value: 0.0023931440965551673 | match short value: -0.001986893565451881 
---------------------------------------------------------------------------------------------------------
 iteration    66600/  200000 | consumed samples:      2131200 | elapsed time per iteration (ms): 273.6 | learning rate: 3.960E-05 | global batch size:    32 | lm loss: 1.965008E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.37 | backward-params-all-reduce: 64.95 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.99 | optimizer-unscale-and-check-inf: 17.66 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.96 | batch-generator: 15.98
 iteration    66700/  200000 | consumed samples:      2134400 | elapsed time per iteration (ms): 259.4 | learning rate: 3.951E-05 | global batch size:    32 | lm loss: 2.031946E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.52 | backward-params-all-reduce: 63.77 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.38 | optimizer-unscale-and-check-inf: 17.46 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.56 | optimizer: 100.20 | batch-generator: 8.63
 iteration    66800/  200000 | consumed samples:      2137600 | elapsed time per iteration (ms): 259.2 | learning rate: 3.941E-05 | global batch size:    32 | lm loss: 2.012395E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.28 | backward-params-all-reduce: 63.18 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.66 | optimizer-unscale-and-check-inf: 18.75 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 100.74 | batch-generator: 8.56
 iteration    66900/  200000 | consumed samples:      2140800 | elapsed time per iteration (ms): 265.6 | learning rate: 3.932E-05 | global batch size:    32 | lm loss: 2.067143E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.73 | backward-params-all-reduce: 63.52 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.75 | optimizer-unscale-and-check-inf: 17.70 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 14.32 | optimizer: 101.53 | batch-generator: 8.55
 iteration    67000/  200000 | consumed samples:      2144000 | elapsed time per iteration (ms): 263.8 | learning rate: 3.923E-05 | global batch size:    32 | lm loss: 2.003870E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.92 | backward-params-all-reduce: 63.59 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.80 | optimizer-unscale-and-check-inf: 16.23 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 99.42 | batch-generator: 8.69
-------------------------------------------------------------------------------------------------
 validation loss at iteration 67000 | lm loss value: 3.349208E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 67000, match long value: -0.004994539905212982 | match short value: -0.08302927607132377 
--------------------------------------------------------------------------------------------------------
 iteration    67100/  200000 | consumed samples:      2147200 | elapsed time per iteration (ms): 274.4 | learning rate: 3.914E-05 | global batch size:    32 | lm loss: 1.995670E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.29 | backward-params-all-reduce: 63.93 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.76 | optimizer-unscale-and-check-inf: 16.37 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.38 | batch-generator: 20.85
 iteration    67200/  200000 | consumed samples:      2150400 | elapsed time per iteration (ms): 261.1 | learning rate: 3.905E-05 | global batch size:    32 | lm loss: 1.953064E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.37 | backward-params-all-reduce: 66.24 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.23 | optimizer-unscale-and-check-inf: 17.16 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.66 | batch-generator: 8.53
 iteration    67300/  200000 | consumed samples:      2153600 | elapsed time per iteration (ms): 259.0 | learning rate: 3.896E-05 | global batch size:    32 | lm loss: 1.972827E-05 | loss scale: 536870912.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.21 | backward-params-all-reduce: 63.78 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.27 | optimizer-unscale-and-check-inf: 17.44 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.99 | batch-generator: 8.54
 iteration    67400/  200000 | consumed samples:      2156800 | elapsed time per iteration (ms): 257.9 | learning rate: 3.887E-05 | global batch size:    32 | lm loss: 1.999813E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.47 | backward-params-all-reduce: 64.44 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.33 | optimizer-unscale-and-check-inf: 17.35 | optimizer-clip-main-grad: 7.44 | optimizer-copy-main-to-model-params: 12.24 | optimizer: 98.53 | batch-generator: 8.49
 iteration    67500/  200000 | consumed samples:      2160000 | elapsed time per iteration (ms): 260.3 | learning rate: 3.878E-05 | global batch size:    32 | lm loss: 2.029142E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.37 | backward-params-all-reduce: 64.62 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.39 | optimizer-unscale-and-check-inf: 17.40 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 100.08 | batch-generator: 8.66
-------------------------------------------------------------------------------------------------
 validation loss at iteration 67500 | lm loss value: 3.114841E-05 | lm loss PPL: 1.000031E+00 | 
 at iteration 67500, match long value: -0.007396704335310297 | match short value: -0.07024376226948172 
--------------------------------------------------------------------------------------------------------
 iteration    67600/  200000 | consumed samples:      2163200 | elapsed time per iteration (ms): 271.0 | learning rate: 3.869E-05 | global batch size:    32 | lm loss: 2.069764E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.29 | backward-params-all-reduce: 66.32 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.20 | optimizer-unscale-and-check-inf: 16.90 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.32 | batch-generator: 14.73
 iteration    67700/  200000 | consumed samples:      2166400 | elapsed time per iteration (ms): 271.7 | learning rate: 3.860E-05 | global batch size:    32 | lm loss: 2.002415E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.87 | backward-params-all-reduce: 63.97 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.59 | optimizer-unscale-and-check-inf: 18.08 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 14.27 | optimizer: 101.65 | batch-generator: 8.66
 iteration    67800/  200000 | consumed samples:      2169600 | elapsed time per iteration (ms): 259.3 | learning rate: 3.851E-05 | global batch size:    32 | lm loss: 2.046265E-05 | loss scale: 268435456.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.28 | backward-params-all-reduce: 63.35 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.57 | optimizer-unscale-and-check-inf: 17.43 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 100.22 | batch-generator: 8.49
 iteration    67900/  200000 | consumed samples:      2172800 | elapsed time per iteration (ms): 259.5 | learning rate: 3.842E-05 | global batch size:    32 | lm loss: 2.237202E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.62 | backward-params-all-reduce: 64.02 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.31 | optimizer-unscale-and-check-inf: 17.37 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.38 | optimizer: 99.35 | batch-generator: 8.53
 iteration    68000/  200000 | consumed samples:      2176000 | elapsed time per iteration (ms): 260.1 | learning rate: 3.832E-05 | global batch size:    32 | lm loss: 2.020336E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.28 | backward-params-all-reduce: 64.55 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.26 | optimizer-unscale-and-check-inf: 17.29 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.92 | batch-generator: 8.56
-------------------------------------------------------------------------------------------------
 validation loss at iteration 68000 | lm loss value: 3.368784E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 68000, match long value: -0.0027303261724954675 | match short value: -0.03497725352454746 
---------------------------------------------------------------------------------------------------------
 iteration    68100/  200000 | consumed samples:      2179200 | elapsed time per iteration (ms): 271.0 | learning rate: 3.823E-05 | global batch size:    32 | lm loss: 1.993208E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.32 | backward-params-all-reduce: 64.47 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.95 | optimizer-unscale-and-check-inf: 17.25 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.41 | batch-generator: 15.06
 iteration    68200/  200000 | consumed samples:      2182400 | elapsed time per iteration (ms): 262.9 | learning rate: 3.814E-05 | global batch size:    32 | lm loss: 2.007221E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.95 | backward-params-all-reduce: 65.86 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.25 | optimizer-unscale-and-check-inf: 16.90 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.35 | batch-generator: 8.54
 iteration    68300/  200000 | consumed samples:      2185600 | elapsed time per iteration (ms): 258.6 | learning rate: 3.805E-05 | global batch size:    32 | lm loss: 1.994406E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.60 | backward-params-all-reduce: 64.30 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.56 | optimizer-unscale-and-check-inf: 17.04 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.78 | batch-generator: 8.46
 iteration    68400/  200000 | consumed samples:      2188800 | elapsed time per iteration (ms): 270.7 | learning rate: 3.796E-05 | global batch size:    32 | lm loss: 2.003538E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 54.45 | backward-params-all-reduce: 63.39 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.39 | optimizer-unscale-and-check-inf: 16.77 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 14.24 | optimizer: 101.17 | batch-generator: 8.52
 iteration    68500/  200000 | consumed samples:      2192000 | elapsed time per iteration (ms): 260.3 | learning rate: 3.787E-05 | global batch size:    32 | lm loss: 2.003283E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.34 | backward-params-all-reduce: 63.78 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.06 | optimizer-unscale-and-check-inf: 17.14 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.44 | batch-generator: 8.60
-------------------------------------------------------------------------------------------------
 validation loss at iteration 68500 | lm loss value: 3.121607E-05 | lm loss PPL: 1.000031E+00 | 
 at iteration 68500, match long value: -0.0006686590216099969 | match short value: -0.04251931398085886 
---------------------------------------------------------------------------------------------------------
 iteration    68600/  200000 | consumed samples:      2195200 | elapsed time per iteration (ms): 274.4 | learning rate: 3.778E-05 | global batch size:    32 | lm loss: 2.012900E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.51 | backward-params-all-reduce: 63.77 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.06 | optimizer-unscale-and-check-inf: 16.83 | optimizer-clip-main-grad: 7.70 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 99.28 | batch-generator: 18.88
 iteration    68700/  200000 | consumed samples:      2198400 | elapsed time per iteration (ms): 258.6 | learning rate: 3.769E-05 | global batch size:    32 | lm loss: 2.016122E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.97 | backward-params-all-reduce: 63.80 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.24 | optimizer-unscale-and-check-inf: 16.70 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 99.24 | batch-generator: 8.68
 iteration    68800/  200000 | consumed samples:      2201600 | elapsed time per iteration (ms): 265.8 | learning rate: 3.760E-05 | global batch size:    32 | lm loss: 1.899425E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.00 | backward-params-all-reduce: 64.62 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.89 | optimizer-unscale-and-check-inf: 17.63 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.45 | optimizer: 99.71 | batch-generator: 14.61
 iteration    68900/  200000 | consumed samples:      2204800 | elapsed time per iteration (ms): 260.6 | learning rate: 3.750E-05 | global batch size:    32 | lm loss: 1.821964E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.25 | backward-params-all-reduce: 64.46 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.84 | optimizer-unscale-and-check-inf: 16.91 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.08 | batch-generator: 8.82
 iteration    69000/  200000 | consumed samples:      2208000 | elapsed time per iteration (ms): 261.6 | learning rate: 3.741E-05 | global batch size:    32 | lm loss: 1.820068E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.28 | backward-params-all-reduce: 64.25 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.50 | optimizer-unscale-and-check-inf: 18.04 | optimizer-clip-main-grad: 7.70 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.87 | batch-generator: 8.58
-------------------------------------------------------------------------------------------------
 validation loss at iteration 69000 | lm loss value: 3.319379E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 69000, match long value: 0.003184191845223783 | match short value: -0.1007743912337697 
------------------------------------------------------------------------------------------------------
 iteration    69100/  200000 | consumed samples:      2211200 | elapsed time per iteration (ms): 283.8 | learning rate: 3.732E-05 | global batch size:    32 | lm loss: 1.837271E-05 | loss scale: 268435456.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.02 | backward-params-all-reduce: 65.28 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.61 | optimizer-unscale-and-check-inf: 16.09 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 14.30 | optimizer: 99.71 | batch-generator: 14.35
 iteration    69200/  200000 | consumed samples:      2214400 | elapsed time per iteration (ms): 262.6 | learning rate: 3.723E-05 | global batch size:    32 | lm loss: 1.830768E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.97 | backward-params-all-reduce: 65.82 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.15 | optimizer-unscale-and-check-inf: 16.94 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.36 | batch-generator: 8.60
 iteration    69300/  200000 | consumed samples:      2217600 | elapsed time per iteration (ms): 261.2 | learning rate: 3.714E-05 | global batch size:    32 | lm loss: 1.794778E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.61 | backward-params-all-reduce: 65.86 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.73 | optimizer-unscale-and-check-inf: 17.83 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 99.84 | batch-generator: 8.69
 iteration    69400/  200000 | consumed samples:      2220800 | elapsed time per iteration (ms): 260.8 | learning rate: 3.705E-05 | global batch size:    32 | lm loss: 1.789028E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.28 | backward-params-all-reduce: 64.09 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.79 | optimizer-unscale-and-check-inf: 16.03 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.56 | optimizer: 98.08 | batch-generator: 8.74
 iteration    69500/  200000 | consumed samples:      2224000 | elapsed time per iteration (ms): 260.9 | learning rate: 3.696E-05 | global batch size:    32 | lm loss: 1.789166E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.40 | backward-params-all-reduce: 63.75 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.91 | optimizer-unscale-and-check-inf: 16.91 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.12 | batch-generator: 8.64
-------------------------------------------------------------------------------------------------
 validation loss at iteration 69500 | lm loss value: 3.349653E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 69500, match long value: -0.00045074748494900704 | match short value: -0.033870821775172956 
-----------------------------------------------------------------------------------------------------------
 iteration    69600/  200000 | consumed samples:      2227200 | elapsed time per iteration (ms): 274.4 | learning rate: 3.687E-05 | global batch size:    32 | lm loss: 1.811014E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.62 | backward-params-all-reduce: 64.48 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.58 | optimizer-unscale-and-check-inf: 15.71 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.54 | optimizer: 97.53 | batch-generator: 15.53
 iteration    69700/  200000 | consumed samples:      2230400 | elapsed time per iteration (ms): 259.2 | learning rate: 3.678E-05 | global batch size:    32 | lm loss: 1.809375E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.27 | backward-params-all-reduce: 63.66 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.74 | optimizer-unscale-and-check-inf: 16.98 | optimizer-clip-main-grad: 7.49 | optimizer-copy-main-to-model-params: 12.27 | optimizer: 97.65 | batch-generator: 8.64
 iteration    69800/  200000 | consumed samples:      2233600 | elapsed time per iteration (ms): 259.7 | learning rate: 3.669E-05 | global batch size:    32 | lm loss: 1.849156E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.28 | backward-params-all-reduce: 63.42 | backward-embedding-all-reduce: 0.05 | optimizer-copy-to-main-grad: 12.77 | optimizer-unscale-and-check-inf: 15.42 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 97.36 | batch-generator: 8.54
 iteration    69900/  200000 | consumed samples:      2236800 | elapsed time per iteration (ms): 274.8 | learning rate: 3.660E-05 | global batch size:    32 | lm loss: 1.852069E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 54.13 | backward-params-all-reduce: 63.80 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.90 | optimizer-unscale-and-check-inf: 15.98 | optimizer-clip-main-grad: 7.54 | optimizer-copy-main-to-model-params: 14.31 | optimizer: 99.86 | batch-generator: 8.57
 iteration    70000/  200000 | consumed samples:      2240000 | elapsed time per iteration (ms): 260.1 | learning rate: 3.650E-05 | global batch size:    32 | lm loss: 2.031122E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.71 | backward-params-all-reduce: 64.16 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.63 | optimizer-unscale-and-check-inf: 16.78 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.66 | batch-generator: 8.67
-------------------------------------------------------------------------------------------------
 validation loss at iteration 70000 | lm loss value: 3.313562E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 70000, match long value: -0.0032712171743773575 | match short value: -0.05192688956081375 
---------------------------------------------------------------------------------------------------------
 iteration    70100/  200000 | consumed samples:      2243200 | elapsed time per iteration (ms): 276.4 | learning rate: 3.641E-05 | global batch size:    32 | lm loss: 1.841026E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.39 | backward-params-all-reduce: 63.58 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.78 | optimizer-unscale-and-check-inf: 16.74 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.82 | batch-generator: 19.73
 iteration    70200/  200000 | consumed samples:      2246400 | elapsed time per iteration (ms): 259.1 | learning rate: 3.632E-05 | global batch size:    32 | lm loss: 1.818016E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.25 | backward-params-all-reduce: 63.47 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.74 | optimizer-unscale-and-check-inf: 14.92 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 96.90 | batch-generator: 8.70
 iteration    70300/  200000 | consumed samples:      2249600 | elapsed time per iteration (ms): 258.1 | learning rate: 3.623E-05 | global batch size:    32 | lm loss: 1.865057E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.23 | backward-params-all-reduce: 63.54 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.27 | optimizer-unscale-and-check-inf: 15.50 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 96.98 | batch-generator: 8.55
 iteration    70400/  200000 | consumed samples:      2252800 | elapsed time per iteration (ms): 259.9 | learning rate: 3.614E-05 | global batch size:    32 | lm loss: 1.829233E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.27 | backward-params-all-reduce: 62.87 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.82 | optimizer-unscale-and-check-inf: 16.40 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.48 | batch-generator: 8.53
 iteration    70500/  200000 | consumed samples:      2256000 | elapsed time per iteration (ms): 259.5 | learning rate: 3.605E-05 | global batch size:    32 | lm loss: 1.833366E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.18 | backward-params-all-reduce: 63.74 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.27 | optimizer-unscale-and-check-inf: 16.48 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.01 | batch-generator: 8.59
-------------------------------------------------------------------------------------------------
 validation loss at iteration 70500 | lm loss value: 3.236312E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 70500, match long value: -0.001696430472977871 | match short value: 0.0019168455438570212 
---------------------------------------------------------------------------------------------------------
 iteration    70600/  200000 | consumed samples:      2259200 | elapsed time per iteration (ms): 287.2 | learning rate: 3.596E-05 | global batch size:    32 | lm loss: 1.872492E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.21 | backward-params-all-reduce: 64.09 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.27 | optimizer-unscale-and-check-inf: 15.41 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 14.24 | optimizer: 98.67 | batch-generator: 17.50
 iteration    70700/  200000 | consumed samples:      2262400 | elapsed time per iteration (ms): 259.9 | learning rate: 3.587E-05 | global batch size:    32 | lm loss: 1.878351E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.48 | backward-params-all-reduce: 62.92 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.45 | optimizer-unscale-and-check-inf: 16.57 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 98.16 | batch-generator: 8.61
 iteration    70800/  200000 | consumed samples:      2265600 | elapsed time per iteration (ms): 261.3 | learning rate: 3.578E-05 | global batch size:    32 | lm loss: 1.838334E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.74 | backward-params-all-reduce: 62.78 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.86 | optimizer-unscale-and-check-inf: 17.32 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.54 | batch-generator: 8.54
 iteration    70900/  200000 | consumed samples:      2268800 | elapsed time per iteration (ms): 262.4 | learning rate: 3.568E-05 | global batch size:    32 | lm loss: 1.840315E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.51 | backward-params-all-reduce: 64.35 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.10 | optimizer-unscale-and-check-inf: 16.62 | optimizer-clip-main-grad: 7.78 | optimizer-copy-main-to-model-params: 12.55 | optimizer: 99.25 | batch-generator: 8.65
 iteration    71000/  200000 | consumed samples:      2272000 | elapsed time per iteration (ms): 259.9 | learning rate: 3.559E-05 | global batch size:    32 | lm loss: 1.846827E-05 | loss scale: 268435456.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.90 | backward-params-all-reduce: 64.78 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.13 | optimizer-unscale-and-check-inf: 16.57 | optimizer-clip-main-grad: 7.53 | optimizer-copy-main-to-model-params: 12.36 | optimizer: 98.25 | batch-generator: 8.59
-------------------------------------------------------------------------------------------------
 validation loss at iteration 71000 | lm loss value: 3.367838E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 71000, match long value: 0.0011062324052208336 | match short value: -0.03941508919620693 
--------------------------------------------------------------------------------------------------------
 iteration    71100/  200000 | consumed samples:      2275200 | elapsed time per iteration (ms): 274.9 | learning rate: 3.550E-05 | global batch size:    32 | lm loss: 1.883873E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.98 | backward-params-all-reduce: 65.77 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.39 | optimizer-unscale-and-check-inf: 16.11 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.39 | optimizer: 97.13 | batch-generator: 15.99
 iteration    71200/  200000 | consumed samples:      2278400 | elapsed time per iteration (ms): 259.8 | learning rate: 3.541E-05 | global batch size:    32 | lm loss: 1.858866E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.04 | backward-params-all-reduce: 63.84 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.42 | optimizer-unscale-and-check-inf: 17.21 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 98.95 | batch-generator: 8.51
 iteration    71300/  200000 | consumed samples:      2281600 | elapsed time per iteration (ms): 271.5 | learning rate: 3.532E-05 | global batch size:    32 | lm loss: 1.887027E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.98 | backward-params-all-reduce: 63.21 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.18 | optimizer-unscale-and-check-inf: 16.83 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 14.21 | optimizer: 99.97 | batch-generator: 8.59
 iteration    71400/  200000 | consumed samples:      2284800 | elapsed time per iteration (ms): 265.4 | learning rate: 3.523E-05 | global batch size:    32 | lm loss: 1.921660E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.61 | backward-params-all-reduce: 64.10 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.53 | optimizer-unscale-and-check-inf: 17.03 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.86 | batch-generator: 8.63
 iteration    71500/  200000 | consumed samples:      2288000 | elapsed time per iteration (ms): 258.8 | learning rate: 3.514E-05 | global batch size:    32 | lm loss: 1.848199E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.49 | backward-params-all-reduce: 62.77 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.27 | optimizer-unscale-and-check-inf: 16.90 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.35 | batch-generator: 8.62
-------------------------------------------------------------------------------------------------
 validation loss at iteration 71500 | lm loss value: 3.319854E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 71500, match long value: 0.005339667610735468 | match short value: -0.0229899400044587 
------------------------------------------------------------------------------------------------------
 iteration    71600/  200000 | consumed samples:      2291200 | elapsed time per iteration (ms): 276.7 | learning rate: 3.505E-05 | global batch size:    32 | lm loss: 1.921345E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.42 | backward-params-all-reduce: 63.71 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.49 | optimizer-unscale-and-check-inf: 16.80 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.57 | batch-generator: 19.75
 iteration    71700/  200000 | consumed samples:      2294400 | elapsed time per iteration (ms): 258.1 | learning rate: 3.496E-05 | global batch size:    32 | lm loss: 1.930100E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.83 | backward-params-all-reduce: 63.57 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.44 | optimizer-unscale-and-check-inf: 16.88 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 98.49 | batch-generator: 8.46
 iteration    71800/  200000 | consumed samples:      2297600 | elapsed time per iteration (ms): 261.1 | learning rate: 3.487E-05 | global batch size:    32 | lm loss: 1.843644E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.37 | backward-params-all-reduce: 63.67 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.60 | optimizer-unscale-and-check-inf: 16.93 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.80 | batch-generator: 8.57
 iteration    71900/  200000 | consumed samples:      2300800 | elapsed time per iteration (ms): 267.6 | learning rate: 3.478E-05 | global batch size:    32 | lm loss: 1.842766E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.48 | backward-params-all-reduce: 64.09 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.46 | optimizer-unscale-and-check-inf: 18.38 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 100.16 | batch-generator: 14.22
 iteration    72000/  200000 | consumed samples:      2304000 | elapsed time per iteration (ms): 262.5 | learning rate: 3.468E-05 | global batch size:    32 | lm loss: 1.727141E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.46 | backward-params-all-reduce: 63.09 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.71 | optimizer-unscale-and-check-inf: 15.81 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 97.72 | batch-generator: 8.58
-------------------------------------------------------------------------------------------------
 validation loss at iteration 72000 | lm loss value: 3.198369E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 72000, match long value: 0.0008107480266848492 | match short value: -0.040230008840841484 
---------------------------------------------------------------------------------------------------------
 iteration    72100/  200000 | consumed samples:      2307200 | elapsed time per iteration (ms): 288.1 | learning rate: 3.459E-05 | global batch size:    32 | lm loss: 1.709724E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.66 | backward-params-all-reduce: 65.14 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.79 | optimizer-unscale-and-check-inf: 15.54 | optimizer-clip-main-grad: 7.72 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.97 | batch-generator: 16.43
 iteration    72200/  200000 | consumed samples:      2310400 | elapsed time per iteration (ms): 260.1 | learning rate: 3.450E-05 | global batch size:    32 | lm loss: 1.666485E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.41 | backward-params-all-reduce: 63.60 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.62 | optimizer-unscale-and-check-inf: 16.35 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.16 | batch-generator: 8.61
 iteration    72300/  200000 | consumed samples:      2313600 | elapsed time per iteration (ms): 261.5 | learning rate: 3.441E-05 | global batch size:    32 | lm loss: 1.677853E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.43 | backward-params-all-reduce: 63.72 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.05 | optimizer-unscale-and-check-inf: 17.96 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.23 | batch-generator: 8.52
 iteration    72400/  200000 | consumed samples:      2316800 | elapsed time per iteration (ms): 263.3 | learning rate: 3.432E-05 | global batch size:    32 | lm loss: 1.655460E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.27 | backward-params-all-reduce: 65.54 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 11.92 | optimizer-unscale-and-check-inf: 20.40 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 101.56 | batch-generator: 8.48
 iteration    72500/  200000 | consumed samples:      2320000 | elapsed time per iteration (ms): 261.7 | learning rate: 3.423E-05 | global batch size:    32 | lm loss: 1.696475E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.22 | backward-params-all-reduce: 63.69 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.01 | optimizer-unscale-and-check-inf: 20.23 | optimizer-clip-main-grad: 7.70 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 101.58 | batch-generator: 8.61
-------------------------------------------------------------------------------------------------
 validation loss at iteration 72500 | lm loss value: 3.346615E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 72500, match long value: 0.0015201574824251043 | match short value: -0.0440131349754046 
-------------------------------------------------------------------------------------------------------
 iteration    72600/  200000 | consumed samples:      2323200 | elapsed time per iteration (ms): 274.0 | learning rate: 3.414E-05 | global batch size:    32 | lm loss: 1.731229E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.35 | backward-params-all-reduce: 63.75 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.44 | optimizer-unscale-and-check-inf: 20.15 | optimizer-clip-main-grad: 7.78 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 102.04 | batch-generator: 15.61
 iteration    72700/  200000 | consumed samples:      2326400 | elapsed time per iteration (ms): 325.6 | learning rate: 3.405E-05 | global batch size:    32 | lm loss: 1.739082E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.57 | backward-params-all-reduce: 63.97 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.38 | optimizer-unscale-and-check-inf: 19.26 | optimizer-clip-main-grad: 7.71 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 101.01 | batch-generator: 68.96
 iteration    72800/  200000 | consumed samples:      2329600 | elapsed time per iteration (ms): 276.4 | learning rate: 3.396E-05 | global batch size:    32 | lm loss: 1.693563E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.97 | backward-params-all-reduce: 64.29 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 11.94 | optimizer-unscale-and-check-inf: 19.21 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 14.26 | optimizer: 102.23 | batch-generator: 8.63
 iteration    72900/  200000 | consumed samples:      2332800 | elapsed time per iteration (ms): 263.4 | learning rate: 3.386E-05 | global batch size:    32 | lm loss: 1.705076E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.12 | backward-params-all-reduce: 65.53 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.61 | optimizer-unscale-and-check-inf: 19.60 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.59 | optimizer: 101.54 | batch-generator: 8.49
 iteration    73000/  200000 | consumed samples:      2336000 | elapsed time per iteration (ms): 261.2 | learning rate: 3.377E-05 | global batch size:    32 | lm loss: 1.707205E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.22 | backward-params-all-reduce: 64.09 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.48 | optimizer-unscale-and-check-inf: 20.28 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 102.13 | batch-generator: 8.63
-------------------------------------------------------------------------------------------------
 validation loss at iteration 73000 | lm loss value: 3.232660E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 73000, match long value: 0.003725708384514324 | match short value: -0.10039247780018576 
-------------------------------------------------------------------------------------------------------
 iteration    73100/  200000 | consumed samples:      2339200 | elapsed time per iteration (ms): 274.8 | learning rate: 3.368E-05 | global batch size:    32 | lm loss: 1.738409E-05 | loss scale: 536870912.0 | grad norm: 0.002 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.35 | backward-params-all-reduce: 63.17 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.36 | optimizer-unscale-and-check-inf: 18.83 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.38 | optimizer: 99.80 | batch-generator: 20.41
 iteration    73200/  200000 | consumed samples:      2342400 | elapsed time per iteration (ms): 257.2 | learning rate: 3.359E-05 | global batch size:    32 | lm loss: 1.781793E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.18 | backward-params-all-reduce: 63.20 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.75 | optimizer-unscale-and-check-inf: 18.13 | optimizer-clip-main-grad: 7.53 | optimizer-copy-main-to-model-params: 12.40 | optimizer: 99.46 | batch-generator: 8.56
 iteration    73300/  200000 | consumed samples:      2345600 | elapsed time per iteration (ms): 258.3 | learning rate: 3.350E-05 | global batch size:    32 | lm loss: 1.722175E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.25 | backward-params-all-reduce: 62.68 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.74 | optimizer-unscale-and-check-inf: 18.76 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 100.73 | batch-generator: 8.58
 iteration    73400/  200000 | consumed samples:      2348800 | elapsed time per iteration (ms): 260.2 | learning rate: 3.341E-05 | global batch size:    32 | lm loss: 1.691673E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.26 | backward-params-all-reduce: 63.77 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.61 | optimizer-unscale-and-check-inf: 17.66 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 99.55 | batch-generator: 8.72
 iteration    73500/  200000 | consumed samples:      2352000 | elapsed time per iteration (ms): 270.4 | learning rate: 3.332E-05 | global batch size:    32 | lm loss: 1.746073E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.98 | backward-params-all-reduce: 63.31 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.23 | optimizer-unscale-and-check-inf: 17.47 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 14.37 | optimizer: 101.90 | batch-generator: 8.57
-------------------------------------------------------------------------------------------------
 validation loss at iteration 73500 | lm loss value: 3.240810E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 73500, match long value: -0.005305517170487382 | match short value: -0.014963200269295249 
---------------------------------------------------------------------------------------------------------
 iteration    73600/  200000 | consumed samples:      2355200 | elapsed time per iteration (ms): 272.6 | learning rate: 3.323E-05 | global batch size:    32 | lm loss: 1.724582E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.29 | backward-params-all-reduce: 64.50 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.91 | optimizer-unscale-and-check-inf: 17.41 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.58 | batch-generator: 16.00
 iteration    73700/  200000 | consumed samples:      2358400 | elapsed time per iteration (ms): 258.5 | learning rate: 3.314E-05 | global batch size:    32 | lm loss: 1.746459E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.37 | backward-params-all-reduce: 63.37 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.97 | optimizer-unscale-and-check-inf: 17.07 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.57 | optimizer: 99.38 | batch-generator: 8.58
 iteration    73800/  200000 | consumed samples:      2361600 | elapsed time per iteration (ms): 257.7 | learning rate: 3.305E-05 | global batch size:    32 | lm loss: 1.736285E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.31 | backward-params-all-reduce: 64.76 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.51 | optimizer-unscale-and-check-inf: 17.10 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.56 | optimizer: 98.90 | batch-generator: 8.61
 iteration    73900/  200000 | consumed samples:      2364800 | elapsed time per iteration (ms): 260.5 | learning rate: 3.296E-05 | global batch size:    32 | lm loss: 1.724613E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.14 | backward-params-all-reduce: 66.48 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.55 | optimizer-unscale-and-check-inf: 18.53 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 100.28 | batch-generator: 8.49
 iteration    74000/  200000 | consumed samples:      2368000 | elapsed time per iteration (ms): 259.2 | learning rate: 3.286E-05 | global batch size:    32 | lm loss: 1.753299E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.54 | backward-params-all-reduce: 63.71 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.04 | optimizer-unscale-and-check-inf: 17.99 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 100.27 | batch-generator: 8.52
-------------------------------------------------------------------------------------------------
 validation loss at iteration 74000 | lm loss value: 3.337666E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 74000, match long value: 0.007335618219305351 | match short value: -0.029337173566619242 
--------------------------------------------------------------------------------------------------------
 iteration    74100/  200000 | consumed samples:      2371200 | elapsed time per iteration (ms): 270.3 | learning rate: 3.277E-05 | global batch size:    32 | lm loss: 1.743801E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.17 | backward-params-all-reduce: 63.26 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.03 | optimizer-unscale-and-check-inf: 17.32 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.67 | batch-generator: 15.97
 iteration    74200/  200000 | consumed samples:      2374400 | elapsed time per iteration (ms): 259.0 | learning rate: 3.268E-05 | global batch size:    32 | lm loss: 1.707047E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.46 | backward-params-all-reduce: 63.09 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.95 | optimizer-unscale-and-check-inf: 16.54 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 98.78 | batch-generator: 8.63
 iteration    74300/  200000 | consumed samples:      2377600 | elapsed time per iteration (ms): 268.2 | learning rate: 3.259E-05 | global batch size:    32 | lm loss: 1.755707E-05 | loss scale: 67108864.0 | grad norm: 0.002 | number of skipped iterations:   4 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.24 | backward-params-all-reduce: 63.92 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.71 | optimizer-unscale-and-check-inf: 17.65 | optimizer-clip-main-grad: 7.42 | optimizer-copy-main-to-model-params: 13.71 | optimizer: 98.67 | batch-generator: 8.52
 iteration    74400/  200000 | consumed samples:      2380800 | elapsed time per iteration (ms): 258.7 | learning rate: 3.250E-05 | global batch size:    32 | lm loss: 1.790091E-05 | loss scale: 67108864.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.95 | backward-params-all-reduce: 64.58 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.97 | optimizer-unscale-and-check-inf: 16.98 | optimizer-clip-main-grad: 7.54 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 99.08 | batch-generator: 8.49
 iteration    74500/  200000 | consumed samples:      2384000 | elapsed time per iteration (ms): 258.5 | learning rate: 3.241E-05 | global batch size:    32 | lm loss: 1.733845E-05 | loss scale: 67108864.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.23 | backward-params-all-reduce: 63.21 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.38 | optimizer-unscale-and-check-inf: 16.49 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.13 | batch-generator: 8.54
-------------------------------------------------------------------------------------------------
 validation loss at iteration 74500 | lm loss value: 3.290508E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 74500, match long value: 0.0032623281218868176 | match short value: -0.002294165031955415 
---------------------------------------------------------------------------------------------------------
 iteration    74600/  200000 | consumed samples:      2387200 | elapsed time per iteration (ms): 276.0 | learning rate: 3.232E-05 | global batch size:    32 | lm loss: 1.754904E-05 | loss scale: 67108864.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.31 | backward-params-all-reduce: 64.41 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.89 | optimizer-unscale-and-check-inf: 17.46 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.55 | optimizer: 99.77 | batch-generator: 20.04
 iteration    74700/  200000 | consumed samples:      2390400 | elapsed time per iteration (ms): 258.4 | learning rate: 3.223E-05 | global batch size:    32 | lm loss: 1.729652E-05 | loss scale: 67108864.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.28 | backward-params-all-reduce: 63.69 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.58 | optimizer-unscale-and-check-inf: 17.21 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.07 | batch-generator: 8.55
 iteration    74800/  200000 | consumed samples:      2393600 | elapsed time per iteration (ms): 259.1 | learning rate: 3.214E-05 | global batch size:    32 | lm loss: 1.802608E-05 | loss scale: 67108864.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.25 | backward-params-all-reduce: 64.72 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.61 | optimizer-unscale-and-check-inf: 17.96 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.82 | batch-generator: 8.43
 iteration    74900/  200000 | consumed samples:      2396800 | elapsed time per iteration (ms): 260.6 | learning rate: 3.205E-05 | global batch size:    32 | lm loss: 1.752080E-05 | loss scale: 67108864.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.86 | backward-params-all-reduce: 65.39 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.96 | optimizer-unscale-and-check-inf: 16.45 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.56 | optimizer: 98.81 | batch-generator: 8.57
 iteration    75000/  200000 | consumed samples:      2400000 | elapsed time per iteration (ms): 273.4 | learning rate: 3.196E-05 | global batch size:    32 | lm loss: 1.753238E-05 | loss scale: 67108864.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.61 | backward-params-all-reduce: 65.38 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.07 | optimizer-unscale-and-check-inf: 15.36 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 14.23 | optimizer: 98.40 | batch-generator: 8.59
-------------------------------------------------------------------------------------------------
 validation loss at iteration 75000 | lm loss value: 3.410710E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 75000, match long value: -0.002401570073312522 | match short value: -0.0784369094788345 
-------------------------------------------------------------------------------------------------------
 iteration    75100/  200000 | consumed samples:      2403200 | elapsed time per iteration (ms): 277.7 | learning rate: 3.187E-05 | global batch size:    32 | lm loss: 1.598863E-05 | loss scale: 67108864.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.30 | backward-params-all-reduce: 64.48 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.05 | optimizer-unscale-and-check-inf: 18.66 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 99.89 | batch-generator: 19.18
 iteration    75200/  200000 | consumed samples:      2406400 | elapsed time per iteration (ms): 261.1 | learning rate: 3.177E-05 | global batch size:    32 | lm loss: 1.581117E-05 | loss scale: 67108864.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.40 | backward-params-all-reduce: 64.71 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.45 | optimizer-unscale-and-check-inf: 16.21 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 97.90 | batch-generator: 8.50
 iteration    75300/  200000 | consumed samples:      2409600 | elapsed time per iteration (ms): 260.2 | learning rate: 3.168E-05 | global batch size:    32 | lm loss: 1.604954E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.40 | backward-params-all-reduce: 64.28 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.91 | optimizer-unscale-and-check-inf: 16.79 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.45 | optimizer: 98.86 | batch-generator: 8.33
 iteration    75400/  200000 | consumed samples:      2412800 | elapsed time per iteration (ms): 262.4 | learning rate: 3.159E-05 | global batch size:    32 | lm loss: 1.612693E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.39 | backward-params-all-reduce: 64.65 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.38 | optimizer-unscale-and-check-inf: 17.31 | optimizer-clip-main-grad: 7.78 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.13 | batch-generator: 8.45
 iteration    75500/  200000 | consumed samples:      2416000 | elapsed time per iteration (ms): 260.5 | learning rate: 3.150E-05 | global batch size:    32 | lm loss: 1.629030E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.36 | backward-params-all-reduce: 63.97 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.55 | optimizer-unscale-and-check-inf: 16.87 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 98.64 | batch-generator: 8.45
-------------------------------------------------------------------------------------------------
 validation loss at iteration 75500 | lm loss value: 3.382528E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 75500, match long value: 0.0028931664858853046 | match short value: -1.1441498468833333e-05 
-----------------------------------------------------------------------------------------------------------
 iteration    75600/  200000 | consumed samples:      2419200 | elapsed time per iteration (ms): 272.6 | learning rate: 3.141E-05 | global batch size:    32 | lm loss: 1.630870E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.24 | backward-params-all-reduce: 63.64 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.40 | optimizer-unscale-and-check-inf: 16.74 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 98.32 | batch-generator: 15.98
 iteration    75700/  200000 | consumed samples:      2422400 | elapsed time per iteration (ms): 272.9 | learning rate: 3.132E-05 | global batch size:    32 | lm loss: 1.598837E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.04 | backward-params-all-reduce: 64.67 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 11.97 | optimizer-unscale-and-check-inf: 16.15 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 14.20 | optimizer: 99.02 | batch-generator: 8.39
 iteration    75800/  200000 | consumed samples:      2425600 | elapsed time per iteration (ms): 261.7 | learning rate: 3.123E-05 | global batch size:    32 | lm loss: 1.607432E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.38 | backward-params-all-reduce: 63.45 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.87 | optimizer-unscale-and-check-inf: 16.86 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 98.87 | batch-generator: 8.51
 iteration    75900/  200000 | consumed samples:      2428800 | elapsed time per iteration (ms): 259.3 | learning rate: 3.114E-05 | global batch size:    32 | lm loss: 1.622850E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.48 | backward-params-all-reduce: 63.70 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.51 | optimizer-unscale-and-check-inf: 16.60 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.44 | optimizer: 98.27 | batch-generator: 8.39
 iteration    76000/  200000 | consumed samples:      2432000 | elapsed time per iteration (ms): 261.7 | learning rate: 3.105E-05 | global batch size:    32 | lm loss: 1.613420E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.18 | backward-params-all-reduce: 65.65 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.69 | optimizer-unscale-and-check-inf: 16.84 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 98.79 | batch-generator: 8.59
-------------------------------------------------------------------------------------------------
 validation loss at iteration 76000 | lm loss value: 3.118932E-05 | lm loss PPL: 1.000031E+00 | 
 at iteration 76000, match long value: 0.0033921995210127688 | match short value: -0.08456396662011406 
--------------------------------------------------------------------------------------------------------
 iteration    76100/  200000 | consumed samples:      2435200 | elapsed time per iteration (ms): 279.0 | learning rate: 3.095E-05 | global batch size:    32 | lm loss: 1.612121E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.14 | backward-params-all-reduce: 64.43 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.71 | optimizer-unscale-and-check-inf: 16.57 | optimizer-clip-main-grad: 7.72 | optimizer-copy-main-to-model-params: 12.55 | optimizer: 98.74 | batch-generator: 20.79
 iteration    76200/  200000 | consumed samples:      2438400 | elapsed time per iteration (ms): 258.7 | learning rate: 3.086E-05 | global batch size:    32 | lm loss: 1.627041E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.75 | backward-params-all-reduce: 63.57 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.51 | optimizer-unscale-and-check-inf: 16.68 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 98.47 | batch-generator: 8.50
 iteration    76300/  200000 | consumed samples:      2441600 | elapsed time per iteration (ms): 261.2 | learning rate: 3.077E-05 | global batch size:    32 | lm loss: 1.608487E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.23 | backward-params-all-reduce: 64.56 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.95 | optimizer-unscale-and-check-inf: 16.88 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.16 | batch-generator: 8.48
 iteration    76400/  200000 | consumed samples:      2444800 | elapsed time per iteration (ms): 265.2 | learning rate: 3.068E-05 | global batch size:    32 | lm loss: 1.658253E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.00 | backward-params-all-reduce: 64.62 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.50 | optimizer-unscale-and-check-inf: 16.70 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 14.32 | optimizer: 100.26 | batch-generator: 8.48
 iteration    76500/  200000 | consumed samples:      2448000 | elapsed time per iteration (ms): 273.7 | learning rate: 3.059E-05 | global batch size:    32 | lm loss: 1.619986E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 54.32 | backward-params-all-reduce: 63.29 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.70 | optimizer-unscale-and-check-inf: 16.61 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.54 | batch-generator: 8.47
-------------------------------------------------------------------------------------------------
 validation loss at iteration 76500 | lm loss value: 3.218983E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 76500, match long value: -0.0004859135454949273 | match short value: -0.039734604273153645 
----------------------------------------------------------------------------------------------------------
 iteration    76600/  200000 | consumed samples:      2451200 | elapsed time per iteration (ms): 272.3 | learning rate: 3.050E-05 | global batch size:    32 | lm loss: 1.629920E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.38 | backward-params-all-reduce: 63.79 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.26 | optimizer-unscale-and-check-inf: 15.97 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 97.49 | batch-generator: 15.54
 iteration    76700/  200000 | consumed samples:      2454400 | elapsed time per iteration (ms): 258.6 | learning rate: 3.041E-05 | global batch size:    32 | lm loss: 1.596725E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.33 | backward-params-all-reduce: 63.44 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.49 | optimizer-unscale-and-check-inf: 15.94 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 97.72 | batch-generator: 8.52
 iteration    76800/  200000 | consumed samples:      2457600 | elapsed time per iteration (ms): 257.5 | learning rate: 3.032E-05 | global batch size:    32 | lm loss: 1.611324E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.21 | backward-params-all-reduce: 63.28 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.47 | optimizer-unscale-and-check-inf: 16.26 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 97.95 | batch-generator: 8.43
 iteration    76900/  200000 | consumed samples:      2460800 | elapsed time per iteration (ms): 260.1 | learning rate: 3.023E-05 | global batch size:    32 | lm loss: 1.652926E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 54.50 | backward-params-all-reduce: 62.26 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.78 | optimizer-unscale-and-check-inf: 14.55 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.54 | optimizer: 96.61 | batch-generator: 8.59
 iteration    77000/  200000 | consumed samples:      2464000 | elapsed time per iteration (ms): 257.4 | learning rate: 3.013E-05 | global batch size:    32 | lm loss: 1.618204E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.72 | backward-params-all-reduce: 62.62 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.03 | optimizer-unscale-and-check-inf: 16.15 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 97.47 | batch-generator: 8.47
-------------------------------------------------------------------------------------------------
 validation loss at iteration 77000 | lm loss value: 3.317849E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 77000, match long value: 0.00041660145324179994 | match short value: -0.00807119468962785 
---------------------------------------------------------------------------------------------------------
 iteration    77100/  200000 | consumed samples:      2467200 | elapsed time per iteration (ms): 270.5 | learning rate: 3.004E-05 | global batch size:    32 | lm loss: 1.641608E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.34 | backward-params-all-reduce: 63.40 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.32 | optimizer-unscale-and-check-inf: 15.92 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 97.53 | batch-generator: 16.19
 iteration    77200/  200000 | consumed samples:      2470400 | elapsed time per iteration (ms): 273.0 | learning rate: 2.995E-05 | global batch size:    32 | lm loss: 1.633260E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.47 | backward-params-all-reduce: 62.63 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.60 | optimizer-unscale-and-check-inf: 15.56 | optimizer-clip-main-grad: 7.52 | optimizer-copy-main-to-model-params: 14.16 | optimizer: 98.99 | batch-generator: 8.52
 iteration    77300/  200000 | consumed samples:      2473600 | elapsed time per iteration (ms): 257.9 | learning rate: 2.986E-05 | global batch size:    32 | lm loss: 1.657921E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.35 | backward-params-all-reduce: 63.78 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 11.96 | optimizer-unscale-and-check-inf: 17.63 | optimizer-clip-main-grad: 7.42 | optimizer-copy-main-to-model-params: 12.23 | optimizer: 97.40 | batch-generator: 8.48
 iteration    77400/  200000 | consumed samples:      2476800 | elapsed time per iteration (ms): 260.2 | learning rate: 2.977E-05 | global batch size:    32 | lm loss: 1.655027E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.27 | backward-params-all-reduce: 64.44 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.39 | optimizer-unscale-and-check-inf: 16.61 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 98.23 | batch-generator: 8.43
 iteration    77500/  200000 | consumed samples:      2480000 | elapsed time per iteration (ms): 258.8 | learning rate: 2.968E-05 | global batch size:    32 | lm loss: 1.646396E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.18 | backward-params-all-reduce: 64.32 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.27 | optimizer-unscale-and-check-inf: 16.81 | optimizer-clip-main-grad: 7.54 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 98.23 | batch-generator: 8.43
-------------------------------------------------------------------------------------------------
 validation loss at iteration 77500 | lm loss value: 3.262482E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 77500, match long value: -0.00044780315994385415 | match short value: -0.015246906683947303 
-----------------------------------------------------------------------------------------------------------
 iteration    77600/  200000 | consumed samples:      2483200 | elapsed time per iteration (ms): 273.4 | learning rate: 2.959E-05 | global batch size:    32 | lm loss: 1.636918E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.55 | backward-params-all-reduce: 63.39 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.67 | optimizer-unscale-and-check-inf: 16.77 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 98.69 | batch-generator: 16.22
 iteration    77700/  200000 | consumed samples:      2486400 | elapsed time per iteration (ms): 259.7 | learning rate: 2.950E-05 | global batch size:    32 | lm loss: 1.635204E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.35 | backward-params-all-reduce: 63.26 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.55 | optimizer-unscale-and-check-inf: 17.31 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.22 | batch-generator: 8.66
 iteration    77800/  200000 | consumed samples:      2489600 | elapsed time per iteration (ms): 260.4 | learning rate: 2.941E-05 | global batch size:    32 | lm loss: 1.624195E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.54 | backward-params-all-reduce: 63.06 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.48 | optimizer-unscale-and-check-inf: 17.41 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.28 | batch-generator: 8.63
 iteration    77900/  200000 | consumed samples:      2492800 | elapsed time per iteration (ms): 271.8 | learning rate: 2.932E-05 | global batch size:    32 | lm loss: 1.646724E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.65 | backward-params-all-reduce: 64.35 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.40 | optimizer-unscale-and-check-inf: 16.47 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 14.16 | optimizer: 99.80 | batch-generator: 8.65
 iteration    78000/  200000 | consumed samples:      2496000 | elapsed time per iteration (ms): 262.6 | learning rate: 2.923E-05 | global batch size:    32 | lm loss: 1.673829E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.84 | backward-params-all-reduce: 63.70 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 11.91 | optimizer-unscale-and-check-inf: 17.89 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.11 | batch-generator: 8.55
-------------------------------------------------------------------------------------------------
 validation loss at iteration 78000 | lm loss value: 3.327621E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 78000, match long value: -0.002540717047094034 | match short value: -0.008071221873211907 
---------------------------------------------------------------------------------------------------------
 iteration    78100/  200000 | consumed samples:      2499200 | elapsed time per iteration (ms): 278.9 | learning rate: 2.913E-05 | global batch size:    32 | lm loss: 1.657897E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.21 | backward-params-all-reduce: 64.01 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.12 | optimizer-unscale-and-check-inf: 16.64 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.06 | batch-generator: 22.93
 iteration    78200/  200000 | consumed samples:      2502400 | elapsed time per iteration (ms): 266.5 | learning rate: 2.904E-05 | global batch size:    32 | lm loss: 1.592202E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.21 | backward-params-all-reduce: 64.60 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.46 | optimizer-unscale-and-check-inf: 18.19 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.45 | optimizer: 99.90 | batch-generator: 14.35
 iteration    78300/  200000 | consumed samples:      2505600 | elapsed time per iteration (ms): 260.4 | learning rate: 2.895E-05 | global batch size:    32 | lm loss: 1.502782E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.48 | backward-params-all-reduce: 64.57 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.66 | optimizer-unscale-and-check-inf: 17.43 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 99.35 | batch-generator: 8.59
 iteration    78400/  200000 | consumed samples:      2508800 | elapsed time per iteration (ms): 261.6 | learning rate: 2.886E-05 | global batch size:    32 | lm loss: 1.503649E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.05 | backward-params-all-reduce: 63.90 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.85 | optimizer-unscale-and-check-inf: 17.33 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 99.53 | batch-generator: 8.60
 iteration    78500/  200000 | consumed samples:      2512000 | elapsed time per iteration (ms): 260.8 | learning rate: 2.877E-05 | global batch size:    32 | lm loss: 1.495466E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.99 | backward-params-all-reduce: 65.23 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.05 | optimizer-unscale-and-check-inf: 17.89 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.19 | batch-generator: 8.50
-------------------------------------------------------------------------------------------------
 validation loss at iteration 78500 | lm loss value: 3.186419E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 78500, match long value: 0.005752385295195386 | match short value: -0.06288400598208214 
-------------------------------------------------------------------------------------------------------
 iteration    78600/  200000 | consumed samples:      2515200 | elapsed time per iteration (ms): 284.2 | learning rate: 2.868E-05 | global batch size:    32 | lm loss: 1.502138E-05 | loss scale: 536870912.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.90 | backward-params-all-reduce: 66.10 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.51 | optimizer-unscale-and-check-inf: 17.85 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 14.19 | optimizer: 101.39 | batch-generator: 16.85
 iteration    78700/  200000 | consumed samples:      2518400 | elapsed time per iteration (ms): 265.6 | learning rate: 2.859E-05 | global batch size:    32 | lm loss: 1.510725E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.24 | backward-params-all-reduce: 63.34 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.24 | optimizer-unscale-and-check-inf: 18.94 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 100.51 | batch-generator: 8.60
 iteration    78800/  200000 | consumed samples:      2521600 | elapsed time per iteration (ms): 261.1 | learning rate: 2.850E-05 | global batch size:    32 | lm loss: 1.536814E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.20 | backward-params-all-reduce: 64.52 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.26 | optimizer-unscale-and-check-inf: 18.42 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 100.00 | batch-generator: 8.57
 iteration    78900/  200000 | consumed samples:      2524800 | elapsed time per iteration (ms): 261.5 | learning rate: 2.841E-05 | global batch size:    32 | lm loss: 1.510202E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.41 | backward-params-all-reduce: 64.14 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.13 | optimizer-unscale-and-check-inf: 17.86 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.22 | batch-generator: 8.50
 iteration    79000/  200000 | consumed samples:      2528000 | elapsed time per iteration (ms): 259.9 | learning rate: 2.831E-05 | global batch size:    32 | lm loss: 1.512289E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.16 | backward-params-all-reduce: 62.76 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.01 | optimizer-unscale-and-check-inf: 19.71 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 101.01 | batch-generator: 8.50
-------------------------------------------------------------------------------------------------
 validation loss at iteration 79000 | lm loss value: 3.226978E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 79000, match long value: 0.002006855625559718 | match short value: -0.015332924116734523 
--------------------------------------------------------------------------------------------------------
 iteration    79100/  200000 | consumed samples:      2531200 | elapsed time per iteration (ms): 276.1 | learning rate: 2.822E-05 | global batch size:    32 | lm loss: 1.518106E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.59 | backward-params-all-reduce: 63.57 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.17 | optimizer-unscale-and-check-inf: 18.09 | optimizer-clip-main-grad: 7.74 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.64 | batch-generator: 16.46
 iteration    79200/  200000 | consumed samples:      2534400 | elapsed time per iteration (ms): 257.8 | learning rate: 2.813E-05 | global batch size:    32 | lm loss: 1.531391E-05 | loss scale: 268435456.0 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.09 | backward-params-all-reduce: 63.78 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.49 | optimizer-unscale-and-check-inf: 18.22 | optimizer-clip-main-grad: 7.44 | optimizer-copy-main-to-model-params: 12.24 | optimizer: 98.56 | batch-generator: 8.56
 iteration    79300/  200000 | consumed samples:      2537600 | elapsed time per iteration (ms): 259.7 | learning rate: 2.804E-05 | global batch size:    32 | lm loss: 1.565326E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.74 | backward-params-all-reduce: 63.12 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.32 | optimizer-unscale-and-check-inf: 18.20 | optimizer-clip-main-grad: 7.53 | optimizer-copy-main-to-model-params: 12.36 | optimizer: 99.06 | batch-generator: 8.58
 iteration    79400/  200000 | consumed samples:      2540800 | elapsed time per iteration (ms): 275.1 | learning rate: 2.795E-05 | global batch size:    32 | lm loss: 1.540708E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.70 | backward-params-all-reduce: 64.31 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.24 | optimizer-unscale-and-check-inf: 17.74 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 14.28 | optimizer: 101.00 | batch-generator: 8.51
 iteration    79500/  200000 | consumed samples:      2544000 | elapsed time per iteration (ms): 261.0 | learning rate: 2.786E-05 | global batch size:    32 | lm loss: 1.540930E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.34 | backward-params-all-reduce: 64.11 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.42 | optimizer-unscale-and-check-inf: 18.62 | optimizer-clip-main-grad: 7.73 | optimizer-copy-main-to-model-params: 12.54 | optimizer: 100.47 | batch-generator: 8.62
-------------------------------------------------------------------------------------------------
 validation loss at iteration 79500 | lm loss value: 3.273566E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 79500, match long value: -0.0006827567106643283 | match short value: -0.048188496985914965 
----------------------------------------------------------------------------------------------------------
 iteration    79600/  200000 | consumed samples:      2547200 | elapsed time per iteration (ms): 279.4 | learning rate: 2.777E-05 | global batch size:    32 | lm loss: 1.565508E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.46 | backward-params-all-reduce: 63.94 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.05 | optimizer-unscale-and-check-inf: 17.99 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.41 | batch-generator: 22.21
 iteration    79700/  200000 | consumed samples:      2550400 | elapsed time per iteration (ms): 259.0 | learning rate: 2.768E-05 | global batch size:    32 | lm loss: 1.551625E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.27 | backward-params-all-reduce: 64.24 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.46 | optimizer-unscale-and-check-inf: 17.30 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.02 | batch-generator: 8.56
 iteration    79800/  200000 | consumed samples:      2553600 | elapsed time per iteration (ms): 260.1 | learning rate: 2.759E-05 | global batch size:    32 | lm loss: 1.564778E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.55 | backward-params-all-reduce: 63.76 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.09 | optimizer-unscale-and-check-inf: 16.46 | optimizer-clip-main-grad: 7.72 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.01 | batch-generator: 8.63
 iteration    79900/  200000 | consumed samples:      2556800 | elapsed time per iteration (ms): 260.0 | learning rate: 2.750E-05 | global batch size:    32 | lm loss: 1.555386E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.26 | backward-params-all-reduce: 63.63 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.86 | optimizer-unscale-and-check-inf: 16.56 | optimizer-clip-main-grad: 7.71 | optimizer-copy-main-to-model-params: 12.54 | optimizer: 98.84 | batch-generator: 8.66
 iteration    80000/  200000 | consumed samples:      2560000 | elapsed time per iteration (ms): 259.1 | learning rate: 2.741E-05 | global batch size:    32 | lm loss: 1.591875E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.07 | backward-params-all-reduce: 63.27 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.64 | optimizer-unscale-and-check-inf: 16.52 | optimizer-clip-main-grad: 7.74 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 98.57 | batch-generator: 8.57
-------------------------------------------------------------------------------------------------
 validation loss at iteration 80000 | lm loss value: 3.262452E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 80000, match long value: 0.0004695361420749187 | match short value: -0.071615833085064 
------------------------------------------------------------------------------------------------------
saving checkpoint at iteration   80000 to verify1
  successfully saved checkpoint at iteration   80000 to verify1
time (ms) | save-checkpoint: 24746.67
 iteration    80100/  200000 | consumed samples:      2563200 | elapsed time per iteration (ms): 531.9 | learning rate: 2.732E-05 | global batch size:    32 | lm loss: 1.563462E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.83 | backward-params-all-reduce: 63.11 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.10 | optimizer-unscale-and-check-inf: 17.12 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 14.24 | optimizer: 100.18 | batch-generator: 15.75
 iteration    80200/  200000 | consumed samples:      2566400 | elapsed time per iteration (ms): 260.6 | learning rate: 2.722E-05 | global batch size:    32 | lm loss: 1.560429E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.17 | backward-params-all-reduce: 63.59 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.20 | optimizer-unscale-and-check-inf: 17.27 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.76 | batch-generator: 8.56
 iteration    80300/  200000 | consumed samples:      2569600 | elapsed time per iteration (ms): 261.4 | learning rate: 2.713E-05 | global batch size:    32 | lm loss: 1.547657E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.44 | backward-params-all-reduce: 64.09 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.17 | optimizer-unscale-and-check-inf: 18.40 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.83 | batch-generator: 8.55
 iteration    80400/  200000 | consumed samples:      2572800 | elapsed time per iteration (ms): 259.0 | learning rate: 2.704E-05 | global batch size:    32 | lm loss: 1.608594E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.54 | backward-params-all-reduce: 64.65 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.30 | optimizer-unscale-and-check-inf: 16.87 | optimizer-clip-main-grad: 7.45 | optimizer-copy-main-to-model-params: 12.28 | optimizer: 97.07 | batch-generator: 8.77
 iteration    80500/  200000 | consumed samples:      2576000 | elapsed time per iteration (ms): 261.2 | learning rate: 2.695E-05 | global batch size:    32 | lm loss: 1.581461E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.26 | backward-params-all-reduce: 64.19 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.80 | optimizer-unscale-and-check-inf: 16.32 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.55 | optimizer: 98.40 | batch-generator: 8.57
-------------------------------------------------------------------------------------------------
 validation loss at iteration 80500 | lm loss value: 3.247853E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 80500, match long value: 0.0009146369946100164 | match short value: -0.007374027612870385 
---------------------------------------------------------------------------------------------------------
 iteration    80600/  200000 | consumed samples:      2579200 | elapsed time per iteration (ms): 273.6 | learning rate: 2.686E-05 | global batch size:    32 | lm loss: 1.567597E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.23 | backward-params-all-reduce: 64.16 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.28 | optimizer-unscale-and-check-inf: 15.91 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.54 | optimizer: 97.44 | batch-generator: 16.34
 iteration    80700/  200000 | consumed samples:      2582400 | elapsed time per iteration (ms): 258.9 | learning rate: 2.677E-05 | global batch size:    32 | lm loss: 1.552638E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.23 | backward-params-all-reduce: 62.74 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.44 | optimizer-unscale-and-check-inf: 16.46 | optimizer-clip-main-grad: 7.72 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 98.33 | batch-generator: 8.77
 iteration    80800/  200000 | consumed samples:      2585600 | elapsed time per iteration (ms): 268.9 | learning rate: 2.668E-05 | global batch size:    32 | lm loss: 1.573817E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.54 | backward-params-all-reduce: 62.65 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.99 | optimizer-unscale-and-check-inf: 17.39 | optimizer-clip-main-grad: 7.83 | optimizer-copy-main-to-model-params: 14.28 | optimizer: 101.65 | batch-generator: 8.58
 iteration    80900/  200000 | consumed samples:      2588800 | elapsed time per iteration (ms): 268.7 | learning rate: 2.659E-05 | global batch size:    32 | lm loss: 1.562637E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.40 | backward-params-all-reduce: 65.20 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.28 | optimizer-unscale-and-check-inf: 18.34 | optimizer-clip-main-grad: 7.73 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 100.08 | batch-generator: 8.76
 iteration    81000/  200000 | consumed samples:      2592000 | elapsed time per iteration (ms): 261.0 | learning rate: 2.650E-05 | global batch size:    32 | lm loss: 1.571589E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.47 | backward-params-all-reduce: 62.95 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.28 | optimizer-unscale-and-check-inf: 17.79 | optimizer-clip-main-grad: 7.72 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 100.45 | batch-generator: 8.59
-------------------------------------------------------------------------------------------------
 validation loss at iteration 81000 | lm loss value: 3.257005E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 81000, match long value: 0.0028083625208592975 | match short value: -0.04918021108239898 
--------------------------------------------------------------------------------------------------------
 iteration    81100/  200000 | consumed samples:      2595200 | elapsed time per iteration (ms): 340.1 | learning rate: 2.641E-05 | global batch size:    32 | lm loss: 1.581101E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.26 | backward-params-all-reduce: 63.47 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.53 | optimizer-unscale-and-check-inf: 16.38 | optimizer-clip-main-grad: 7.70 | optimizer-copy-main-to-model-params: 12.55 | optimizer: 98.41 | batch-generator: 84.78
 iteration    81200/  200000 | consumed samples:      2598400 | elapsed time per iteration (ms): 260.6 | learning rate: 2.632E-05 | global batch size:    32 | lm loss: 1.560698E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.15 | backward-params-all-reduce: 64.08 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.48 | optimizer-unscale-and-check-inf: 18.59 | optimizer-clip-main-grad: 7.70 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 100.48 | batch-generator: 8.66
 iteration    81300/  200000 | consumed samples:      2601600 | elapsed time per iteration (ms): 267.6 | learning rate: 2.622E-05 | global batch size:    32 | lm loss: 1.519933E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.23 | backward-params-all-reduce: 64.94 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.17 | optimizer-unscale-and-check-inf: 19.02 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 100.50 | batch-generator: 14.74
 iteration    81400/  200000 | consumed samples:      2604800 | elapsed time per iteration (ms): 263.9 | learning rate: 2.613E-05 | global batch size:    32 | lm loss: 1.457819E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.18 | backward-params-all-reduce: 65.77 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.28 | optimizer-unscale-and-check-inf: 16.59 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 99.19 | batch-generator: 8.73
 iteration    81500/  200000 | consumed samples:      2608000 | elapsed time per iteration (ms): 262.0 | learning rate: 2.604E-05 | global batch size:    32 | lm loss: 1.469014E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.86 | backward-params-all-reduce: 65.24 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.51 | optimizer-unscale-and-check-inf: 18.01 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.79 | batch-generator: 8.52
-------------------------------------------------------------------------------------------------
 validation loss at iteration 81500 | lm loss value: 3.146036E-05 | lm loss PPL: 1.000031E+00 | 
 at iteration 81500, match long value: -0.000499031690762531 | match short value: -0.03158593897269405 
--------------------------------------------------------------------------------------------------------
 iteration    81600/  200000 | consumed samples:      2611200 | elapsed time per iteration (ms): 282.8 | learning rate: 2.595E-05 | global batch size:    32 | lm loss: 1.452785E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.29 | backward-params-all-reduce: 63.58 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.53 | optimizer-unscale-and-check-inf: 17.73 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 14.35 | optimizer: 101.52 | batch-generator: 15.57
 iteration    81700/  200000 | consumed samples:      2614400 | elapsed time per iteration (ms): 260.5 | learning rate: 2.586E-05 | global batch size:    32 | lm loss: 1.490067E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.27 | backward-params-all-reduce: 64.32 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.43 | optimizer-unscale-and-check-inf: 18.64 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 100.38 | batch-generator: 8.49
 iteration    81800/  200000 | consumed samples:      2617600 | elapsed time per iteration (ms): 260.7 | learning rate: 2.577E-05 | global batch size:    32 | lm loss: 1.467630E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.26 | backward-params-all-reduce: 65.12 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.31 | optimizer-unscale-and-check-inf: 18.81 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 100.34 | batch-generator: 8.44
 iteration    81900/  200000 | consumed samples:      2620800 | elapsed time per iteration (ms): 259.8 | learning rate: 2.568E-05 | global batch size:    32 | lm loss: 1.465540E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.31 | backward-params-all-reduce: 64.43 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.23 | optimizer-unscale-and-check-inf: 17.78 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 100.33 | batch-generator: 8.50
 iteration    82000/  200000 | consumed samples:      2624000 | elapsed time per iteration (ms): 259.7 | learning rate: 2.559E-05 | global batch size:    32 | lm loss: 1.464891E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.14 | backward-params-all-reduce: 63.71 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.55 | optimizer-unscale-and-check-inf: 18.97 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 100.77 | batch-generator: 8.35
-------------------------------------------------------------------------------------------------
 validation loss at iteration 82000 | lm loss value: 3.303119E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 82000, match long value: -0.0011138266910212507 | match short value: -0.007949105765491335 
----------------------------------------------------------------------------------------------------------
 iteration    82100/  200000 | consumed samples:      2627200 | elapsed time per iteration (ms): 272.5 | learning rate: 2.550E-05 | global batch size:    32 | lm loss: 1.446060E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.12 | backward-params-all-reduce: 64.44 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.31 | optimizer-unscale-and-check-inf: 17.28 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.97 | batch-generator: 15.81
 iteration    82200/  200000 | consumed samples:      2630400 | elapsed time per iteration (ms): 260.5 | learning rate: 2.540E-05 | global batch size:    32 | lm loss: 1.472184E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.52 | backward-params-all-reduce: 64.35 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.19 | optimizer-unscale-and-check-inf: 18.26 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 100.72 | batch-generator: 8.48
 iteration    82300/  200000 | consumed samples:      2633600 | elapsed time per iteration (ms): 270.2 | learning rate: 2.531E-05 | global batch size:    32 | lm loss: 1.484627E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.84 | backward-params-all-reduce: 63.19 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.75 | optimizer-unscale-and-check-inf: 16.99 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 14.31 | optimizer: 100.80 | batch-generator: 8.55
 iteration    82400/  200000 | consumed samples:      2636800 | elapsed time per iteration (ms): 260.6 | learning rate: 2.522E-05 | global batch size:    32 | lm loss: 1.473388E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.72 | backward-params-all-reduce: 64.09 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.73 | optimizer-unscale-and-check-inf: 16.90 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.95 | batch-generator: 8.53
 iteration    82500/  200000 | consumed samples:      2640000 | elapsed time per iteration (ms): 260.2 | learning rate: 2.513E-05 | global batch size:    32 | lm loss: 1.476903E-05 | loss scale: 536870912.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.18 | backward-params-all-reduce: 65.18 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.30 | optimizer-unscale-and-check-inf: 18.32 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.84 | batch-generator: 8.49
-------------------------------------------------------------------------------------------------
 validation loss at iteration 82500 | lm loss value: 3.253694E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 82500, match long value: -0.0006210972145443357 | match short value: -0.026851632172964116 
----------------------------------------------------------------------------------------------------------
 iteration    82600/  200000 | consumed samples:      2643200 | elapsed time per iteration (ms): 276.3 | learning rate: 2.504E-05 | global batch size:    32 | lm loss: 1.540304E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.45 | backward-params-all-reduce: 64.25 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.87 | optimizer-unscale-and-check-inf: 18.14 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.27 | optimizer: 99.09 | batch-generator: 20.97
 iteration    82700/  200000 | consumed samples:      2646400 | elapsed time per iteration (ms): 258.0 | learning rate: 2.495E-05 | global batch size:    32 | lm loss: 1.540469E-05 | loss scale: 67108864.0 | grad norm: 0.001 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.12 | backward-params-all-reduce: 64.23 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.84 | optimizer-unscale-and-check-inf: 18.44 | optimizer-clip-main-grad: 7.42 | optimizer-copy-main-to-model-params: 12.22 | optimizer: 99.08 | batch-generator: 8.42
 iteration    82800/  200000 | consumed samples:      2649600 | elapsed time per iteration (ms): 259.8 | learning rate: 2.486E-05 | global batch size:    32 | lm loss: 1.511832E-05 | loss scale: 67108864.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.20 | backward-params-all-reduce: 63.75 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.93 | optimizer-unscale-and-check-inf: 17.94 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 100.19 | batch-generator: 8.48
 iteration    82900/  200000 | consumed samples:      2652800 | elapsed time per iteration (ms): 261.4 | learning rate: 2.477E-05 | global batch size:    32 | lm loss: 1.477263E-05 | loss scale: 67108864.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.12 | backward-params-all-reduce: 64.24 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.56 | optimizer-unscale-and-check-inf: 17.56 | optimizer-clip-main-grad: 7.70 | optimizer-copy-main-to-model-params: 12.55 | optimizer: 100.53 | batch-generator: 8.61
 iteration    83000/  200000 | consumed samples:      2656000 | elapsed time per iteration (ms): 266.0 | learning rate: 2.468E-05 | global batch size:    32 | lm loss: 1.488248E-05 | loss scale: 67108864.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.60 | backward-params-all-reduce: 63.65 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.82 | optimizer-unscale-and-check-inf: 18.05 | optimizer-clip-main-grad: 7.70 | optimizer-copy-main-to-model-params: 14.28 | optimizer: 102.01 | batch-generator: 8.64
-------------------------------------------------------------------------------------------------
 validation loss at iteration 83000 | lm loss value: 3.309504E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 83000, match long value: 0.0005718281943295898 | match short value: -0.08842591286334184 
--------------------------------------------------------------------------------------------------------
 iteration    83100/  200000 | consumed samples:      2659200 | elapsed time per iteration (ms): 277.3 | learning rate: 2.459E-05 | global batch size:    32 | lm loss: 1.481252E-05 | loss scale: 67108864.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.75 | backward-params-all-reduce: 65.57 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.07 | optimizer-unscale-and-check-inf: 16.87 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.54 | optimizer: 99.25 | batch-generator: 14.96
 iteration    83200/  200000 | consumed samples:      2662400 | elapsed time per iteration (ms): 260.3 | learning rate: 2.450E-05 | global batch size:    32 | lm loss: 1.468373E-05 | loss scale: 67108864.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.96 | backward-params-all-reduce: 64.71 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.88 | optimizer-unscale-and-check-inf: 18.29 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 100.43 | batch-generator: 8.43
 iteration    83300/  200000 | consumed samples:      2665600 | elapsed time per iteration (ms): 259.7 | learning rate: 2.441E-05 | global batch size:    32 | lm loss: 1.484490E-05 | loss scale: 67108864.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.19 | backward-params-all-reduce: 63.75 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.85 | optimizer-unscale-and-check-inf: 17.66 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 99.73 | batch-generator: 8.59
 iteration    83400/  200000 | consumed samples:      2668800 | elapsed time per iteration (ms): 259.3 | learning rate: 2.431E-05 | global batch size:    32 | lm loss: 1.479196E-05 | loss scale: 67108864.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.56 | backward-params-all-reduce: 64.55 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.91 | optimizer-unscale-and-check-inf: 17.88 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 100.02 | batch-generator: 8.47
 iteration    83500/  200000 | consumed samples:      2672000 | elapsed time per iteration (ms): 258.9 | learning rate: 2.422E-05 | global batch size:    32 | lm loss: 1.498100E-05 | loss scale: 67108864.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.29 | backward-params-all-reduce: 62.83 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.29 | optimizer-unscale-and-check-inf: 17.10 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.66 | batch-generator: 8.62
-------------------------------------------------------------------------------------------------
 validation loss at iteration 83500 | lm loss value: 3.225119E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 83500, match long value: -0.0017118465260956882 | match short value: -0.04387199971927047 
---------------------------------------------------------------------------------------------------------
 iteration    83600/  200000 | consumed samples:      2675200 | elapsed time per iteration (ms): 274.4 | learning rate: 2.413E-05 | global batch size:    32 | lm loss: 1.496260E-05 | loss scale: 67108864.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.05 | backward-params-all-reduce: 65.44 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.47 | optimizer-unscale-and-check-inf: 16.75 | optimizer-clip-main-grad: 7.73 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.70 | batch-generator: 15.92
 iteration    83700/  200000 | consumed samples:      2678400 | elapsed time per iteration (ms): 260.0 | learning rate: 2.404E-05 | global batch size:    32 | lm loss: 1.514173E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.19 | backward-params-all-reduce: 64.20 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.55 | optimizer-unscale-and-check-inf: 17.79 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.63 | batch-generator: 8.48
 iteration    83800/  200000 | consumed samples:      2681600 | elapsed time per iteration (ms): 270.2 | learning rate: 2.395E-05 | global batch size:    32 | lm loss: 1.499125E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.84 | backward-params-all-reduce: 64.90 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.80 | optimizer-unscale-and-check-inf: 16.45 | optimizer-clip-main-grad: 7.53 | optimizer-copy-main-to-model-params: 14.22 | optimizer: 100.14 | batch-generator: 8.45
 iteration    83900/  200000 | consumed samples:      2684800 | elapsed time per iteration (ms): 261.2 | learning rate: 2.386E-05 | global batch size:    32 | lm loss: 1.510536E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.18 | backward-params-all-reduce: 64.77 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.82 | optimizer-unscale-and-check-inf: 17.75 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.80 | batch-generator: 8.43
 iteration    84000/  200000 | consumed samples:      2688000 | elapsed time per iteration (ms): 258.0 | learning rate: 2.377E-05 | global batch size:    32 | lm loss: 1.531550E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.23 | backward-params-all-reduce: 63.41 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.35 | optimizer-unscale-and-check-inf: 17.33 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.98 | batch-generator: 8.51
-------------------------------------------------------------------------------------------------
 validation loss at iteration 84000 | lm loss value: 3.371625E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 84000, match long value: -0.0019075572566245238 | match short value: -0.005140148125729745 
----------------------------------------------------------------------------------------------------------
 iteration    84100/  200000 | consumed samples:      2691200 | elapsed time per iteration (ms): 276.3 | learning rate: 2.368E-05 | global batch size:    32 | lm loss: 1.504557E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.42 | backward-params-all-reduce: 63.92 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.68 | optimizer-unscale-and-check-inf: 16.42 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.38 | batch-generator: 19.63
 iteration    84200/  200000 | consumed samples:      2694400 | elapsed time per iteration (ms): 259.6 | learning rate: 2.359E-05 | global batch size:    32 | lm loss: 1.495691E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 54.15 | backward-params-all-reduce: 62.73 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.65 | optimizer-unscale-and-check-inf: 17.04 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.98 | batch-generator: 8.52
 iteration    84300/  200000 | consumed samples:      2697600 | elapsed time per iteration (ms): 257.9 | learning rate: 2.350E-05 | global batch size:    32 | lm loss: 1.517591E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.03 | backward-params-all-reduce: 63.35 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.57 | optimizer-unscale-and-check-inf: 16.32 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.20 | batch-generator: 8.54
 iteration    84400/  200000 | consumed samples:      2700800 | elapsed time per iteration (ms): 265.7 | learning rate: 2.340E-05 | global batch size:    32 | lm loss: 1.479697E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.02 | backward-params-all-reduce: 65.86 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.67 | optimizer-unscale-and-check-inf: 17.02 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 98.86 | batch-generator: 13.89
 iteration    84500/  200000 | consumed samples:      2704000 | elapsed time per iteration (ms): 269.1 | learning rate: 2.331E-05 | global batch size:    32 | lm loss: 1.407563E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.96 | backward-params-all-reduce: 64.29 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.08 | optimizer-unscale-and-check-inf: 16.14 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 14.16 | optimizer: 100.13 | batch-generator: 8.47
-------------------------------------------------------------------------------------------------
 validation loss at iteration 84500 | lm loss value: 3.242135E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 84500, match long value: 0.0011865437303305326 | match short value: -0.03728509645350176 
--------------------------------------------------------------------------------------------------------
 iteration    84600/  200000 | consumed samples:      2707200 | elapsed time per iteration (ms): 270.7 | learning rate: 2.322E-05 | global batch size:    32 | lm loss: 1.398917E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.23 | backward-params-all-reduce: 65.98 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.47 | optimizer-unscale-and-check-inf: 16.13 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.93 | batch-generator: 15.42
 iteration    84700/  200000 | consumed samples:      2710400 | elapsed time per iteration (ms): 258.9 | learning rate: 2.313E-05 | global batch size:    32 | lm loss: 1.406295E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.74 | backward-params-all-reduce: 63.68 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.61 | optimizer-unscale-and-check-inf: 16.43 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.27 | batch-generator: 8.46
 iteration    84800/  200000 | consumed samples:      2713600 | elapsed time per iteration (ms): 259.1 | learning rate: 2.304E-05 | global batch size:    32 | lm loss: 1.410733E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.70 | backward-params-all-reduce: 63.32 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.35 | optimizer-unscale-and-check-inf: 16.42 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.04 | batch-generator: 8.54
 iteration    84900/  200000 | consumed samples:      2716800 | elapsed time per iteration (ms): 259.8 | learning rate: 2.295E-05 | global batch size:    32 | lm loss: 1.411197E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.53 | backward-params-all-reduce: 64.08 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.89 | optimizer-unscale-and-check-inf: 15.89 | optimizer-clip-main-grad: 7.73 | optimizer-copy-main-to-model-params: 12.56 | optimizer: 99.30 | batch-generator: 8.59
 iteration    85000/  200000 | consumed samples:      2720000 | elapsed time per iteration (ms): 259.7 | learning rate: 2.286E-05 | global batch size:    32 | lm loss: 1.410131E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.22 | backward-params-all-reduce: 64.67 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.38 | optimizer-unscale-and-check-inf: 16.58 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.34 | batch-generator: 8.48
-------------------------------------------------------------------------------------------------
 validation loss at iteration 85000 | lm loss value: 3.392316E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 85000, match long value: 0.0025293522881936286 | match short value: -0.011647802078122238 
---------------------------------------------------------------------------------------------------------
 iteration    85100/  200000 | consumed samples:      2723200 | elapsed time per iteration (ms): 270.4 | learning rate: 2.277E-05 | global batch size:    32 | lm loss: 1.461842E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.09 | backward-params-all-reduce: 64.01 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.39 | optimizer-unscale-and-check-inf: 17.23 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.88 | batch-generator: 16.21
 iteration    85200/  200000 | consumed samples:      2726400 | elapsed time per iteration (ms): 266.7 | learning rate: 2.268E-05 | global batch size:    32 | lm loss: 1.477179E-05 | loss scale: 268435456.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.51 | backward-params-all-reduce: 64.92 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.67 | optimizer-unscale-and-check-inf: 16.94 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 14.21 | optimizer: 101.64 | batch-generator: 8.53
 iteration    85300/  200000 | consumed samples:      2729600 | elapsed time per iteration (ms): 264.7 | learning rate: 2.258E-05 | global batch size:    32 | lm loss: 1.458710E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.52 | backward-params-all-reduce: 64.41 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 14.03 | optimizer-unscale-and-check-inf: 17.14 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 100.40 | batch-generator: 8.42
 iteration    85400/  200000 | consumed samples:      2732800 | elapsed time per iteration (ms): 259.1 | learning rate: 2.249E-05 | global batch size:    32 | lm loss: 1.411726E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.76 | backward-params-all-reduce: 63.20 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.61 | optimizer-unscale-and-check-inf: 16.60 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.56 | batch-generator: 8.56
 iteration    85500/  200000 | consumed samples:      2736000 | elapsed time per iteration (ms): 258.5 | learning rate: 2.240E-05 | global batch size:    32 | lm loss: 1.400463E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.19 | backward-params-all-reduce: 64.19 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.39 | optimizer-unscale-and-check-inf: 16.31 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.93 | batch-generator: 8.49
-------------------------------------------------------------------------------------------------
 validation loss at iteration 85500 | lm loss value: 3.073693E-05 | lm loss PPL: 1.000031E+00 | 
 at iteration 85500, match long value: 0.00038189498406070247 | match short value: -0.052222172516658796 
----------------------------------------------------------------------------------------------------------
 iteration    85600/  200000 | consumed samples:      2739200 | elapsed time per iteration (ms): 273.9 | learning rate: 2.231E-05 | global batch size:    32 | lm loss: 1.427013E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.32 | backward-params-all-reduce: 63.17 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.62 | optimizer-unscale-and-check-inf: 16.94 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.77 | batch-generator: 19.33
 iteration    85700/  200000 | consumed samples:      2742400 | elapsed time per iteration (ms): 260.7 | learning rate: 2.222E-05 | global batch size:    32 | lm loss: 1.417646E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.04 | backward-params-all-reduce: 63.74 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.47 | optimizer-unscale-and-check-inf: 17.67 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.49 | batch-generator: 8.53
 iteration    85800/  200000 | consumed samples:      2745600 | elapsed time per iteration (ms): 260.2 | learning rate: 2.213E-05 | global batch size:    32 | lm loss: 1.436988E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.68 | backward-params-all-reduce: 64.51 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.66 | optimizer-unscale-and-check-inf: 17.02 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.01 | batch-generator: 8.52
 iteration    85900/  200000 | consumed samples:      2748800 | elapsed time per iteration (ms): 260.9 | learning rate: 2.204E-05 | global batch size:    32 | lm loss: 1.432129E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.68 | backward-params-all-reduce: 63.51 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.21 | optimizer-unscale-and-check-inf: 17.00 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 98.48 | batch-generator: 8.45
 iteration    86000/  200000 | consumed samples:      2752000 | elapsed time per iteration (ms): 271.2 | learning rate: 2.195E-05 | global batch size:    32 | lm loss: 1.454325E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.30 | backward-params-all-reduce: 63.66 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.69 | optimizer-unscale-and-check-inf: 16.11 | optimizer-clip-main-grad: 7.52 | optimizer-copy-main-to-model-params: 14.13 | optimizer: 98.65 | batch-generator: 8.65
-------------------------------------------------------------------------------------------------
 validation loss at iteration 86000 | lm loss value: 3.389814E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 86000, match long value: 0.001122299505350346 | match short value: 0.00047793132771562624 
---------------------------------------------------------------------------------------------------------
 iteration    86100/  200000 | consumed samples:      2755200 | elapsed time per iteration (ms): 272.9 | learning rate: 2.186E-05 | global batch size:    32 | lm loss: 1.443415E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.98 | backward-params-all-reduce: 64.14 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.52 | optimizer-unscale-and-check-inf: 18.10 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.97 | batch-generator: 15.14
 iteration    86200/  200000 | consumed samples:      2758400 | elapsed time per iteration (ms): 259.7 | learning rate: 2.177E-05 | global batch size:    32 | lm loss: 1.443964E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.25 | backward-params-all-reduce: 62.39 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.94 | optimizer-unscale-and-check-inf: 17.43 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.70 | batch-generator: 8.58
 iteration    86300/  200000 | consumed samples:      2761600 | elapsed time per iteration (ms): 258.9 | learning rate: 2.168E-05 | global batch size:    32 | lm loss: 1.441948E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.21 | backward-params-all-reduce: 63.18 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.79 | optimizer-unscale-and-check-inf: 17.19 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.22 | batch-generator: 8.45
 iteration    86400/  200000 | consumed samples:      2764800 | elapsed time per iteration (ms): 258.4 | learning rate: 2.158E-05 | global batch size:    32 | lm loss: 1.422852E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.90 | backward-params-all-reduce: 63.64 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.45 | optimizer-unscale-and-check-inf: 16.69 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.40 | batch-generator: 8.59
 iteration    86500/  200000 | consumed samples:      2768000 | elapsed time per iteration (ms): 258.9 | learning rate: 2.149E-05 | global batch size:    32 | lm loss: 1.430744E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.18 | backward-params-all-reduce: 63.09 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.18 | optimizer-unscale-and-check-inf: 16.11 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.59 | optimizer: 97.68 | batch-generator: 8.55
-------------------------------------------------------------------------------------------------
 validation loss at iteration 86500 | lm loss value: 3.233220E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 86500, match long value: 0.00048105005009896335 | match short value: -0.06581796234010007 
---------------------------------------------------------------------------------------------------------
 iteration    86600/  200000 | consumed samples:      2771200 | elapsed time per iteration (ms): 273.4 | learning rate: 2.140E-05 | global batch size:    32 | lm loss: 1.436559E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.52 | backward-params-all-reduce: 63.79 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.51 | optimizer-unscale-and-check-inf: 16.70 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.45 | optimizer: 98.40 | batch-generator: 14.83
 iteration    86700/  200000 | consumed samples:      2774400 | elapsed time per iteration (ms): 277.5 | learning rate: 2.131E-05 | global batch size:    32 | lm loss: 1.452949E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.87 | backward-params-all-reduce: 63.46 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.74 | optimizer-unscale-and-check-inf: 16.60 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 14.22 | optimizer: 100.33 | batch-generator: 8.52
 iteration    86800/  200000 | consumed samples:      2777600 | elapsed time per iteration (ms): 260.6 | learning rate: 2.122E-05 | global batch size:    32 | lm loss: 1.442820E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.36 | backward-params-all-reduce: 63.20 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.38 | optimizer-unscale-and-check-inf: 17.49 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.16 | batch-generator: 8.46
 iteration    86900/  200000 | consumed samples:      2780800 | elapsed time per iteration (ms): 259.5 | learning rate: 2.113E-05 | global batch size:    32 | lm loss: 1.430228E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.28 | backward-params-all-reduce: 63.10 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.20 | optimizer-unscale-and-check-inf: 17.47 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.91 | batch-generator: 8.46
 iteration    87000/  200000 | consumed samples:      2784000 | elapsed time per iteration (ms): 260.4 | learning rate: 2.104E-05 | global batch size:    32 | lm loss: 1.445076E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.38 | backward-params-all-reduce: 63.88 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.25 | optimizer-unscale-and-check-inf: 17.44 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 98.98 | batch-generator: 8.36
-------------------------------------------------------------------------------------------------
 validation loss at iteration 87000 | lm loss value: 3.130101E-05 | lm loss PPL: 1.000031E+00 | 
 at iteration 87000, match long value: -2.4812630720559353e-05 | match short value: -0.008921347243330264 
-----------------------------------------------------------------------------------------------------------
 iteration    87100/  200000 | consumed samples:      2787200 | elapsed time per iteration (ms): 275.6 | learning rate: 2.095E-05 | global batch size:    32 | lm loss: 1.445870E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.30 | backward-params-all-reduce: 63.34 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.13 | optimizer-unscale-and-check-inf: 17.44 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 98.79 | batch-generator: 20.89
 iteration    87200/  200000 | consumed samples:      2790400 | elapsed time per iteration (ms): 259.8 | learning rate: 2.086E-05 | global batch size:    32 | lm loss: 1.469345E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.48 | backward-params-all-reduce: 64.03 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.67 | optimizer-unscale-and-check-inf: 17.00 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.39 | optimizer: 98.37 | batch-generator: 8.50
 iteration    87300/  200000 | consumed samples:      2793600 | elapsed time per iteration (ms): 259.2 | learning rate: 2.077E-05 | global batch size:    32 | lm loss: 1.455107E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.98 | backward-params-all-reduce: 64.00 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.70 | optimizer-unscale-and-check-inf: 16.84 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.39 | optimizer: 98.28 | batch-generator: 8.52
 iteration    87400/  200000 | consumed samples:      2796800 | elapsed time per iteration (ms): 267.3 | learning rate: 2.067E-05 | global batch size:    32 | lm loss: 1.450875E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.57 | backward-params-all-reduce: 65.57 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.08 | optimizer-unscale-and-check-inf: 15.70 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 14.26 | optimizer: 98.76 | batch-generator: 8.54
 iteration    87500/  200000 | consumed samples:      2800000 | elapsed time per iteration (ms): 265.3 | learning rate: 2.058E-05 | global batch size:    32 | lm loss: 1.453837E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.51 | backward-params-all-reduce: 63.88 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.11 | optimizer-unscale-and-check-inf: 16.72 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.07 | batch-generator: 8.51
-------------------------------------------------------------------------------------------------
 validation loss at iteration 87500 | lm loss value: 3.216794E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 87500, match long value: 0.0011756059498884972 | match short value: -0.07124714040321879 
--------------------------------------------------------------------------------------------------------
 iteration    87600/  200000 | consumed samples:      2803200 | elapsed time per iteration (ms): 279.0 | learning rate: 2.049E-05 | global batch size:    32 | lm loss: 1.358689E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.26 | backward-params-all-reduce: 65.03 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.22 | optimizer-unscale-and-check-inf: 17.60 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.09 | batch-generator: 21.00
 iteration    87700/  200000 | consumed samples:      2806400 | elapsed time per iteration (ms): 261.3 | learning rate: 2.040E-05 | global batch size:    32 | lm loss: 1.376768E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.55 | backward-params-all-reduce: 63.93 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.47 | optimizer-unscale-and-check-inf: 16.19 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 97.97 | batch-generator: 8.50
 iteration    87800/  200000 | consumed samples:      2809600 | elapsed time per iteration (ms): 259.2 | learning rate: 2.031E-05 | global batch size:    32 | lm loss: 1.365495E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.64 | backward-params-all-reduce: 63.29 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.32 | optimizer-unscale-and-check-inf: 16.24 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 97.84 | batch-generator: 8.51
 iteration    87900/  200000 | consumed samples:      2812800 | elapsed time per iteration (ms): 258.3 | learning rate: 2.022E-05 | global batch size:    32 | lm loss: 1.359282E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.08 | backward-params-all-reduce: 62.99 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.34 | optimizer-unscale-and-check-inf: 15.84 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.54 | optimizer: 97.52 | batch-generator: 8.52
 iteration    88000/  200000 | consumed samples:      2816000 | elapsed time per iteration (ms): 261.4 | learning rate: 2.013E-05 | global batch size:    32 | lm loss: 1.375467E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.49 | backward-params-all-reduce: 64.33 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.47 | optimizer-unscale-and-check-inf: 16.36 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 98.04 | batch-generator: 8.49
-------------------------------------------------------------------------------------------------
 validation loss at iteration 88000 | lm loss value: 3.113145E-05 | lm loss PPL: 1.000031E+00 | 
 at iteration 88000, match long value: -0.0003215168936770814 | match short value: -0.03504337289789791 
---------------------------------------------------------------------------------------------------------
 iteration    88100/  200000 | consumed samples:      2819200 | elapsed time per iteration (ms): 273.7 | learning rate: 2.004E-05 | global batch size:    32 | lm loss: 1.373446E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.33 | backward-params-all-reduce: 64.35 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.27 | optimizer-unscale-and-check-inf: 15.59 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 97.11 | batch-generator: 16.09
 iteration    88200/  200000 | consumed samples:      2822400 | elapsed time per iteration (ms): 273.3 | learning rate: 1.995E-05 | global batch size:    32 | lm loss: 1.394810E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.12 | backward-params-all-reduce: 64.39 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.09 | optimizer-unscale-and-check-inf: 15.45 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 14.23 | optimizer: 98.49 | batch-generator: 8.45
 iteration    88300/  200000 | consumed samples:      2825600 | elapsed time per iteration (ms): 259.5 | learning rate: 1.986E-05 | global batch size:    32 | lm loss: 1.379010E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.61 | backward-params-all-reduce: 64.36 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.06 | optimizer-unscale-and-check-inf: 15.48 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.44 | optimizer: 96.66 | batch-generator: 8.37
 iteration    88400/  200000 | consumed samples:      2828800 | elapsed time per iteration (ms): 260.2 | learning rate: 1.976E-05 | global batch size:    32 | lm loss: 1.366860E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.30 | backward-params-all-reduce: 63.79 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.49 | optimizer-unscale-and-check-inf: 16.20 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 97.93 | batch-generator: 8.42
 iteration    88500/  200000 | consumed samples:      2832000 | elapsed time per iteration (ms): 260.7 | learning rate: 1.967E-05 | global batch size:    32 | lm loss: 1.373977E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.21 | backward-params-all-reduce: 65.43 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.27 | optimizer-unscale-and-check-inf: 16.48 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 97.97 | batch-generator: 8.38
-------------------------------------------------------------------------------------------------
 validation loss at iteration 88500 | lm loss value: 3.437823E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 88500, match long value: 0.008786634889017143 | match short value: 0.0048891444888505805 
--------------------------------------------------------------------------------------------------------
 iteration    88600/  200000 | consumed samples:      2835200 | elapsed time per iteration (ms): 279.8 | learning rate: 1.958E-05 | global batch size:    32 | lm loss: 1.395343E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.59 | backward-params-all-reduce: 63.75 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.40 | optimizer-unscale-and-check-inf: 15.73 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 97.29 | batch-generator: 20.64
 iteration    88700/  200000 | consumed samples:      2838400 | elapsed time per iteration (ms): 259.2 | learning rate: 1.949E-05 | global batch size:    32 | lm loss: 1.402384E-05 | loss scale: 536870912.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.15 | backward-params-all-reduce: 63.38 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.57 | optimizer-unscale-and-check-inf: 15.44 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 97.24 | batch-generator: 8.59
 iteration    88800/  200000 | consumed samples:      2841600 | elapsed time per iteration (ms): 259.0 | learning rate: 1.940E-05 | global batch size:    32 | lm loss: 1.378922E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.70 | backward-params-all-reduce: 64.05 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.28 | optimizer-unscale-and-check-inf: 15.64 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.54 | optimizer: 97.19 | batch-generator: 8.68
 iteration    88900/  200000 | consumed samples:      2844800 | elapsed time per iteration (ms): 272.9 | learning rate: 1.931E-05 | global batch size:    32 | lm loss: 1.393608E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.42 | backward-params-all-reduce: 63.23 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.33 | optimizer-unscale-and-check-inf: 14.95 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 14.26 | optimizer: 98.22 | batch-generator: 8.47
 iteration    89000/  200000 | consumed samples:      2848000 | elapsed time per iteration (ms): 258.2 | learning rate: 1.922E-05 | global batch size:    32 | lm loss: 1.407268E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.43 | backward-params-all-reduce: 62.82 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.33 | optimizer-unscale-and-check-inf: 15.95 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 97.49 | batch-generator: 8.45
-------------------------------------------------------------------------------------------------
 validation loss at iteration 89000 | lm loss value: 3.222416E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 89000, match long value: -0.0028105070528103766 | match short value: -0.051439291103446756 
----------------------------------------------------------------------------------------------------------
 iteration    89100/  200000 | consumed samples:      2851200 | elapsed time per iteration (ms): 273.0 | learning rate: 1.913E-05 | global batch size:    32 | lm loss: 1.401280E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.32 | backward-params-all-reduce: 64.62 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.28 | optimizer-unscale-and-check-inf: 15.84 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 97.33 | batch-generator: 17.23
 iteration    89200/  200000 | consumed samples:      2854400 | elapsed time per iteration (ms): 260.4 | learning rate: 1.904E-05 | global batch size:    32 | lm loss: 1.399967E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.19 | backward-params-all-reduce: 64.14 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.04 | optimizer-unscale-and-check-inf: 17.99 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.28 | batch-generator: 8.39
 iteration    89300/  200000 | consumed samples:      2857600 | elapsed time per iteration (ms): 259.3 | learning rate: 1.895E-05 | global batch size:    32 | lm loss: 1.391883E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.21 | backward-params-all-reduce: 64.35 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.60 | optimizer-unscale-and-check-inf: 19.15 | optimizer-clip-main-grad: 7.51 | optimizer-copy-main-to-model-params: 12.25 | optimizer: 99.70 | batch-generator: 8.57
 iteration    89400/  200000 | consumed samples:      2860800 | elapsed time per iteration (ms): 262.7 | learning rate: 1.885E-05 | global batch size:    32 | lm loss: 1.403464E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.07 | backward-params-all-reduce: 64.19 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.69 | optimizer-unscale-and-check-inf: 19.49 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 101.51 | batch-generator: 8.43
 iteration    89500/  200000 | consumed samples:      2864000 | elapsed time per iteration (ms): 263.2 | learning rate: 1.876E-05 | global batch size:    32 | lm loss: 1.419039E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.37 | backward-params-all-reduce: 64.68 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.24 | optimizer-unscale-and-check-inf: 19.77 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 101.29 | batch-generator: 8.58
-------------------------------------------------------------------------------------------------
 validation loss at iteration 89500 | lm loss value: 3.178119E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 89500, match long value: 0.007566270709713628 | match short value: 0.011227960050864605 
-------------------------------------------------------------------------------------------------------
 iteration    89600/  200000 | consumed samples:      2867200 | elapsed time per iteration (ms): 348.8 | learning rate: 1.867E-05 | global batch size:    32 | lm loss: 1.397039E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.89 | backward-params-all-reduce: 64.05 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.25 | optimizer-unscale-and-check-inf: 18.20 | optimizer-clip-main-grad: 7.72 | optimizer-copy-main-to-model-params: 14.22 | optimizer: 101.56 | batch-generator: 81.40
 iteration    89700/  200000 | consumed samples:      2870400 | elapsed time per iteration (ms): 262.8 | learning rate: 1.858E-05 | global batch size:    32 | lm loss: 1.413533E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.09 | backward-params-all-reduce: 63.98 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.05 | optimizer-unscale-and-check-inf: 17.93 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.20 | batch-generator: 8.59
 iteration    89800/  200000 | consumed samples:      2873600 | elapsed time per iteration (ms): 260.3 | learning rate: 1.849E-05 | global batch size:    32 | lm loss: 1.401097E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.20 | backward-params-all-reduce: 64.30 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.03 | optimizer-unscale-and-check-inf: 18.66 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.54 | optimizer: 100.03 | batch-generator: 8.58
 iteration    89900/  200000 | consumed samples:      2876800 | elapsed time per iteration (ms): 258.9 | learning rate: 1.840E-05 | global batch size:    32 | lm loss: 1.390569E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.10 | backward-params-all-reduce: 63.44 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.85 | optimizer-unscale-and-check-inf: 16.82 | optimizer-clip-main-grad: 7.76 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.13 | batch-generator: 8.57
 iteration    90000/  200000 | consumed samples:      2880000 | elapsed time per iteration (ms): 259.1 | learning rate: 1.831E-05 | global batch size:    32 | lm loss: 1.414711E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.23 | backward-params-all-reduce: 64.17 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.11 | optimizer-unscale-and-check-inf: 16.75 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.13 | batch-generator: 8.65
-------------------------------------------------------------------------------------------------
 validation loss at iteration 90000 | lm loss value: 3.305470E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 90000, match long value: 0.002674887179975013 | match short value: -0.04925151348812065 
-------------------------------------------------------------------------------------------------------
 iteration    90100/  200000 | consumed samples:      2883200 | elapsed time per iteration (ms): 274.7 | learning rate: 1.822E-05 | global batch size:    32 | lm loss: 1.409888E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.35 | backward-params-all-reduce: 63.15 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.53 | optimizer-unscale-and-check-inf: 17.89 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.75 | batch-generator: 19.94
 iteration    90200/  200000 | consumed samples:      2886400 | elapsed time per iteration (ms): 259.1 | learning rate: 1.813E-05 | global batch size:    32 | lm loss: 1.402505E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.28 | backward-params-all-reduce: 63.03 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.03 | optimizer-unscale-and-check-inf: 17.18 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.48 | batch-generator: 8.48
 iteration    90300/  200000 | consumed samples:      2889600 | elapsed time per iteration (ms): 259.2 | learning rate: 1.804E-05 | global batch size:    32 | lm loss: 1.399434E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.27 | backward-params-all-reduce: 64.03 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.73 | optimizer-unscale-and-check-inf: 16.44 | optimizer-clip-main-grad: 7.53 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 98.35 | batch-generator: 8.49
 iteration    90400/  200000 | consumed samples:      2892800 | elapsed time per iteration (ms): 265.9 | learning rate: 1.795E-05 | global batch size:    32 | lm loss: 1.429855E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.03 | backward-params-all-reduce: 64.46 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.55 | optimizer-unscale-and-check-inf: 17.30 | optimizer-clip-main-grad: 7.43 | optimizer-copy-main-to-model-params: 14.02 | optimizer: 99.48 | batch-generator: 8.55
 iteration    90500/  200000 | consumed samples:      2896000 | elapsed time per iteration (ms): 261.5 | learning rate: 1.785E-05 | global batch size:    32 | lm loss: 1.405735E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.43 | backward-params-all-reduce: 64.68 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.47 | optimizer-unscale-and-check-inf: 18.02 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.71 | batch-generator: 8.52
-------------------------------------------------------------------------------------------------
 validation loss at iteration 90500 | lm loss value: 3.333165E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 90500, match long value: -0.0023466704337699114 | match short value: -0.030637500037221786 
----------------------------------------------------------------------------------------------------------
 iteration    90600/  200000 | consumed samples:      2899200 | elapsed time per iteration (ms): 270.2 | learning rate: 1.776E-05 | global batch size:    32 | lm loss: 1.396069E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.14 | backward-params-all-reduce: 64.29 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.53 | optimizer-unscale-and-check-inf: 16.76 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.50 | batch-generator: 14.76
 iteration    90700/  200000 | consumed samples:      2902400 | elapsed time per iteration (ms): 266.8 | learning rate: 1.767E-05 | global batch size:    32 | lm loss: 1.361072E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.45 | backward-params-all-reduce: 65.60 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.95 | optimizer-unscale-and-check-inf: 15.62 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 98.78 | batch-generator: 14.24
 iteration    90800/  200000 | consumed samples:      2905600 | elapsed time per iteration (ms): 256.8 | learning rate: 1.758E-05 | global batch size:    32 | lm loss: 1.327860E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.90 | backward-params-all-reduce: 62.75 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 14.03 | optimizer-unscale-and-check-inf: 15.98 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 99.28 | batch-generator: 8.44
 iteration    90900/  200000 | consumed samples:      2908800 | elapsed time per iteration (ms): 258.3 | learning rate: 1.749E-05 | global batch size:    32 | lm loss: 1.334406E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.30 | backward-params-all-reduce: 64.25 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.13 | optimizer-unscale-and-check-inf: 17.48 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.80 | batch-generator: 8.43
 iteration    91000/  200000 | consumed samples:      2912000 | elapsed time per iteration (ms): 259.7 | learning rate: 1.740E-05 | global batch size:    32 | lm loss: 1.322632E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.50 | backward-params-all-reduce: 65.31 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 14.03 | optimizer-unscale-and-check-inf: 16.51 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.80 | batch-generator: 8.65
-------------------------------------------------------------------------------------------------
 validation loss at iteration 91000 | lm loss value: 3.165581E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 91000, match long value: 0.0012483850739304497 | match short value: -0.06266460697631245 
--------------------------------------------------------------------------------------------------------
 iteration    91100/  200000 | consumed samples:      2915200 | elapsed time per iteration (ms): 277.5 | learning rate: 1.731E-05 | global batch size:    32 | lm loss: 1.322617E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.57 | backward-params-all-reduce: 63.76 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.23 | optimizer-unscale-and-check-inf: 17.43 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.41 | optimizer: 101.04 | batch-generator: 14.73
 iteration    91200/  200000 | consumed samples:      2918400 | elapsed time per iteration (ms): 257.7 | learning rate: 1.722E-05 | global batch size:    32 | lm loss: 1.345237E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.25 | backward-params-all-reduce: 63.82 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.35 | optimizer-unscale-and-check-inf: 17.34 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.95 | batch-generator: 8.46
 iteration    91300/  200000 | consumed samples:      2921600 | elapsed time per iteration (ms): 259.0 | learning rate: 1.713E-05 | global batch size:    32 | lm loss: 1.340361E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.17 | backward-params-all-reduce: 64.54 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.29 | optimizer-unscale-and-check-inf: 16.96 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.50 | batch-generator: 8.56
 iteration    91400/  200000 | consumed samples:      2924800 | elapsed time per iteration (ms): 257.8 | learning rate: 1.703E-05 | global batch size:    32 | lm loss: 1.345419E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.40 | backward-params-all-reduce: 63.44 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.33 | optimizer-unscale-and-check-inf: 17.63 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 100.19 | batch-generator: 8.49
 iteration    91500/  200000 | consumed samples:      2928000 | elapsed time per iteration (ms): 258.9 | learning rate: 1.694E-05 | global batch size:    32 | lm loss: 1.362418E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.11 | backward-params-all-reduce: 65.23 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.13 | optimizer-unscale-and-check-inf: 16.10 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 98.50 | batch-generator: 8.53
-------------------------------------------------------------------------------------------------
 validation loss at iteration 91500 | lm loss value: 3.268477E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 91500, match long value: 0.000834363461828037 | match short value: -0.010247330242310037 
--------------------------------------------------------------------------------------------------------
 iteration    91600/  200000 | consumed samples:      2931200 | elapsed time per iteration (ms): 270.1 | learning rate: 1.686E-05 | global batch size:    32 | lm loss: 1.342512E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   4 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.25 | backward-params-all-reduce: 63.98 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.96 | optimizer-unscale-and-check-inf: 17.20 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.02 | optimizer: 96.67 | batch-generator: 19.86
 iteration    91700/  200000 | consumed samples:      2934400 | elapsed time per iteration (ms): 257.8 | learning rate: 1.677E-05 | global batch size:    32 | lm loss: 1.358289E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.04 | backward-params-all-reduce: 64.19 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.38 | optimizer-unscale-and-check-inf: 16.49 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.56 | optimizer: 99.22 | batch-generator: 8.57
 iteration    91800/  200000 | consumed samples:      2937600 | elapsed time per iteration (ms): 263.8 | learning rate: 1.667E-05 | global batch size:    32 | lm loss: 1.346010E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.45 | backward-params-all-reduce: 63.62 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.04 | optimizer-unscale-and-check-inf: 16.05 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 14.27 | optimizer: 100.07 | batch-generator: 8.51
 iteration    91900/  200000 | consumed samples:      2940800 | elapsed time per iteration (ms): 259.8 | learning rate: 1.658E-05 | global batch size:    32 | lm loss: 1.358130E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.39 | backward-params-all-reduce: 63.65 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.50 | optimizer-unscale-and-check-inf: 17.21 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 100.02 | batch-generator: 8.51
 iteration    92000/  200000 | consumed samples:      2944000 | elapsed time per iteration (ms): 255.8 | learning rate: 1.649E-05 | global batch size:    32 | lm loss: 1.357473E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.23 | backward-params-all-reduce: 62.80 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.36 | optimizer-unscale-and-check-inf: 16.07 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 98.69 | batch-generator: 8.55
-------------------------------------------------------------------------------------------------
 validation loss at iteration 92000 | lm loss value: 3.276440E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 92000, match long value: 0.0011179381827608448 | match short value: -0.021305961812555355 
---------------------------------------------------------------------------------------------------------
 iteration    92100/  200000 | consumed samples:      2947200 | elapsed time per iteration (ms): 267.6 | learning rate: 1.640E-05 | global batch size:    32 | lm loss: 1.363758E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.10 | backward-params-all-reduce: 63.71 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.85 | optimizer-unscale-and-check-inf: 16.38 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.54 | batch-generator: 14.20
 iteration    92200/  200000 | consumed samples:      2950400 | elapsed time per iteration (ms): 257.4 | learning rate: 1.631E-05 | global batch size:    32 | lm loss: 1.338309E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.39 | backward-params-all-reduce: 62.17 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.50 | optimizer-unscale-and-check-inf: 16.98 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.75 | batch-generator: 8.54
 iteration    92300/  200000 | consumed samples:      2953600 | elapsed time per iteration (ms): 258.7 | learning rate: 1.622E-05 | global batch size:    32 | lm loss: 1.378083E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.08 | backward-params-all-reduce: 63.36 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.39 | optimizer-unscale-and-check-inf: 16.75 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.36 | batch-generator: 8.48
 iteration    92400/  200000 | consumed samples:      2956800 | elapsed time per iteration (ms): 258.0 | learning rate: 1.613E-05 | global batch size:    32 | lm loss: 1.364478E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.19 | backward-params-all-reduce: 63.32 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.35 | optimizer-unscale-and-check-inf: 15.91 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 97.39 | batch-generator: 8.46
 iteration    92500/  200000 | consumed samples:      2960000 | elapsed time per iteration (ms): 259.9 | learning rate: 1.604E-05 | global batch size:    32 | lm loss: 1.382854E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.19 | backward-params-all-reduce: 63.88 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.13 | optimizer-unscale-and-check-inf: 16.46 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 97.85 | batch-generator: 8.52
-------------------------------------------------------------------------------------------------
 validation loss at iteration 92500 | lm loss value: 3.143280E-05 | lm loss PPL: 1.000031E+00 | 
 at iteration 92500, match long value: 0.0027421774442575567 | match short value: -0.03677342753193056 
--------------------------------------------------------------------------------------------------------
 iteration    92600/  200000 | consumed samples:      2963200 | elapsed time per iteration (ms): 280.2 | learning rate: 1.595E-05 | global batch size:    32 | lm loss: 1.362254E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.81 | backward-params-all-reduce: 64.49 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.38 | optimizer-unscale-and-check-inf: 15.83 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 14.46 | optimizer: 99.45 | batch-generator: 16.61
 iteration    92700/  200000 | consumed samples:      2966400 | elapsed time per iteration (ms): 259.7 | learning rate: 1.585E-05 | global batch size:    32 | lm loss: 1.370559E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.72 | backward-params-all-reduce: 63.41 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.56 | optimizer-unscale-and-check-inf: 16.82 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.68 | batch-generator: 8.47
 iteration    92800/  200000 | consumed samples:      2969600 | elapsed time per iteration (ms): 260.6 | learning rate: 1.576E-05 | global batch size:    32 | lm loss: 1.375185E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.17 | backward-params-all-reduce: 64.21 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.36 | optimizer-unscale-and-check-inf: 17.09 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.72 | batch-generator: 8.47
 iteration    92900/  200000 | consumed samples:      2972800 | elapsed time per iteration (ms): 259.1 | learning rate: 1.567E-05 | global batch size:    32 | lm loss: 1.366939E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.01 | backward-params-all-reduce: 63.56 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.68 | optimizer-unscale-and-check-inf: 16.24 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.23 | batch-generator: 8.55
 iteration    93000/  200000 | consumed samples:      2976000 | elapsed time per iteration (ms): 258.2 | learning rate: 1.558E-05 | global batch size:    32 | lm loss: 1.378624E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.31 | backward-params-all-reduce: 64.03 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.22 | optimizer-unscale-and-check-inf: 15.98 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 97.44 | batch-generator: 8.64
-------------------------------------------------------------------------------------------------
 validation loss at iteration 93000 | lm loss value: 3.314384E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 93000, match long value: -0.0004891586201313798 | match short value: -0.016564891716106218 
----------------------------------------------------------------------------------------------------------
 iteration    93100/  200000 | consumed samples:      2979200 | elapsed time per iteration (ms): 275.2 | learning rate: 1.549E-05 | global batch size:    32 | lm loss: 1.369348E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.42 | backward-params-all-reduce: 63.72 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.64 | optimizer-unscale-and-check-inf: 16.62 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.54 | batch-generator: 16.31
 iteration    93200/  200000 | consumed samples:      2982400 | elapsed time per iteration (ms): 261.2 | learning rate: 1.540E-05 | global batch size:    32 | lm loss: 1.371874E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.48 | backward-params-all-reduce: 64.90 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.48 | optimizer-unscale-and-check-inf: 16.36 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 98.17 | batch-generator: 8.68
 iteration    93300/  200000 | consumed samples:      2985600 | elapsed time per iteration (ms): 271.5 | learning rate: 1.531E-05 | global batch size:    32 | lm loss: 1.375430E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.82 | backward-params-all-reduce: 63.52 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.27 | optimizer-unscale-and-check-inf: 16.88 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 14.15 | optimizer: 100.10 | batch-generator: 8.72
 iteration    93400/  200000 | consumed samples:      2988800 | elapsed time per iteration (ms): 260.1 | learning rate: 1.522E-05 | global batch size:    32 | lm loss: 1.377629E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.37 | backward-params-all-reduce: 63.64 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.48 | optimizer-unscale-and-check-inf: 16.06 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 97.80 | batch-generator: 8.72
 iteration    93500/  200000 | consumed samples:      2992000 | elapsed time per iteration (ms): 258.7 | learning rate: 1.513E-05 | global batch size:    32 | lm loss: 1.369208E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.19 | backward-params-all-reduce: 63.67 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.21 | optimizer-unscale-and-check-inf: 17.19 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 98.71 | batch-generator: 8.65
-------------------------------------------------------------------------------------------------
 validation loss at iteration 93500 | lm loss value: 3.303885E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 93500, match long value: 0.0037909060795870116 | match short value: -0.06502206422852753 
--------------------------------------------------------------------------------------------------------
 iteration    93600/  200000 | consumed samples:      2995200 | elapsed time per iteration (ms): 277.6 | learning rate: 1.503E-05 | global batch size:    32 | lm loss: 1.382594E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.80 | backward-params-all-reduce: 63.35 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.80 | optimizer-unscale-and-check-inf: 16.36 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.47 | batch-generator: 18.50
 iteration    93700/  200000 | consumed samples:      2998400 | elapsed time per iteration (ms): 259.3 | learning rate: 1.494E-05 | global batch size:    32 | lm loss: 1.371921E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.46 | backward-params-all-reduce: 63.59 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.59 | optimizer-unscale-and-check-inf: 16.56 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.36 | batch-generator: 8.48
 iteration    93800/  200000 | consumed samples:      3001600 | elapsed time per iteration (ms): 264.9 | learning rate: 1.485E-05 | global batch size:    32 | lm loss: 1.350406E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.09 | backward-params-all-reduce: 63.73 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.42 | optimizer-unscale-and-check-inf: 17.03 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.66 | batch-generator: 14.29
 iteration    93900/  200000 | consumed samples:      3004800 | elapsed time per iteration (ms): 261.8 | learning rate: 1.476E-05 | global batch size:    32 | lm loss: 1.312964E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.16 | backward-params-all-reduce: 64.59 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.73 | optimizer-unscale-and-check-inf: 17.80 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.56 | optimizer: 99.88 | batch-generator: 8.54
 iteration    94000/  200000 | consumed samples:      3008000 | elapsed time per iteration (ms): 270.5 | learning rate: 1.467E-05 | global batch size:    32 | lm loss: 1.306763E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.18 | backward-params-all-reduce: 64.47 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.71 | optimizer-unscale-and-check-inf: 16.69 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 14.27 | optimizer: 100.41 | batch-generator: 8.64
-------------------------------------------------------------------------------------------------
 validation loss at iteration 94000 | lm loss value: 3.243141E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 94000, match long value: 0.002093943232194628 | match short value: -0.022870167898028627 
--------------------------------------------------------------------------------------------------------
 iteration    94100/  200000 | consumed samples:      3011200 | elapsed time per iteration (ms): 277.0 | learning rate: 1.458E-05 | global batch size:    32 | lm loss: 1.285491E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.44 | backward-params-all-reduce: 66.47 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.35 | optimizer-unscale-and-check-inf: 16.08 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 97.72 | batch-generator: 15.33
 iteration    94200/  200000 | consumed samples:      3014400 | elapsed time per iteration (ms): 260.7 | learning rate: 1.449E-05 | global batch size:    32 | lm loss: 1.321644E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.22 | backward-params-all-reduce: 63.99 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.66 | optimizer-unscale-and-check-inf: 16.44 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.54 | optimizer: 98.50 | batch-generator: 8.54
 iteration    94300/  200000 | consumed samples:      3017600 | elapsed time per iteration (ms): 256.3 | learning rate: 1.440E-05 | global batch size:    32 | lm loss: 1.319265E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.45 | backward-params-all-reduce: 62.52 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.53 | optimizer-unscale-and-check-inf: 15.53 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 97.31 | batch-generator: 8.60
 iteration    94400/  200000 | consumed samples:      3020800 | elapsed time per iteration (ms): 259.4 | learning rate: 1.431E-05 | global batch size:    32 | lm loss: 1.313851E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.30 | backward-params-all-reduce: 64.65 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.44 | optimizer-unscale-and-check-inf: 16.21 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 97.96 | batch-generator: 8.50
 iteration    94500/  200000 | consumed samples:      3024000 | elapsed time per iteration (ms): 258.8 | learning rate: 1.421E-05 | global batch size:    32 | lm loss: 1.303485E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.25 | backward-params-all-reduce: 63.71 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.36 | optimizer-unscale-and-check-inf: 15.66 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 97.23 | batch-generator: 8.54
-------------------------------------------------------------------------------------------------
 validation loss at iteration 94500 | lm loss value: 3.208848E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 94500, match long value: 0.00435340522791424 | match short value: -0.04853763082439841 
------------------------------------------------------------------------------------------------------
 iteration    94600/  200000 | consumed samples:      3027200 | elapsed time per iteration (ms): 272.2 | learning rate: 1.412E-05 | global batch size:    32 | lm loss: 1.328143E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.39 | backward-params-all-reduce: 63.87 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.84 | optimizer-unscale-and-check-inf: 16.02 | optimizer-clip-main-grad: 7.51 | optimizer-copy-main-to-model-params: 12.36 | optimizer: 97.41 | batch-generator: 16.78
 iteration    94700/  200000 | consumed samples:      3030400 | elapsed time per iteration (ms): 261.7 | learning rate: 1.403E-05 | global batch size:    32 | lm loss: 1.329060E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.52 | backward-params-all-reduce: 63.65 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.11 | optimizer-unscale-and-check-inf: 18.34 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 99.64 | batch-generator: 8.55
 iteration    94800/  200000 | consumed samples:      3033600 | elapsed time per iteration (ms): 272.3 | learning rate: 1.394E-05 | global batch size:    32 | lm loss: 1.327691E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.46 | backward-params-all-reduce: 65.95 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.22 | optimizer-unscale-and-check-inf: 16.44 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 14.22 | optimizer: 99.58 | batch-generator: 8.54
 iteration    94900/  200000 | consumed samples:      3036800 | elapsed time per iteration (ms): 260.2 | learning rate: 1.385E-05 | global batch size:    32 | lm loss: 1.326200E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.42 | backward-params-all-reduce: 63.19 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.12 | optimizer-unscale-and-check-inf: 16.63 | optimizer-clip-main-grad: 7.82 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.23 | batch-generator: 8.63
 iteration    95000/  200000 | consumed samples:      3040000 | elapsed time per iteration (ms): 259.9 | learning rate: 1.376E-05 | global batch size:    32 | lm loss: 1.328410E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.39 | backward-params-all-reduce: 64.22 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.51 | optimizer-unscale-and-check-inf: 16.62 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.34 | batch-generator: 8.50
-------------------------------------------------------------------------------------------------
 validation loss at iteration 95000 | lm loss value: 3.172026E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 95000, match long value: -0.0015712622563568716 | match short value: -0.011683723341691633 
----------------------------------------------------------------------------------------------------------
 iteration    95100/  200000 | consumed samples:      3043200 | elapsed time per iteration (ms): 279.5 | learning rate: 1.367E-05 | global batch size:    32 | lm loss: 1.317586E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.31 | backward-params-all-reduce: 63.72 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.84 | optimizer-unscale-and-check-inf: 16.30 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 98.44 | batch-generator: 19.70
 iteration    95200/  200000 | consumed samples:      3046400 | elapsed time per iteration (ms): 258.7 | learning rate: 1.358E-05 | global batch size:    32 | lm loss: 1.333104E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.14 | backward-params-all-reduce: 63.77 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.33 | optimizer-unscale-and-check-inf: 16.32 | optimizer-clip-main-grad: 7.42 | optimizer-copy-main-to-model-params: 12.26 | optimizer: 96.50 | batch-generator: 8.49
 iteration    95300/  200000 | consumed samples:      3049600 | elapsed time per iteration (ms): 263.4 | learning rate: 1.349E-05 | global batch size:    32 | lm loss: 1.333647E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.27 | backward-params-all-reduce: 65.16 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.63 | optimizer-unscale-and-check-inf: 16.34 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.27 | batch-generator: 8.51
 iteration    95400/  200000 | consumed samples:      3052800 | elapsed time per iteration (ms): 258.3 | learning rate: 1.340E-05 | global batch size:    32 | lm loss: 1.345802E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.06 | backward-params-all-reduce: 63.47 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.12 | optimizer-unscale-and-check-inf: 17.32 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.76 | batch-generator: 8.49
 iteration    95500/  200000 | consumed samples:      3056000 | elapsed time per iteration (ms): 270.7 | learning rate: 1.331E-05 | global batch size:    32 | lm loss: 1.350127E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.78 | backward-params-all-reduce: 63.24 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.77 | optimizer-unscale-and-check-inf: 16.09 | optimizer-clip-main-grad: 7.73 | optimizer-copy-main-to-model-params: 14.38 | optimizer: 100.14 | batch-generator: 8.66
-------------------------------------------------------------------------------------------------
 validation loss at iteration 95500 | lm loss value: 3.306009E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 95500, match long value: 0.004654622859377643 | match short value: -0.03863386037221035 
-------------------------------------------------------------------------------------------------------
 iteration    95600/  200000 | consumed samples:      3059200 | elapsed time per iteration (ms): 271.9 | learning rate: 1.322E-05 | global batch size:    32 | lm loss: 1.348082E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.98 | backward-params-all-reduce: 64.06 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.34 | optimizer-unscale-and-check-inf: 17.31 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.54 | optimizer: 99.02 | batch-generator: 15.27
 iteration    95700/  200000 | consumed samples:      3062400 | elapsed time per iteration (ms): 259.4 | learning rate: 1.312E-05 | global batch size:    32 | lm loss: 1.341784E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.29 | backward-params-all-reduce: 63.46 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.38 | optimizer-unscale-and-check-inf: 16.00 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 97.62 | batch-generator: 8.52
 iteration    95800/  200000 | consumed samples:      3065600 | elapsed time per iteration (ms): 261.0 | learning rate: 1.303E-05 | global batch size:    32 | lm loss: 1.342463E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.38 | backward-params-all-reduce: 65.08 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.61 | optimizer-unscale-and-check-inf: 16.58 | optimizer-clip-main-grad: 7.77 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 98.64 | batch-generator: 8.70
 iteration    95900/  200000 | consumed samples:      3068800 | elapsed time per iteration (ms): 261.9 | learning rate: 1.294E-05 | global batch size:    32 | lm loss: 1.341485E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.90 | backward-params-all-reduce: 64.81 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.47 | optimizer-unscale-and-check-inf: 16.39 | optimizer-clip-main-grad: 7.74 | optimizer-copy-main-to-model-params: 12.54 | optimizer: 98.30 | batch-generator: 8.69
 iteration    96000/  200000 | consumed samples:      3072000 | elapsed time per iteration (ms): 260.8 | learning rate: 1.285E-05 | global batch size:    32 | lm loss: 1.345103E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.33 | backward-params-all-reduce: 64.09 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.28 | optimizer-unscale-and-check-inf: 17.08 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.59 | batch-generator: 8.57
-------------------------------------------------------------------------------------------------
 validation loss at iteration 96000 | lm loss value: 3.185646E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 96000, match long value: 0.0009998188936811178 | match short value: -0.07604732479289597 
--------------------------------------------------------------------------------------------------------
 iteration    96100/  200000 | consumed samples:      3075200 | elapsed time per iteration (ms): 274.5 | learning rate: 1.276E-05 | global batch size:    32 | lm loss: 1.341692E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.40 | backward-params-all-reduce: 65.23 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.26 | optimizer-unscale-and-check-inf: 16.00 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 97.56 | batch-generator: 17.39
 iteration    96200/  200000 | consumed samples:      3078400 | elapsed time per iteration (ms): 270.0 | learning rate: 1.267E-05 | global batch size:    32 | lm loss: 1.370889E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.57 | backward-params-all-reduce: 64.37 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.83 | optimizer-unscale-and-check-inf: 16.06 | optimizer-clip-main-grad: 7.74 | optimizer-copy-main-to-model-params: 14.27 | optimizer: 100.10 | batch-generator: 8.60
 iteration    96300/  200000 | consumed samples:      3081600 | elapsed time per iteration (ms): 261.3 | learning rate: 1.258E-05 | global batch size:    32 | lm loss: 1.351955E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.23 | backward-params-all-reduce: 64.13 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.17 | optimizer-unscale-and-check-inf: 16.30 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.55 | optimizer: 97.80 | batch-generator: 8.54
 iteration    96400/  200000 | consumed samples:      3084800 | elapsed time per iteration (ms): 258.3 | learning rate: 1.249E-05 | global batch size:    32 | lm loss: 1.353477E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.10 | backward-params-all-reduce: 63.31 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 11.92 | optimizer-unscale-and-check-inf: 16.34 | optimizer-clip-main-grad: 7.54 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 97.47 | batch-generator: 8.43
 iteration    96500/  200000 | consumed samples:      3088000 | elapsed time per iteration (ms): 260.7 | learning rate: 1.240E-05 | global batch size:    32 | lm loss: 1.342731E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.10 | backward-params-all-reduce: 63.68 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.05 | optimizer-unscale-and-check-inf: 16.06 | optimizer-clip-main-grad: 7.42 | optimizer-copy-main-to-model-params: 12.25 | optimizer: 95.94 | batch-generator: 8.46
-------------------------------------------------------------------------------------------------
 validation loss at iteration 96500 | lm loss value: 3.271094E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 96500, match long value: 0.0014042355699084152 | match short value: -0.06290844868556274 
--------------------------------------------------------------------------------------------------------
 iteration    96600/  200000 | consumed samples:      3091200 | elapsed time per iteration (ms): 277.3 | learning rate: 1.231E-05 | global batch size:    32 | lm loss: 1.363274E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.27 | backward-params-all-reduce: 63.81 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.19 | optimizer-unscale-and-check-inf: 16.40 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 97.76 | batch-generator: 19.12
 iteration    96700/  200000 | consumed samples:      3094400 | elapsed time per iteration (ms): 258.7 | learning rate: 1.222E-05 | global batch size:    32 | lm loss: 1.362763E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.06 | backward-params-all-reduce: 63.92 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.03 | optimizer-unscale-and-check-inf: 16.40 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 97.64 | batch-generator: 8.56
 iteration    96800/  200000 | consumed samples:      3097600 | elapsed time per iteration (ms): 259.1 | learning rate: 1.212E-05 | global batch size:    32 | lm loss: 1.356725E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.31 | backward-params-all-reduce: 63.87 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.21 | optimizer-unscale-and-check-inf: 15.59 | optimizer-clip-main-grad: 7.54 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 97.01 | batch-generator: 8.55
 iteration    96900/  200000 | consumed samples:      3100800 | elapsed time per iteration (ms): 271.7 | learning rate: 1.203E-05 | global batch size:    32 | lm loss: 1.331273E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.52 | backward-params-all-reduce: 63.43 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.30 | optimizer-unscale-and-check-inf: 16.57 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 14.21 | optimizer: 99.81 | batch-generator: 13.95
 iteration    97000/  200000 | consumed samples:      3104000 | elapsed time per iteration (ms): 269.7 | learning rate: 1.194E-05 | global batch size:    32 | lm loss: 1.304543E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.81 | backward-params-all-reduce: 64.41 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.63 | optimizer-unscale-and-check-inf: 15.83 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 97.66 | batch-generator: 8.65
-------------------------------------------------------------------------------------------------
 validation loss at iteration 97000 | lm loss value: 3.164019E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 97000, match long value: 0.005685608420105921 | match short value: -0.031163809552567968 
--------------------------------------------------------------------------------------------------------
 iteration    97100/  200000 | consumed samples:      3107200 | elapsed time per iteration (ms): 277.7 | learning rate: 1.185E-05 | global batch size:    32 | lm loss: 1.305344E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.17 | backward-params-all-reduce: 64.85 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.04 | optimizer-unscale-and-check-inf: 15.76 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.03 | batch-generator: 16.42
 iteration    97200/  200000 | consumed samples:      3110400 | elapsed time per iteration (ms): 260.7 | learning rate: 1.176E-05 | global batch size:    32 | lm loss: 1.313159E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.45 | backward-params-all-reduce: 63.46 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.62 | optimizer-unscale-and-check-inf: 15.54 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 98.42 | batch-generator: 8.58
 iteration    97300/  200000 | consumed samples:      3113600 | elapsed time per iteration (ms): 257.7 | learning rate: 1.167E-05 | global batch size:    32 | lm loss: 1.295754E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.26 | backward-params-all-reduce: 63.12 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.78 | optimizer-unscale-and-check-inf: 15.37 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 97.40 | batch-generator: 8.66
 iteration    97400/  200000 | consumed samples:      3116800 | elapsed time per iteration (ms): 257.9 | learning rate: 1.158E-05 | global batch size:    32 | lm loss: 1.283193E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.26 | backward-params-all-reduce: 63.84 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.71 | optimizer-unscale-and-check-inf: 14.89 | optimizer-clip-main-grad: 7.51 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 96.73 | batch-generator: 8.54
 iteration    97500/  200000 | consumed samples:      3120000 | elapsed time per iteration (ms): 259.6 | learning rate: 1.149E-05 | global batch size:    32 | lm loss: 1.287127E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.25 | backward-params-all-reduce: 64.21 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.81 | optimizer-unscale-and-check-inf: 16.87 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.02 | batch-generator: 8.58
-------------------------------------------------------------------------------------------------
 validation loss at iteration 97500 | lm loss value: 3.326693E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 97500, match long value: -0.001961104970702249 | match short value: -0.02074455549410798 
--------------------------------------------------------------------------------------------------------
 iteration    97600/  200000 | consumed samples:      3123200 | elapsed time per iteration (ms): 276.3 | learning rate: 1.140E-05 | global batch size:    32 | lm loss: 1.297465E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.27 | backward-params-all-reduce: 63.90 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.75 | optimizer-unscale-and-check-inf: 15.14 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 97.13 | batch-generator: 16.79
 iteration    97700/  200000 | consumed samples:      3126400 | elapsed time per iteration (ms): 274.6 | learning rate: 1.130E-05 | global batch size:    32 | lm loss: 1.305673E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.33 | backward-params-all-reduce: 64.21 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.70 | optimizer-unscale-and-check-inf: 16.09 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 14.23 | optimizer: 99.76 | batch-generator: 8.50
 iteration    97800/  200000 | consumed samples:      3129600 | elapsed time per iteration (ms): 263.9 | learning rate: 1.121E-05 | global batch size:    32 | lm loss: 1.306831E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.28 | backward-params-all-reduce: 65.75 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.50 | optimizer-unscale-and-check-inf: 20.65 | optimizer-clip-main-grad: 7.75 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 102.56 | batch-generator: 8.60
 iteration    97900/  200000 | consumed samples:      3132800 | elapsed time per iteration (ms): 262.3 | learning rate: 1.112E-05 | global batch size:    32 | lm loss: 1.307747E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.04 | backward-params-all-reduce: 64.72 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.45 | optimizer-unscale-and-check-inf: 19.76 | optimizer-clip-main-grad: 7.75 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 101.60 | batch-generator: 8.49
 iteration    98000/  200000 | consumed samples:      3136000 | elapsed time per iteration (ms): 261.1 | learning rate: 1.103E-05 | global batch size:    32 | lm loss: 1.320754E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.27 | backward-params-all-reduce: 64.15 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 11.95 | optimizer-unscale-and-check-inf: 19.75 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 101.03 | batch-generator: 8.49
-------------------------------------------------------------------------------------------------
 validation loss at iteration 98000 | lm loss value: 3.199914E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 98000, match long value: -0.000673985572982981 | match short value: -0.04366274558808876 
--------------------------------------------------------------------------------------------------------
 iteration    98100/  200000 | consumed samples:      3139200 | elapsed time per iteration (ms): 344.0 | learning rate: 1.094E-05 | global batch size:    32 | lm loss: 1.330210E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.20 | backward-params-all-reduce: 64.38 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.62 | optimizer-unscale-and-check-inf: 18.96 | optimizer-clip-main-grad: 7.70 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 100.98 | batch-generator: 85.57
 iteration    98200/  200000 | consumed samples:      3142400 | elapsed time per iteration (ms): 263.6 | learning rate: 1.085E-05 | global batch size:    32 | lm loss: 1.314115E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.74 | backward-params-all-reduce: 64.77 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.04 | optimizer-unscale-and-check-inf: 20.00 | optimizer-clip-main-grad: 7.73 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 101.45 | batch-generator: 8.58
 iteration    98300/  200000 | consumed samples:      3145600 | elapsed time per iteration (ms): 261.6 | learning rate: 1.076E-05 | global batch size:    32 | lm loss: 1.341839E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.34 | backward-params-all-reduce: 63.72 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.14 | optimizer-unscale-and-check-inf: 20.27 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 101.71 | batch-generator: 8.46
 iteration    98400/  200000 | consumed samples:      3148800 | elapsed time per iteration (ms): 271.8 | learning rate: 1.067E-05 | global batch size:    32 | lm loss: 1.304024E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.25 | backward-params-all-reduce: 63.65 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.19 | optimizer-unscale-and-check-inf: 19.21 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 14.32 | optimizer: 102.52 | batch-generator: 8.49
 iteration    98500/  200000 | consumed samples:      3152000 | elapsed time per iteration (ms): 261.1 | learning rate: 1.058E-05 | global batch size:    32 | lm loss: 1.307332E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.39 | backward-params-all-reduce: 63.16 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.58 | optimizer-unscale-and-check-inf: 18.42 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 100.28 | batch-generator: 8.50
-------------------------------------------------------------------------------------------------
 validation loss at iteration 98500 | lm loss value: 3.126300E-05 | lm loss PPL: 1.000031E+00 | 
 at iteration 98500, match long value: 0.0036005159219212875 | match short value: -0.01991711854907784 
--------------------------------------------------------------------------------------------------------
 iteration    98600/  200000 | consumed samples:      3155200 | elapsed time per iteration (ms): 272.1 | learning rate: 1.048E-05 | global batch size:    32 | lm loss: 1.328465E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.97 | backward-params-all-reduce: 64.61 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.67 | optimizer-unscale-and-check-inf: 18.37 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 100.34 | batch-generator: 13.30
 iteration    98700/  200000 | consumed samples:      3158400 | elapsed time per iteration (ms): 261.0 | learning rate: 1.039E-05 | global batch size:    32 | lm loss: 1.327464E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.04 | backward-params-all-reduce: 64.89 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.30 | optimizer-unscale-and-check-inf: 19.85 | optimizer-clip-main-grad: 7.70 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 101.55 | batch-generator: 8.51
 iteration    98800/  200000 | consumed samples:      3161600 | elapsed time per iteration (ms): 258.9 | learning rate: 1.030E-05 | global batch size:    32 | lm loss: 1.328734E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.16 | backward-params-all-reduce: 63.38 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.41 | optimizer-unscale-and-check-inf: 18.15 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 99.77 | batch-generator: 8.49
 iteration    98900/  200000 | consumed samples:      3164800 | elapsed time per iteration (ms): 260.2 | learning rate: 1.021E-05 | global batch size:    32 | lm loss: 1.330378E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.29 | backward-params-all-reduce: 63.62 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.82 | optimizer-unscale-and-check-inf: 16.62 | optimizer-clip-main-grad: 7.75 | optimizer-copy-main-to-model-params: 12.39 | optimizer: 99.24 | batch-generator: 8.62
 iteration    99000/  200000 | consumed samples:      3168000 | elapsed time per iteration (ms): 263.6 | learning rate: 1.012E-05 | global batch size:    32 | lm loss: 1.338589E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.20 | backward-params-all-reduce: 66.62 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.84 | optimizer-unscale-and-check-inf: 16.99 | optimizer-clip-main-grad: 7.87 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 100.43 | batch-generator: 8.68
-------------------------------------------------------------------------------------------------
 validation loss at iteration 99000 | lm loss value: 3.230281E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 99000, match long value: 0.0026960035574546183 | match short value: -0.007510742621642861 
---------------------------------------------------------------------------------------------------------
 iteration    99100/  200000 | consumed samples:      3171200 | elapsed time per iteration (ms): 274.7 | learning rate: 1.003E-05 | global batch size:    32 | lm loss: 1.313564E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.39 | backward-params-all-reduce: 64.90 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.64 | optimizer-unscale-and-check-inf: 17.09 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 14.25 | optimizer: 100.71 | batch-generator: 15.31
 iteration    99200/  200000 | consumed samples:      3174400 | elapsed time per iteration (ms): 265.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.328619E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.47 | backward-params-all-reduce: 65.38 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.19 | optimizer-unscale-and-check-inf: 16.65 | optimizer-clip-main-grad: 7.73 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 99.20 | batch-generator: 8.58
 iteration    99300/  200000 | consumed samples:      3177600 | elapsed time per iteration (ms): 258.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.307379E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.48 | backward-params-all-reduce: 64.20 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 12.62 | optimizer-unscale-and-check-inf: 17.34 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.31 | batch-generator: 8.54
 iteration    99400/  200000 | consumed samples:      3180800 | elapsed time per iteration (ms): 261.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.325023E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.35 | backward-params-all-reduce: 63.93 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.51 | optimizer-unscale-and-check-inf: 18.40 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 100.29 | batch-generator: 8.57
 iteration    99500/  200000 | consumed samples:      3184000 | elapsed time per iteration (ms): 259.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.331627E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.10 | backward-params-all-reduce: 62.42 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.74 | optimizer-unscale-and-check-inf: 18.13 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 100.16 | batch-generator: 8.56
-------------------------------------------------------------------------------------------------
 validation loss at iteration 99500 | lm loss value: 3.320519E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 99500, match long value: 0.0024522505382031656 | match short value: -0.05236089534372872 
--------------------------------------------------------------------------------------------------------
 iteration    99600/  200000 | consumed samples:      3187200 | elapsed time per iteration (ms): 276.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.332578E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.22 | backward-params-all-reduce: 63.67 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.66 | optimizer-unscale-and-check-inf: 16.50 | optimizer-clip-main-grad: 7.79 | optimizer-copy-main-to-model-params: 12.55 | optimizer: 99.72 | batch-generator: 20.78
 iteration    99700/  200000 | consumed samples:      3190400 | elapsed time per iteration (ms): 260.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.321546E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.26 | backward-params-all-reduce: 63.84 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.83 | optimizer-unscale-and-check-inf: 17.51 | optimizer-clip-main-grad: 7.75 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.77 | batch-generator: 8.59
 iteration    99800/  200000 | consumed samples:      3193600 | elapsed time per iteration (ms): 258.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.346510E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.16 | backward-params-all-reduce: 63.08 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.77 | optimizer-unscale-and-check-inf: 18.13 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 100.08 | batch-generator: 8.49
 iteration    99900/  200000 | consumed samples:      3196800 | elapsed time per iteration (ms): 266.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.326507E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.79 | backward-params-all-reduce: 62.64 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.15 | optimizer-unscale-and-check-inf: 16.03 | optimizer-clip-main-grad: 7.49 | optimizer-copy-main-to-model-params: 14.08 | optimizer: 98.92 | batch-generator: 8.61
 iteration   100000/  200000 | consumed samples:      3200000 | elapsed time per iteration (ms): 257.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.343776E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.12 | backward-params-all-reduce: 63.48 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.09 | optimizer-unscale-and-check-inf: 16.95 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.30 | batch-generator: 8.50
--------------------------------------------------------------------------------------------------
 validation loss at iteration 100000 | lm loss value: 3.173686E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 100000, match long value: 0.00023520760121087763 | match short value: -0.0450245444071505 
---------------------------------------------------------------------------------------------------------
saving checkpoint at iteration  100000 to verify1
  successfully saved checkpoint at iteration  100000 to verify1
time (ms) | save-checkpoint: 22252.31
 iteration   100100/  200000 | consumed samples:      3203200 | elapsed time per iteration (ms): 502.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.302470E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.62 | backward-params-all-reduce: 65.32 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.67 | optimizer-unscale-and-check-inf: 17.93 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.89 | batch-generator: 19.35
 iteration   100200/  200000 | consumed samples:      3206400 | elapsed time per iteration (ms): 259.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.292432E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.30 | backward-params-all-reduce: 64.57 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.69 | optimizer-unscale-and-check-inf: 16.89 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.88 | batch-generator: 8.52
 iteration   100300/  200000 | consumed samples:      3209600 | elapsed time per iteration (ms): 259.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.275902E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.22 | backward-params-all-reduce: 65.41 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.83 | optimizer-unscale-and-check-inf: 16.75 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.91 | batch-generator: 8.55
 iteration   100400/  200000 | consumed samples:      3212800 | elapsed time per iteration (ms): 259.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.278028E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.58 | backward-params-all-reduce: 65.13 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.38 | optimizer-unscale-and-check-inf: 16.38 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.06 | batch-generator: 8.56
 iteration   100500/  200000 | consumed samples:      3216000 | elapsed time per iteration (ms): 259.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.285568E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.20 | backward-params-all-reduce: 64.60 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.85 | optimizer-unscale-and-check-inf: 15.10 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.24 | batch-generator: 8.53
--------------------------------------------------------------------------------------------------
 validation loss at iteration 100500 | lm loss value: 3.208215E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 100500, match long value: 0.0012715080126487316 | match short value: -0.05220944451897617 
---------------------------------------------------------------------------------------------------------
 iteration   100600/  200000 | consumed samples:      3219200 | elapsed time per iteration (ms): 280.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.277689E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.78 | backward-params-all-reduce: 64.85 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.26 | optimizer-unscale-and-check-inf: 15.96 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 14.26 | optimizer: 100.26 | batch-generator: 14.18
 iteration   100700/  200000 | consumed samples:      3222400 | elapsed time per iteration (ms): 260.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.296174E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.39 | backward-params-all-reduce: 64.12 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 14.03 | optimizer-unscale-and-check-inf: 16.21 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.50 | batch-generator: 8.54
 iteration   100800/  200000 | consumed samples:      3225600 | elapsed time per iteration (ms): 258.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.292238E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.15 | backward-params-all-reduce: 64.12 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 14.07 | optimizer-unscale-and-check-inf: 15.93 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.23 | batch-generator: 8.47
 iteration   100900/  200000 | consumed samples:      3228800 | elapsed time per iteration (ms): 257.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.283246E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.58 | backward-params-all-reduce: 64.26 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.96 | optimizer-unscale-and-check-inf: 15.98 | optimizer-clip-main-grad: 7.41 | optimizer-copy-main-to-model-params: 12.25 | optimizer: 97.77 | batch-generator: 8.45
 iteration   101000/  200000 | consumed samples:      3232000 | elapsed time per iteration (ms): 258.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.292983E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.14 | backward-params-all-reduce: 63.21 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.38 | optimizer-unscale-and-check-inf: 16.25 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.54 | optimizer: 99.00 | batch-generator: 8.56
--------------------------------------------------------------------------------------------------
 validation loss at iteration 101000 | lm loss value: 3.068959E-05 | lm loss PPL: 1.000031E+00 | 
 at iteration 101000, match long value: -0.001620278516034315 | match short value: -0.031143818641853872 
----------------------------------------------------------------------------------------------------------
 iteration   101100/  200000 | consumed samples:      3235200 | elapsed time per iteration (ms): 273.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.311599E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.30 | backward-params-all-reduce: 63.14 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 14.15 | optimizer-unscale-and-check-inf: 16.29 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 99.67 | batch-generator: 18.84
 iteration   101200/  200000 | consumed samples:      3238400 | elapsed time per iteration (ms): 258.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.308514E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.08 | backward-params-all-reduce: 63.30 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.88 | optimizer-unscale-and-check-inf: 16.27 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 99.55 | batch-generator: 8.62
 iteration   101300/  200000 | consumed samples:      3241600 | elapsed time per iteration (ms): 268.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.315253E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.64 | backward-params-all-reduce: 64.10 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.33 | optimizer-unscale-and-check-inf: 16.06 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 14.32 | optimizer: 100.49 | batch-generator: 8.68
 iteration   101400/  200000 | consumed samples:      3244800 | elapsed time per iteration (ms): 265.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.307280E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.04 | backward-params-all-reduce: 63.10 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 14.11 | optimizer-unscale-and-check-inf: 16.16 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 99.52 | batch-generator: 8.45
 iteration   101500/  200000 | consumed samples:      3248000 | elapsed time per iteration (ms): 257.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.308524E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.02 | backward-params-all-reduce: 63.66 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.11 | optimizer-unscale-and-check-inf: 16.27 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 98.57 | batch-generator: 8.53
--------------------------------------------------------------------------------------------------
 validation loss at iteration 101500 | lm loss value: 3.196954E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 101500, match long value: -0.00013090388618637876 | match short value: -0.056963084509917467 
------------------------------------------------------------------------------------------------------------
 iteration   101600/  200000 | consumed samples:      3251200 | elapsed time per iteration (ms): 269.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.314032E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.62 | backward-params-all-reduce: 65.08 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.18 | optimizer-unscale-and-check-inf: 16.59 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.13 | batch-generator: 14.27
 iteration   101700/  200000 | consumed samples:      3254400 | elapsed time per iteration (ms): 259.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.305406E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.47 | backward-params-all-reduce: 64.62 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.05 | optimizer-unscale-and-check-inf: 17.53 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.87 | batch-generator: 8.53
 iteration   101800/  200000 | consumed samples:      3257600 | elapsed time per iteration (ms): 259.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.305385E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.27 | backward-params-all-reduce: 63.62 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.26 | optimizer-unscale-and-check-inf: 17.37 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.93 | batch-generator: 8.62
 iteration   101900/  200000 | consumed samples:      3260800 | elapsed time per iteration (ms): 258.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.304221E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.23 | backward-params-all-reduce: 63.71 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.84 | optimizer-unscale-and-check-inf: 17.08 | optimizer-clip-main-grad: 7.52 | optimizer-copy-main-to-model-params: 12.26 | optimizer: 97.90 | batch-generator: 8.66
 iteration   102000/  200000 | consumed samples:      3264000 | elapsed time per iteration (ms): 257.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.321595E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.26 | backward-params-all-reduce: 63.61 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.14 | optimizer-unscale-and-check-inf: 17.33 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.72 | batch-generator: 8.56
--------------------------------------------------------------------------------------------------
 validation loss at iteration 102000 | lm loss value: 3.392384E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 102000, match long value: 0.0022434494539041094 | match short value: -0.026680409337085605 
----------------------------------------------------------------------------------------------------------
 iteration   102100/  200000 | consumed samples:      3267200 | elapsed time per iteration (ms): 284.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.320529E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.22 | backward-params-all-reduce: 63.93 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.18 | optimizer-unscale-and-check-inf: 16.54 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 14.26 | optimizer: 99.79 | batch-generator: 16.03
 iteration   102200/  200000 | consumed samples:      3270400 | elapsed time per iteration (ms): 259.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.320541E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.25 | backward-params-all-reduce: 64.06 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.09 | optimizer-unscale-and-check-inf: 16.60 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 97.99 | batch-generator: 8.65
 iteration   102300/  200000 | consumed samples:      3273600 | elapsed time per iteration (ms): 258.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.334513E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.31 | backward-params-all-reduce: 62.86 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.27 | optimizer-unscale-and-check-inf: 17.98 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.53 | batch-generator: 8.62
 iteration   102400/  200000 | consumed samples:      3276800 | elapsed time per iteration (ms): 259.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.338614E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.07 | backward-params-all-reduce: 63.63 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.62 | optimizer-unscale-and-check-inf: 17.15 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.35 | optimizer: 98.36 | batch-generator: 8.61
 iteration   102500/  200000 | consumed samples:      3280000 | elapsed time per iteration (ms): 260.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.338165E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.78 | backward-params-all-reduce: 64.56 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.32 | optimizer-unscale-and-check-inf: 16.18 | optimizer-clip-main-grad: 7.81 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 97.96 | batch-generator: 8.64
--------------------------------------------------------------------------------------------------
 validation loss at iteration 102500 | lm loss value: 3.165553E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 102500, match long value: 0.00020248592652128623 | match short value: -0.03248050203047041 
----------------------------------------------------------------------------------------------------------
 iteration   102600/  200000 | consumed samples:      3283200 | elapsed time per iteration (ms): 275.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.344919E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.61 | backward-params-all-reduce: 64.33 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.55 | optimizer-unscale-and-check-inf: 16.91 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 98.85 | batch-generator: 20.11
 iteration   102700/  200000 | consumed samples:      3286400 | elapsed time per iteration (ms): 261.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.346631E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.37 | backward-params-all-reduce: 64.21 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.44 | optimizer-unscale-and-check-inf: 17.42 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.16 | batch-generator: 8.61
 iteration   102800/  200000 | consumed samples:      3289600 | elapsed time per iteration (ms): 271.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.350935E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.74 | backward-params-all-reduce: 64.01 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.20 | optimizer-unscale-and-check-inf: 16.76 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 14.25 | optimizer: 100.02 | batch-generator: 8.66
 iteration   102900/  200000 | consumed samples:      3292800 | elapsed time per iteration (ms): 259.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.333869E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.38 | backward-params-all-reduce: 63.02 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.46 | optimizer-unscale-and-check-inf: 16.20 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.40 | optimizer: 97.33 | batch-generator: 8.64
 iteration   103000/  200000 | consumed samples:      3296000 | elapsed time per iteration (ms): 259.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.347387E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.17 | backward-params-all-reduce: 63.64 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.12 | optimizer-unscale-and-check-inf: 16.92 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.31 | batch-generator: 8.61
--------------------------------------------------------------------------------------------------
 validation loss at iteration 103000 | lm loss value: 3.304600E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 103000, match long value: 0.0036664422323982432 | match short value: -0.02277574422382343 
---------------------------------------------------------------------------------------------------------
 iteration   103100/  200000 | consumed samples:      3299200 | elapsed time per iteration (ms): 272.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.361884E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.41 | backward-params-all-reduce: 64.34 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.46 | optimizer-unscale-and-check-inf: 16.44 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.58 | optimizer: 98.22 | batch-generator: 16.94
 iteration   103200/  200000 | consumed samples:      3302400 | elapsed time per iteration (ms): 264.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.308867E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.66 | backward-params-all-reduce: 63.26 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.46 | optimizer-unscale-and-check-inf: 17.06 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.72 | batch-generator: 14.19
 iteration   103300/  200000 | consumed samples:      3305600 | elapsed time per iteration (ms): 262.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.287388E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.20 | backward-params-all-reduce: 64.95 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.52 | optimizer-unscale-and-check-inf: 18.62 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.39 | optimizer: 99.76 | batch-generator: 8.58
 iteration   103400/  200000 | consumed samples:      3308800 | elapsed time per iteration (ms): 262.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.282588E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.38 | backward-params-all-reduce: 64.74 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.36 | optimizer-unscale-and-check-inf: 18.49 | optimizer-clip-main-grad: 7.80 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 100.33 | batch-generator: 8.64
 iteration   103500/  200000 | consumed samples:      3312000 | elapsed time per iteration (ms): 269.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.266990E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.98 | backward-params-all-reduce: 65.90 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.25 | optimizer-unscale-and-check-inf: 18.47 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 14.21 | optimizer: 101.70 | batch-generator: 8.50
--------------------------------------------------------------------------------------------------
 validation loss at iteration 103500 | lm loss value: 3.274277E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 103500, match long value: 0.001036349255068038 | match short value: -0.01637906464915745 
--------------------------------------------------------------------------------------------------------
 iteration   103600/  200000 | consumed samples:      3315200 | elapsed time per iteration (ms): 283.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.284962E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.82 | backward-params-all-reduce: 65.19 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.19 | optimizer-unscale-and-check-inf: 18.49 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 100.02 | batch-generator: 16.10
 iteration   103700/  200000 | consumed samples:      3318400 | elapsed time per iteration (ms): 261.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.293795E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.63 | backward-params-all-reduce: 64.83 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.70 | optimizer-unscale-and-check-inf: 17.19 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.17 | batch-generator: 8.71
 iteration   103800/  200000 | consumed samples:      3321600 | elapsed time per iteration (ms): 262.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.300173E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.22 | backward-params-all-reduce: 64.26 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.17 | optimizer-unscale-and-check-inf: 18.55 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.96 | batch-generator: 8.47
 iteration   103900/  200000 | consumed samples:      3324800 | elapsed time per iteration (ms): 259.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.276873E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.40 | backward-params-all-reduce: 63.97 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.16 | optimizer-unscale-and-check-inf: 18.03 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.48 | batch-generator: 8.51
 iteration   104000/  200000 | consumed samples:      3328000 | elapsed time per iteration (ms): 260.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.297061E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.28 | backward-params-all-reduce: 64.52 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.45 | optimizer-unscale-and-check-inf: 16.76 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.48 | batch-generator: 8.63
--------------------------------------------------------------------------------------------------
 validation loss at iteration 104000 | lm loss value: 3.193263E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 104000, match long value: 0.0004024892744229614 | match short value: -0.00870769391428842 
---------------------------------------------------------------------------------------------------------
 iteration   104100/  200000 | consumed samples:      3331200 | elapsed time per iteration (ms): 277.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.315220E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.53 | backward-params-all-reduce: 64.14 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.17 | optimizer-unscale-and-check-inf: 17.95 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 99.43 | batch-generator: 21.57
 iteration   104200/  200000 | consumed samples:      3334400 | elapsed time per iteration (ms): 259.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.304487E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.11 | backward-params-all-reduce: 64.89 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.43 | optimizer-unscale-and-check-inf: 17.22 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 98.92 | batch-generator: 8.50
 iteration   104300/  200000 | consumed samples:      3337600 | elapsed time per iteration (ms): 278.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.312046E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 54.03 | backward-params-all-reduce: 64.41 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.47 | optimizer-unscale-and-check-inf: 17.71 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 14.36 | optimizer: 101.34 | batch-generator: 8.67
 iteration   104400/  200000 | consumed samples:      3340800 | elapsed time per iteration (ms): 261.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.322329E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.17 | backward-params-all-reduce: 64.87 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.17 | optimizer-unscale-and-check-inf: 18.06 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 12.56 | optimizer: 99.74 | batch-generator: 8.68
 iteration   104500/  200000 | consumed samples:      3344000 | elapsed time per iteration (ms): 259.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.322826E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.13 | backward-params-all-reduce: 63.64 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.54 | optimizer-unscale-and-check-inf: 17.49 | optimizer-clip-main-grad: 7.72 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 99.44 | batch-generator: 8.65
--------------------------------------------------------------------------------------------------
 validation loss at iteration 104500 | lm loss value: 3.311690E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 104500, match long value: -0.006217586439841306 | match short value: -0.025870194405677543 
----------------------------------------------------------------------------------------------------------
 iteration   104600/  200000 | consumed samples:      3347200 | elapsed time per iteration (ms): 271.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.319833E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.28 | backward-params-all-reduce: 63.96 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.26 | optimizer-unscale-and-check-inf: 16.38 | optimizer-clip-main-grad: 7.76 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.08 | batch-generator: 15.02
 iteration   104700/  200000 | consumed samples:      3350400 | elapsed time per iteration (ms): 262.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.310767E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.45 | backward-params-all-reduce: 66.78 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.31 | optimizer-unscale-and-check-inf: 16.81 | optimizer-clip-main-grad: 7.71 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 98.51 | batch-generator: 8.54
 iteration   104800/  200000 | consumed samples:      3353600 | elapsed time per iteration (ms): 261.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.313269E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.16 | backward-params-all-reduce: 64.12 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.22 | optimizer-unscale-and-check-inf: 17.86 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 99.44 | batch-generator: 8.66
 iteration   104900/  200000 | consumed samples:      3356800 | elapsed time per iteration (ms): 259.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.321965E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.11 | backward-params-all-reduce: 63.70 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.20 | optimizer-unscale-and-check-inf: 16.76 | optimizer-clip-main-grad: 7.54 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.11 | batch-generator: 8.41
 iteration   105000/  200000 | consumed samples:      3360000 | elapsed time per iteration (ms): 275.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.324094E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.35 | backward-params-all-reduce: 65.90 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.17 | optimizer-unscale-and-check-inf: 16.54 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 14.25 | optimizer: 99.71 | batch-generator: 8.60
--------------------------------------------------------------------------------------------------
 validation loss at iteration 105000 | lm loss value: 3.160782E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 105000, match long value: -0.004420086720238799 | match short value: -0.03489615961922812 
---------------------------------------------------------------------------------------------------------
 iteration   105100/  200000 | consumed samples:      3363200 | elapsed time per iteration (ms): 275.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.322023E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.80 | backward-params-all-reduce: 64.49 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.12 | optimizer-unscale-and-check-inf: 17.05 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.48 | batch-generator: 16.93
 iteration   105200/  200000 | consumed samples:      3366400 | elapsed time per iteration (ms): 259.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.347745E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.66 | backward-params-all-reduce: 63.39 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.31 | optimizer-unscale-and-check-inf: 17.51 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.57 | optimizer: 99.18 | batch-generator: 8.67
 iteration   105300/  200000 | consumed samples:      3369600 | elapsed time per iteration (ms): 257.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.332518E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.37 | backward-params-all-reduce: 62.56 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.45 | optimizer-unscale-and-check-inf: 16.93 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.58 | optimizer: 98.75 | batch-generator: 8.56
 iteration   105400/  200000 | consumed samples:      3372800 | elapsed time per iteration (ms): 258.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.321244E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.78 | backward-params-all-reduce: 62.79 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.45 | optimizer-unscale-and-check-inf: 16.25 | optimizer-clip-main-grad: 7.71 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 98.09 | batch-generator: 8.58
 iteration   105500/  200000 | consumed samples:      3376000 | elapsed time per iteration (ms): 260.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.328525E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.96 | backward-params-all-reduce: 64.18 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.43 | optimizer-unscale-and-check-inf: 17.18 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.97 | batch-generator: 8.55
--------------------------------------------------------------------------------------------------
 validation loss at iteration 105500 | lm loss value: 3.211989E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 105500, match long value: 0.0013692886797873913 | match short value: -0.025427538632185977 
----------------------------------------------------------------------------------------------------------
 iteration   105600/  200000 | consumed samples:      3379200 | elapsed time per iteration (ms): 276.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.329386E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.31 | backward-params-all-reduce: 63.71 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.07 | optimizer-unscale-and-check-inf: 15.81 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 97.17 | batch-generator: 21.65
 iteration   105700/  200000 | consumed samples:      3382400 | elapsed time per iteration (ms): 270.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.318587E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.64 | backward-params-all-reduce: 64.18 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.79 | optimizer-unscale-and-check-inf: 16.34 | optimizer-clip-main-grad: 7.72 | optimizer-copy-main-to-model-params: 14.28 | optimizer: 100.36 | batch-generator: 8.91
 iteration   105800/  200000 | consumed samples:      3385600 | elapsed time per iteration (ms): 265.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.332139E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.46 | backward-params-all-reduce: 64.81 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.02 | optimizer-unscale-and-check-inf: 16.21 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 97.40 | batch-generator: 8.67
 iteration   105900/  200000 | consumed samples:      3388800 | elapsed time per iteration (ms): 257.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.352162E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.13 | backward-params-all-reduce: 62.60 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.17 | optimizer-unscale-and-check-inf: 16.10 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 97.55 | batch-generator: 8.60
 iteration   106000/  200000 | consumed samples:      3392000 | elapsed time per iteration (ms): 259.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.346363E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.33 | backward-params-all-reduce: 62.54 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.10 | optimizer-unscale-and-check-inf: 17.64 | optimizer-clip-main-grad: 7.54 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.94 | batch-generator: 8.44
--------------------------------------------------------------------------------------------------
 validation loss at iteration 106000 | lm loss value: 3.227761E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 106000, match long value: 0.005003050166543319 | match short value: -0.03862796704286586 
--------------------------------------------------------------------------------------------------------
 iteration   106100/  200000 | consumed samples:      3395200 | elapsed time per iteration (ms): 271.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.336227E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.28 | backward-params-all-reduce: 63.86 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.07 | optimizer-unscale-and-check-inf: 16.08 | optimizer-clip-main-grad: 7.54 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 97.33 | batch-generator: 16.82
 iteration   106200/  200000 | consumed samples:      3398400 | elapsed time per iteration (ms): 261.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.321433E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.44 | backward-params-all-reduce: 64.19 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.05 | optimizer-unscale-and-check-inf: 18.42 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.69 | batch-generator: 8.61
 iteration   106300/  200000 | consumed samples:      3401600 | elapsed time per iteration (ms): 267.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.295019E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.93 | backward-params-all-reduce: 65.73 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.39 | optimizer-unscale-and-check-inf: 19.03 | optimizer-clip-main-grad: 7.72 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 100.85 | batch-generator: 14.00
 iteration   106400/  200000 | consumed samples:      3404800 | elapsed time per iteration (ms): 264.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.286377E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.54 | backward-params-all-reduce: 65.47 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.40 | optimizer-unscale-and-check-inf: 18.54 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 100.28 | batch-generator: 8.59
 iteration   106500/  200000 | consumed samples:      3408000 | elapsed time per iteration (ms): 342.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.282768E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.26 | backward-params-all-reduce: 63.75 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.06 | optimizer-unscale-and-check-inf: 18.05 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 14.27 | optimizer: 101.21 | batch-generator: 77.43
--------------------------------------------------------------------------------------------------
 validation loss at iteration 106500 | lm loss value: 3.269958E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 106500, match long value: -0.0022379842056379578 | match short value: -0.028430613854393074 
-----------------------------------------------------------------------------------------------------------
 iteration   106600/  200000 | consumed samples:      3411200 | elapsed time per iteration (ms): 274.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.293969E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.40 | backward-params-all-reduce: 64.65 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.65 | optimizer-unscale-and-check-inf: 19.75 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 101.70 | batch-generator: 15.63
 iteration   106700/  200000 | consumed samples:      3414400 | elapsed time per iteration (ms): 260.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.276157E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.26 | backward-params-all-reduce: 64.84 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.16 | optimizer-unscale-and-check-inf: 17.69 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 100.19 | batch-generator: 8.67
 iteration   106800/  200000 | consumed samples:      3417600 | elapsed time per iteration (ms): 260.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.301008E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.25 | backward-params-all-reduce: 64.04 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.82 | optimizer-unscale-and-check-inf: 18.05 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 100.12 | batch-generator: 8.58
 iteration   106900/  200000 | consumed samples:      3420800 | elapsed time per iteration (ms): 261.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.302686E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.16 | backward-params-all-reduce: 63.85 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.90 | optimizer-unscale-and-check-inf: 18.78 | optimizer-clip-main-grad: 7.53 | optimizer-copy-main-to-model-params: 12.38 | optimizer: 100.24 | batch-generator: 8.53
 iteration   107000/  200000 | consumed samples:      3424000 | elapsed time per iteration (ms): 260.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.277852E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.96 | backward-params-all-reduce: 66.31 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.01 | optimizer-unscale-and-check-inf: 17.93 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 100.15 | batch-generator: 8.45
--------------------------------------------------------------------------------------------------
 validation loss at iteration 107000 | lm loss value: 3.217499E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 107000, match long value: 0.0053671396419492905 | match short value: -0.03137093489084305 
---------------------------------------------------------------------------------------------------------
 iteration   107100/  200000 | consumed samples:      3427200 | elapsed time per iteration (ms): 275.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.301398E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.47 | backward-params-all-reduce: 63.37 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.11 | optimizer-unscale-and-check-inf: 17.08 | optimizer-clip-main-grad: 7.54 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.39 | batch-generator: 18.67
 iteration   107200/  200000 | consumed samples:      3430400 | elapsed time per iteration (ms): 270.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.306414E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.87 | backward-params-all-reduce: 63.70 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.27 | optimizer-unscale-and-check-inf: 18.18 | optimizer-clip-main-grad: 7.54 | optimizer-copy-main-to-model-params: 14.20 | optimizer: 101.32 | batch-generator: 8.41
 iteration   107300/  200000 | consumed samples:      3433600 | elapsed time per iteration (ms): 261.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.300716E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.54 | backward-params-all-reduce: 63.45 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.14 | optimizer-unscale-and-check-inf: 17.80 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.39 | optimizer: 99.54 | batch-generator: 8.53
 iteration   107400/  200000 | consumed samples:      3436800 | elapsed time per iteration (ms): 258.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.320762E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.64 | backward-params-all-reduce: 63.84 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.20 | optimizer-unscale-and-check-inf: 17.04 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.51 | batch-generator: 8.50
 iteration   107500/  200000 | consumed samples:      3440000 | elapsed time per iteration (ms): 258.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.309658E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.49 | backward-params-all-reduce: 63.14 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 13.23 | optimizer-unscale-and-check-inf: 16.60 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.08 | batch-generator: 8.57
--------------------------------------------------------------------------------------------------
 validation loss at iteration 107500 | lm loss value: 3.244319E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 107500, match long value: -0.0015816959241187322 | match short value: -0.05380908730714129 
----------------------------------------------------------------------------------------------------------
 iteration   107600/  200000 | consumed samples:      3443200 | elapsed time per iteration (ms): 268.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.300843E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.47 | backward-params-all-reduce: 63.73 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.10 | optimizer-unscale-and-check-inf: 16.46 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 98.78 | batch-generator: 14.81
 iteration   107700/  200000 | consumed samples:      3446400 | elapsed time per iteration (ms): 258.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.313847E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.01 | backward-params-all-reduce: 65.31 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.24 | optimizer-unscale-and-check-inf: 17.13 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.44 | optimizer: 98.51 | batch-generator: 8.54
 iteration   107800/  200000 | consumed samples:      3449600 | elapsed time per iteration (ms): 259.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.328890E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.48 | backward-params-all-reduce: 65.27 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.00 | optimizer-unscale-and-check-inf: 16.69 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 98.87 | batch-generator: 8.47
 iteration   107900/  200000 | consumed samples:      3452800 | elapsed time per iteration (ms): 266.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.310781E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.10 | backward-params-all-reduce: 63.38 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.02 | optimizer-unscale-and-check-inf: 16.75 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 14.36 | optimizer: 100.91 | batch-generator: 8.43
 iteration   108000/  200000 | consumed samples:      3456000 | elapsed time per iteration (ms): 263.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.322409E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.59 | backward-params-all-reduce: 64.54 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.10 | optimizer-unscale-and-check-inf: 16.55 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.90 | batch-generator: 8.56
--------------------------------------------------------------------------------------------------
 validation loss at iteration 108000 | lm loss value: 3.275175E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 108000, match long value: -0.00025652071028229873 | match short value: -0.03907451926585996 
-----------------------------------------------------------------------------------------------------------
 iteration   108100/  200000 | consumed samples:      3459200 | elapsed time per iteration (ms): 270.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.314614E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.42 | backward-params-all-reduce: 65.50 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.04 | optimizer-unscale-and-check-inf: 16.03 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.32 | batch-generator: 15.35
 iteration   108200/  200000 | consumed samples:      3462400 | elapsed time per iteration (ms): 257.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.330443E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.30 | backward-params-all-reduce: 63.95 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.25 | optimizer-unscale-and-check-inf: 15.90 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 98.37 | batch-generator: 8.56
 iteration   108300/  200000 | consumed samples:      3465600 | elapsed time per iteration (ms): 257.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.338690E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.20 | backward-params-all-reduce: 63.41 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.89 | optimizer-unscale-and-check-inf: 16.62 | optimizer-clip-main-grad: 7.50 | optimizer-copy-main-to-model-params: 12.37 | optimizer: 98.03 | batch-generator: 8.51
 iteration   108400/  200000 | consumed samples:      3468800 | elapsed time per iteration (ms): 256.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.333239E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.61 | backward-params-all-reduce: 64.37 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.49 | optimizer-unscale-and-check-inf: 16.36 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 97.99 | batch-generator: 8.43
 iteration   108500/  200000 | consumed samples:      3472000 | elapsed time per iteration (ms): 257.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.305825E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.18 | backward-params-all-reduce: 64.20 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.67 | optimizer-unscale-and-check-inf: 15.62 | optimizer-clip-main-grad: 7.54 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.49 | batch-generator: 8.53
--------------------------------------------------------------------------------------------------
 validation loss at iteration 108500 | lm loss value: 3.170903E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 108500, match long value: 0.000820768030993622 | match short value: -0.037545005327190455 
---------------------------------------------------------------------------------------------------------
 iteration   108600/  200000 | consumed samples:      3475200 | elapsed time per iteration (ms): 271.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.324040E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.59 | backward-params-all-reduce: 64.22 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.97 | optimizer-unscale-and-check-inf: 16.21 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 98.48 | batch-generator: 16.77
 iteration   108700/  200000 | consumed samples:      3478400 | elapsed time per iteration (ms): 270.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.317121E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 54.05 | backward-params-all-reduce: 62.73 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.19 | optimizer-unscale-and-check-inf: 15.67 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 14.28 | optimizer: 99.95 | batch-generator: 8.95
 iteration   108800/  200000 | consumed samples:      3481600 | elapsed time per iteration (ms): 259.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.344471E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.31 | backward-params-all-reduce: 65.17 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.77 | optimizer-unscale-and-check-inf: 15.85 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 97.89 | batch-generator: 8.75
 iteration   108900/  200000 | consumed samples:      3484800 | elapsed time per iteration (ms): 259.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.326355E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.48 | backward-params-all-reduce: 64.07 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.65 | optimizer-unscale-and-check-inf: 15.95 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 98.90 | batch-generator: 8.70
 iteration   109000/  200000 | consumed samples:      3488000 | elapsed time per iteration (ms): 260.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.330991E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.35 | backward-params-all-reduce: 64.14 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.04 | optimizer-unscale-and-check-inf: 15.80 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.11 | batch-generator: 8.58
--------------------------------------------------------------------------------------------------
 validation loss at iteration 109000 | lm loss value: 3.217651E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 109000, match long value: 0.0031676220137751366 | match short value: -0.024091685069725106 
----------------------------------------------------------------------------------------------------------
 iteration   109100/  200000 | consumed samples:      3491200 | elapsed time per iteration (ms): 276.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.331954E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.57 | backward-params-all-reduce: 63.42 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.76 | optimizer-unscale-and-check-inf: 15.45 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.56 | optimizer: 98.59 | batch-generator: 19.86
 iteration   109200/  200000 | consumed samples:      3494400 | elapsed time per iteration (ms): 260.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.320492E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.75 | backward-params-all-reduce: 64.39 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.57 | optimizer-unscale-and-check-inf: 16.34 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.16 | batch-generator: 8.64
 iteration   109300/  200000 | consumed samples:      3497600 | elapsed time per iteration (ms): 257.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.336065E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.20 | backward-params-all-reduce: 63.07 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.26 | optimizer-unscale-and-check-inf: 15.95 | optimizer-clip-main-grad: 7.44 | optimizer-copy-main-to-model-params: 12.26 | optimizer: 97.08 | batch-generator: 8.61
 iteration   109400/  200000 | consumed samples:      3500800 | elapsed time per iteration (ms): 277.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.322586E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.76 | backward-params-all-reduce: 63.88 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.55 | optimizer-unscale-and-check-inf: 17.32 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 14.26 | optimizer: 100.97 | batch-generator: 14.12
 iteration   109500/  200000 | consumed samples:      3504000 | elapsed time per iteration (ms): 265.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.284646E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.34 | backward-params-all-reduce: 65.02 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.18 | optimizer-unscale-and-check-inf: 16.87 | optimizer-clip-main-grad: 7.70 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 99.44 | batch-generator: 8.53
--------------------------------------------------------------------------------------------------
 validation loss at iteration 109500 | lm loss value: 3.230751E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 109500, match long value: 0.001980457762438187 | match short value: -0.03536903912378058 
--------------------------------------------------------------------------------------------------------
 iteration   109600/  200000 | consumed samples:      3507200 | elapsed time per iteration (ms): 273.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.281842E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.70 | backward-params-all-reduce: 64.65 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.13 | optimizer-unscale-and-check-inf: 16.33 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.73 | batch-generator: 15.89
 iteration   109700/  200000 | consumed samples:      3510400 | elapsed time per iteration (ms): 261.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.293372E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.48 | backward-params-all-reduce: 65.36 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.51 | optimizer-unscale-and-check-inf: 15.91 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 97.56 | batch-generator: 8.45
 iteration   109800/  200000 | consumed samples:      3513600 | elapsed time per iteration (ms): 258.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.299940E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.12 | backward-params-all-reduce: 63.83 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.45 | optimizer-unscale-and-check-inf: 16.10 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.75 | batch-generator: 8.48
 iteration   109900/  200000 | consumed samples:      3516800 | elapsed time per iteration (ms): 259.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.285234E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.84 | backward-params-all-reduce: 62.92 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.59 | optimizer-unscale-and-check-inf: 14.66 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 97.58 | batch-generator: 8.56
 iteration   110000/  200000 | consumed samples:      3520000 | elapsed time per iteration (ms): 259.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.276800E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.26 | backward-params-all-reduce: 64.48 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.83 | optimizer-unscale-and-check-inf: 15.69 | optimizer-clip-main-grad: 7.45 | optimizer-copy-main-to-model-params: 12.24 | optimizer: 96.36 | batch-generator: 8.44
--------------------------------------------------------------------------------------------------
 validation loss at iteration 110000 | lm loss value: 3.263814E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 110000, match long value: -0.002385895545816711 | match short value: -0.04373119478027284 
---------------------------------------------------------------------------------------------------------
 iteration   110100/  200000 | consumed samples:      3523200 | elapsed time per iteration (ms): 285.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.283854E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.02 | backward-params-all-reduce: 64.35 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.05 | optimizer-unscale-and-check-inf: 16.32 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 14.30 | optimizer: 100.39 | batch-generator: 16.28
 iteration   110200/  200000 | consumed samples:      3526400 | elapsed time per iteration (ms): 265.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.302261E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.54 | backward-params-all-reduce: 63.49 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.32 | optimizer-unscale-and-check-inf: 16.01 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 97.50 | batch-generator: 8.50
 iteration   110300/  200000 | consumed samples:      3529600 | elapsed time per iteration (ms): 259.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.297510E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.48 | backward-params-all-reduce: 63.23 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.55 | optimizer-unscale-and-check-inf: 17.71 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 99.57 | batch-generator: 8.67
 iteration   110400/  200000 | consumed samples:      3532800 | elapsed time per iteration (ms): 259.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.284969E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.28 | backward-params-all-reduce: 63.58 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.07 | optimizer-unscale-and-check-inf: 15.82 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.24 | batch-generator: 8.64
 iteration   110500/  200000 | consumed samples:      3536000 | elapsed time per iteration (ms): 261.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.296623E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.04 | backward-params-all-reduce: 63.83 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.26 | optimizer-unscale-and-check-inf: 15.74 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 98.21 | batch-generator: 8.54
--------------------------------------------------------------------------------------------------
 validation loss at iteration 110500 | lm loss value: 3.212108E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 110500, match long value: -0.0046022413695808746 | match short value: -0.03260222241021225 
----------------------------------------------------------------------------------------------------------
 iteration   110600/  200000 | consumed samples:      3539200 | elapsed time per iteration (ms): 276.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.302666E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.63 | backward-params-all-reduce: 64.54 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.82 | optimizer-unscale-and-check-inf: 15.96 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 98.02 | batch-generator: 19.32
 iteration   110700/  200000 | consumed samples:      3542400 | elapsed time per iteration (ms): 258.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.307837E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.46 | backward-params-all-reduce: 63.41 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.21 | optimizer-unscale-and-check-inf: 16.55 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 98.00 | batch-generator: 8.56
 iteration   110800/  200000 | consumed samples:      3545600 | elapsed time per iteration (ms): 261.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.311082E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.28 | backward-params-all-reduce: 63.09 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.09 | optimizer-unscale-and-check-inf: 16.36 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.64 | batch-generator: 8.51
 iteration   110900/  200000 | consumed samples:      3548800 | elapsed time per iteration (ms): 273.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.306677E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.87 | backward-params-all-reduce: 64.88 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 11.92 | optimizer-unscale-and-check-inf: 15.71 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 14.32 | optimizer: 98.71 | batch-generator: 8.56
 iteration   111000/  200000 | consumed samples:      3552000 | elapsed time per iteration (ms): 259.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.310384E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.22 | backward-params-all-reduce: 65.23 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.53 | optimizer-unscale-and-check-inf: 16.11 | optimizer-clip-main-grad: 7.54 | optimizer-copy-main-to-model-params: 12.43 | optimizer: 97.73 | batch-generator: 8.34
--------------------------------------------------------------------------------------------------
 validation loss at iteration 111000 | lm loss value: 3.173681E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 111000, match long value: -0.0007074624412814981 | match short value: -0.03689645269725855 
----------------------------------------------------------------------------------------------------------
 iteration   111100/  200000 | consumed samples:      3555200 | elapsed time per iteration (ms): 271.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.309484E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.14 | backward-params-all-reduce: 64.31 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.24 | optimizer-unscale-and-check-inf: 14.56 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 96.06 | batch-generator: 14.60
 iteration   111200/  200000 | consumed samples:      3558400 | elapsed time per iteration (ms): 258.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.309432E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.75 | backward-params-all-reduce: 64.59 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.42 | optimizer-unscale-and-check-inf: 14.56 | optimizer-clip-main-grad: 7.54 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 96.18 | batch-generator: 8.52
 iteration   111300/  200000 | consumed samples:      3561600 | elapsed time per iteration (ms): 260.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.305337E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.08 | backward-params-all-reduce: 64.21 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.51 | optimizer-unscale-and-check-inf: 15.79 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 97.55 | batch-generator: 8.47
 iteration   111400/  200000 | consumed samples:      3564800 | elapsed time per iteration (ms): 258.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.334699E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.06 | backward-params-all-reduce: 63.75 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.57 | optimizer-unscale-and-check-inf: 15.79 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 97.62 | batch-generator: 8.50
 iteration   111500/  200000 | consumed samples:      3568000 | elapsed time per iteration (ms): 260.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.314901E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.99 | backward-params-all-reduce: 65.09 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.34 | optimizer-unscale-and-check-inf: 15.71 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.45 | optimizer: 97.20 | batch-generator: 8.53
--------------------------------------------------------------------------------------------------
 validation loss at iteration 111500 | lm loss value: 3.293029E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 111500, match long value: 0.0023042871698550154 | match short value: -0.041683068920642226 
----------------------------------------------------------------------------------------------------------
 iteration   111600/  200000 | consumed samples:      3571200 | elapsed time per iteration (ms): 285.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.332053E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.09 | backward-params-all-reduce: 64.07 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.41 | optimizer-unscale-and-check-inf: 15.33 | optimizer-clip-main-grad: 7.52 | optimizer-copy-main-to-model-params: 14.17 | optimizer: 98.57 | batch-generator: 14.81
 iteration   111700/  200000 | consumed samples:      3574400 | elapsed time per iteration (ms): 264.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.333008E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.78 | backward-params-all-reduce: 64.44 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.26 | optimizer-unscale-and-check-inf: 16.56 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 98.01 | batch-generator: 8.52
 iteration   111800/  200000 | consumed samples:      3577600 | elapsed time per iteration (ms): 257.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.311893E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   3 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.45 | backward-params-all-reduce: 62.99 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.59 | optimizer-unscale-and-check-inf: 15.84 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.10 | optimizer: 95.54 | batch-generator: 8.48
 iteration   111900/  200000 | consumed samples:      3580800 | elapsed time per iteration (ms): 256.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.333605E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.38 | backward-params-all-reduce: 62.17 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.43 | optimizer-unscale-and-check-inf: 14.89 | optimizer-clip-main-grad: 7.51 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 96.43 | batch-generator: 8.51
 iteration   112000/  200000 | consumed samples:      3584000 | elapsed time per iteration (ms): 256.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.318608E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.06 | backward-params-all-reduce: 63.02 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.35 | optimizer-unscale-and-check-inf: 14.79 | optimizer-clip-main-grad: 7.52 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 96.28 | batch-generator: 8.52
--------------------------------------------------------------------------------------------------
 validation loss at iteration 112000 | lm loss value: 3.302138E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 112000, match long value: -0.0018680858316094267 | match short value: -0.005681657828562612 
-----------------------------------------------------------------------------------------------------------
 iteration   112100/  200000 | consumed samples:      3587200 | elapsed time per iteration (ms): 276.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.334807E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.00 | backward-params-all-reduce: 64.50 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.26 | optimizer-unscale-and-check-inf: 14.36 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.60 | optimizer: 96.02 | batch-generator: 21.27
 iteration   112200/  200000 | consumed samples:      3590400 | elapsed time per iteration (ms): 258.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.338489E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.24 | backward-params-all-reduce: 63.23 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.01 | optimizer-unscale-and-check-inf: 15.54 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 97.83 | batch-generator: 8.82
 iteration   112300/  200000 | consumed samples:      3593600 | elapsed time per iteration (ms): 268.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.341239E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.76 | backward-params-all-reduce: 64.15 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.34 | optimizer-unscale-and-check-inf: 15.44 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 14.20 | optimizer: 98.69 | batch-generator: 8.66
 iteration   112400/  200000 | consumed samples:      3596800 | elapsed time per iteration (ms): 266.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.343318E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.72 | backward-params-all-reduce: 64.51 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.29 | optimizer-unscale-and-check-inf: 15.19 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 96.72 | batch-generator: 8.53
 iteration   112500/  200000 | consumed samples:      3600000 | elapsed time per iteration (ms): 259.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.346754E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.19 | backward-params-all-reduce: 64.20 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.73 | optimizer-unscale-and-check-inf: 15.73 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 97.64 | batch-generator: 8.47
--------------------------------------------------------------------------------------------------
 validation loss at iteration 112500 | lm loss value: 3.144842E-05 | lm loss PPL: 1.000031E+00 | 
 at iteration 112500, match long value: 0.002277602391283642 | match short value: -0.021995419813151573 
---------------------------------------------------------------------------------------------------------
 iteration   112600/  200000 | consumed samples:      3603200 | elapsed time per iteration (ms): 277.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.284554E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.86 | backward-params-all-reduce: 63.67 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.68 | optimizer-unscale-and-check-inf: 15.65 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 97.54 | batch-generator: 20.42
 iteration   112700/  200000 | consumed samples:      3606400 | elapsed time per iteration (ms): 261.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.290563E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.14 | backward-params-all-reduce: 65.02 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.59 | optimizer-unscale-and-check-inf: 16.03 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 97.82 | batch-generator: 8.38
 iteration   112800/  200000 | consumed samples:      3609600 | elapsed time per iteration (ms): 260.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.272558E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.07 | backward-params-all-reduce: 64.79 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.56 | optimizer-unscale-and-check-inf: 15.70 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 97.53 | batch-generator: 8.64
 iteration   112900/  200000 | consumed samples:      3612800 | elapsed time per iteration (ms): 262.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.275968E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.15 | backward-params-all-reduce: 64.43 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.61 | optimizer-unscale-and-check-inf: 16.09 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 97.93 | batch-generator: 8.43
 iteration   113000/  200000 | consumed samples:      3616000 | elapsed time per iteration (ms): 264.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.275549E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.63 | backward-params-all-reduce: 65.29 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.72 | optimizer-unscale-and-check-inf: 14.96 | optimizer-clip-main-grad: 7.52 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 96.83 | batch-generator: 8.41
--------------------------------------------------------------------------------------------------
 validation loss at iteration 113000 | lm loss value: 3.377058E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 113000, match long value: 0.00024579170840570436 | match short value: 0.002023214699734689 
----------------------------------------------------------------------------------------------------------
 iteration   113100/  200000 | consumed samples:      3619200 | elapsed time per iteration (ms): 287.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.270932E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.78 | backward-params-all-reduce: 64.29 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.88 | optimizer-unscale-and-check-inf: 14.57 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 14.27 | optimizer: 98.50 | batch-generator: 15.38
 iteration   113200/  200000 | consumed samples:      3622400 | elapsed time per iteration (ms): 260.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.261381E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.73 | backward-params-all-reduce: 63.98 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.77 | optimizer-unscale-and-check-inf: 15.69 | optimizer-clip-main-grad: 7.52 | optimizer-copy-main-to-model-params: 12.45 | optimizer: 97.63 | batch-generator: 8.38
 iteration   113300/  200000 | consumed samples:      3625600 | elapsed time per iteration (ms): 260.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.293390E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.14 | backward-params-all-reduce: 63.73 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.29 | optimizer-unscale-and-check-inf: 16.30 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 97.81 | batch-generator: 8.45
 iteration   113400/  200000 | consumed samples:      3628800 | elapsed time per iteration (ms): 259.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.274364E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.75 | backward-params-all-reduce: 63.62 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.21 | optimizer-unscale-and-check-inf: 15.11 | optimizer-clip-main-grad: 7.51 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 96.47 | batch-generator: 8.30
 iteration   113500/  200000 | consumed samples:      3632000 | elapsed time per iteration (ms): 259.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.298208E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.15 | backward-params-all-reduce: 63.56 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.29 | optimizer-unscale-and-check-inf: 15.60 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 97.10 | batch-generator: 8.36
--------------------------------------------------------------------------------------------------
 validation loss at iteration 113500 | lm loss value: 3.126385E-05 | lm loss PPL: 1.000031E+00 | 
 at iteration 113500, match long value: -0.00036522651102855553 | match short value: -0.049264865802858736 
------------------------------------------------------------------------------------------------------------
 iteration   113600/  200000 | consumed samples:      3635200 | elapsed time per iteration (ms): 276.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.281285E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.37 | backward-params-all-reduce: 63.91 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.36 | optimizer-unscale-and-check-inf: 15.21 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 96.86 | batch-generator: 18.51
 iteration   113700/  200000 | consumed samples:      3638400 | elapsed time per iteration (ms): 260.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.296230E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.15 | backward-params-all-reduce: 63.71 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.54 | optimizer-unscale-and-check-inf: 16.03 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 97.84 | batch-generator: 8.60
 iteration   113800/  200000 | consumed samples:      3641600 | elapsed time per iteration (ms): 274.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.293501E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.87 | backward-params-all-reduce: 63.90 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.14 | optimizer-unscale-and-check-inf: 14.59 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 14.34 | optimizer: 97.81 | batch-generator: 8.64
 iteration   113900/  200000 | consumed samples:      3644800 | elapsed time per iteration (ms): 260.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.297405E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.41 | backward-params-all-reduce: 64.84 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.27 | optimizer-unscale-and-check-inf: 16.11 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 97.54 | batch-generator: 8.59
 iteration   114000/  200000 | consumed samples:      3648000 | elapsed time per iteration (ms): 259.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.310530E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.04 | backward-params-all-reduce: 64.27 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.58 | optimizer-unscale-and-check-inf: 15.02 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 96.86 | batch-generator: 8.62
--------------------------------------------------------------------------------------------------
 validation loss at iteration 114000 | lm loss value: 3.108832E-05 | lm loss PPL: 1.000031E+00 | 
 at iteration 114000, match long value: -0.0032369092552705005 | match short value: -0.03227817351542004 
----------------------------------------------------------------------------------------------------------
 iteration   114100/  200000 | consumed samples:      3651200 | elapsed time per iteration (ms): 273.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.317066E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.56 | backward-params-all-reduce: 64.12 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.06 | optimizer-unscale-and-check-inf: 16.07 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 98.48 | batch-generator: 14.69
 iteration   114200/  200000 | consumed samples:      3654400 | elapsed time per iteration (ms): 260.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.318503E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.19 | backward-params-all-reduce: 63.65 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.45 | optimizer-unscale-and-check-inf: 16.23 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 97.91 | batch-generator: 8.51
 iteration   114300/  200000 | consumed samples:      3657600 | elapsed time per iteration (ms): 258.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.304580E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.80 | backward-params-all-reduce: 63.89 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.45 | optimizer-unscale-and-check-inf: 15.37 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 97.01 | batch-generator: 8.55
 iteration   114400/  200000 | consumed samples:      3660800 | elapsed time per iteration (ms): 259.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.308534E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.60 | backward-params-all-reduce: 64.36 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.28 | optimizer-unscale-and-check-inf: 16.65 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.45 | optimizer: 98.07 | batch-generator: 8.40
 iteration   114500/  200000 | consumed samples:      3664000 | elapsed time per iteration (ms): 271.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.307513E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.90 | backward-params-all-reduce: 63.30 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.16 | optimizer-unscale-and-check-inf: 16.17 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 14.24 | optimizer: 99.33 | batch-generator: 8.57
--------------------------------------------------------------------------------------------------
 validation loss at iteration 114500 | lm loss value: 3.462143E-05 | lm loss PPL: 1.000035E+00 | 
 at iteration 114500, match long value: 0.0028419792939448052 | match short value: -0.028396027535073948 
----------------------------------------------------------------------------------------------------------
 iteration   114600/  200000 | consumed samples:      3667200 | elapsed time per iteration (ms): 278.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.315941E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.25 | backward-params-all-reduce: 64.79 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.71 | optimizer-unscale-and-check-inf: 14.81 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 96.76 | batch-generator: 15.73
 iteration   114700/  200000 | consumed samples:      3670400 | elapsed time per iteration (ms): 259.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.314087E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.12 | backward-params-all-reduce: 63.08 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.06 | optimizer-unscale-and-check-inf: 19.14 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 100.51 | batch-generator: 8.51
 iteration   114800/  200000 | consumed samples:      3673600 | elapsed time per iteration (ms): 260.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.300761E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.22 | backward-params-all-reduce: 63.70 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 11.96 | optimizer-unscale-and-check-inf: 18.97 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 100.25 | batch-generator: 8.50
 iteration   114900/  200000 | consumed samples:      3676800 | elapsed time per iteration (ms): 262.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.304695E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.28 | backward-params-all-reduce: 65.18 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.58 | optimizer-unscale-and-check-inf: 19.27 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 101.14 | batch-generator: 8.45
 iteration   115000/  200000 | consumed samples:      3680000 | elapsed time per iteration (ms): 256.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.321451E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   3 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.32 | backward-params-all-reduce: 62.37 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 11.93 | optimizer-unscale-and-check-inf: 19.39 | optimizer-clip-main-grad: 7.38 | optimizer-copy-main-to-model-params: 12.08 | optimizer: 98.45 | batch-generator: 8.47
--------------------------------------------------------------------------------------------------
 validation loss at iteration 115000 | lm loss value: 3.310259E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 115000, match long value: 0.0034991978402891615 | match short value: -0.04948727641266229 
---------------------------------------------------------------------------------------------------------
 iteration   115100/  200000 | consumed samples:      3683200 | elapsed time per iteration (ms): 333.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.335643E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.97 | backward-params-all-reduce: 62.43 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.17 | optimizer-unscale-and-check-inf: 18.80 | optimizer-clip-main-grad: 7.70 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 100.37 | batch-generator: 78.30
 iteration   115200/  200000 | consumed samples:      3686400 | elapsed time per iteration (ms): 261.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.331065E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.23 | backward-params-all-reduce: 63.55 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 11.91 | optimizer-unscale-and-check-inf: 18.69 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.79 | batch-generator: 8.70
 iteration   115300/  200000 | consumed samples:      3689600 | elapsed time per iteration (ms): 273.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.325671E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.54 | backward-params-all-reduce: 63.54 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.37 | optimizer-unscale-and-check-inf: 17.54 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 14.22 | optimizer: 100.85 | batch-generator: 8.62
 iteration   115400/  200000 | consumed samples:      3692800 | elapsed time per iteration (ms): 260.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.345392E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.05 | backward-params-all-reduce: 64.17 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.08 | optimizer-unscale-and-check-inf: 19.03 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 100.42 | batch-generator: 8.57
 iteration   115500/  200000 | consumed samples:      3696000 | elapsed time per iteration (ms): 260.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.344679E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.15 | backward-params-all-reduce: 63.31 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.30 | optimizer-unscale-and-check-inf: 17.83 | optimizer-clip-main-grad: 7.85 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 100.64 | batch-generator: 8.79
--------------------------------------------------------------------------------------------------
 validation loss at iteration 115500 | lm loss value: 3.228333E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 115500, match long value: -0.0018315841525303684 | match short value: -0.03732920828976299 
----------------------------------------------------------------------------------------------------------
 iteration   115600/  200000 | consumed samples:      3699200 | elapsed time per iteration (ms): 270.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.334039E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.37 | backward-params-all-reduce: 64.11 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.50 | optimizer-unscale-and-check-inf: 17.98 | optimizer-clip-main-grad: 7.71 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.85 | batch-generator: 14.28
 iteration   115700/  200000 | consumed samples:      3702400 | elapsed time per iteration (ms): 268.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.284110E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.29 | backward-params-all-reduce: 64.71 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.92 | optimizer-unscale-and-check-inf: 17.78 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.95 | batch-generator: 14.71
 iteration   115800/  200000 | consumed samples:      3705600 | elapsed time per iteration (ms): 260.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.245741E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.26 | backward-params-all-reduce: 65.27 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.64 | optimizer-unscale-and-check-inf: 17.77 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.60 | batch-generator: 8.51
 iteration   115900/  200000 | consumed samples:      3708800 | elapsed time per iteration (ms): 262.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.268548E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.72 | backward-params-all-reduce: 67.98 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.07 | optimizer-unscale-and-check-inf: 18.55 | optimizer-clip-main-grad: 7.54 | optimizer-copy-main-to-model-params: 12.41 | optimizer: 99.70 | batch-generator: 8.47
 iteration   116000/  200000 | consumed samples:      3712000 | elapsed time per iteration (ms): 273.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.279891E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.49 | backward-params-all-reduce: 65.43 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.08 | optimizer-unscale-and-check-inf: 16.63 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 14.19 | optimizer: 100.66 | batch-generator: 8.62
--------------------------------------------------------------------------------------------------
 validation loss at iteration 116000 | lm loss value: 3.191962E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 116000, match long value: -0.0007694121960451051 | match short value: -0.03494105652753361 
----------------------------------------------------------------------------------------------------------
 iteration   116100/  200000 | consumed samples:      3715200 | elapsed time per iteration (ms): 270.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.278122E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.80 | backward-params-all-reduce: 64.73 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.51 | optimizer-unscale-and-check-inf: 16.90 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.64 | batch-generator: 14.12
 iteration   116200/  200000 | consumed samples:      3718400 | elapsed time per iteration (ms): 260.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.272623E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.49 | backward-params-all-reduce: 64.41 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.03 | optimizer-unscale-and-check-inf: 16.40 | optimizer-clip-main-grad: 7.73 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 98.82 | batch-generator: 8.71
 iteration   116300/  200000 | consumed samples:      3721600 | elapsed time per iteration (ms): 260.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.284872E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.58 | backward-params-all-reduce: 63.99 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.28 | optimizer-unscale-and-check-inf: 16.82 | optimizer-clip-main-grad: 7.53 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.24 | batch-generator: 8.54
 iteration   116400/  200000 | consumed samples:      3724800 | elapsed time per iteration (ms): 258.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.295840E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.25 | backward-params-all-reduce: 64.61 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.22 | optimizer-unscale-and-check-inf: 17.51 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 100.01 | batch-generator: 8.67
 iteration   116500/  200000 | consumed samples:      3728000 | elapsed time per iteration (ms): 257.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.276847E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.69 | backward-params-all-reduce: 63.33 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.13 | optimizer-unscale-and-check-inf: 17.43 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.54 | optimizer: 99.80 | batch-generator: 8.77
--------------------------------------------------------------------------------------------------
 validation loss at iteration 116500 | lm loss value: 3.227561E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 116500, match long value: -0.002852223612561165 | match short value: -0.04046094788110339 
---------------------------------------------------------------------------------------------------------
 iteration   116600/  200000 | consumed samples:      3731200 | elapsed time per iteration (ms): 275.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.295268E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.32 | backward-params-all-reduce: 65.37 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.89 | optimizer-unscale-and-check-inf: 17.55 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.69 | batch-generator: 18.32
 iteration   116700/  200000 | consumed samples:      3734400 | elapsed time per iteration (ms): 270.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.292630E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.67 | backward-params-all-reduce: 65.27 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.02 | optimizer-unscale-and-check-inf: 16.33 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 14.25 | optimizer: 100.38 | batch-generator: 8.58
 iteration   116800/  200000 | consumed samples:      3737600 | elapsed time per iteration (ms): 261.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.288359E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.41 | backward-params-all-reduce: 64.21 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.06 | optimizer-unscale-and-check-inf: 16.95 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.23 | batch-generator: 8.60
 iteration   116900/  200000 | consumed samples:      3740800 | elapsed time per iteration (ms): 259.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.274489E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.00 | backward-params-all-reduce: 65.21 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.26 | optimizer-unscale-and-check-inf: 16.19 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 98.84 | batch-generator: 8.64
 iteration   117000/  200000 | consumed samples:      3744000 | elapsed time per iteration (ms): 258.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.307365E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.89 | backward-params-all-reduce: 65.12 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.98 | optimizer-unscale-and-check-inf: 16.67 | optimizer-clip-main-grad: 7.71 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.00 | batch-generator: 8.69
--------------------------------------------------------------------------------------------------
 validation loss at iteration 117000 | lm loss value: 3.269091E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 117000, match long value: 0.002332393724669182 | match short value: -0.011603813383053307 
---------------------------------------------------------------------------------------------------------
 iteration   117100/  200000 | consumed samples:      3747200 | elapsed time per iteration (ms): 271.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.306382E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.47 | backward-params-all-reduce: 64.94 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.61 | optimizer-unscale-and-check-inf: 16.30 | optimizer-clip-main-grad: 7.54 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.12 | batch-generator: 14.53
 iteration   117200/  200000 | consumed samples:      3750400 | elapsed time per iteration (ms): 257.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.290820E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.36 | backward-params-all-reduce: 63.64 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.25 | optimizer-unscale-and-check-inf: 16.87 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 99.37 | batch-generator: 8.48
 iteration   117300/  200000 | consumed samples:      3753600 | elapsed time per iteration (ms): 258.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.293185E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.22 | backward-params-all-reduce: 63.16 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.84 | optimizer-unscale-and-check-inf: 17.67 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 99.69 | batch-generator: 8.59
 iteration   117400/  200000 | consumed samples:      3756800 | elapsed time per iteration (ms): 261.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.306224E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.42 | backward-params-all-reduce: 64.34 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.03 | optimizer-unscale-and-check-inf: 16.50 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 98.91 | batch-generator: 8.66
 iteration   117500/  200000 | consumed samples:      3760000 | elapsed time per iteration (ms): 269.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.303178E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.50 | backward-params-all-reduce: 64.15 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 14.02 | optimizer-unscale-and-check-inf: 15.49 | optimizer-clip-main-grad: 7.54 | optimizer-copy-main-to-model-params: 14.24 | optimizer: 99.98 | batch-generator: 8.78
--------------------------------------------------------------------------------------------------
 validation loss at iteration 117500 | lm loss value: 3.223013E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 117500, match long value: 0.0057898297805157066 | match short value: -0.015183291259676988 
----------------------------------------------------------------------------------------------------------
 iteration   117600/  200000 | consumed samples:      3763200 | elapsed time per iteration (ms): 267.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.303742E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.10 | backward-params-all-reduce: 63.70 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.83 | optimizer-unscale-and-check-inf: 16.27 | optimizer-clip-main-grad: 7.47 | optimizer-copy-main-to-model-params: 12.36 | optimizer: 97.64 | batch-generator: 13.88
 iteration   117700/  200000 | consumed samples:      3766400 | elapsed time per iteration (ms): 258.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.326057E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.60 | backward-params-all-reduce: 66.97 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.70 | optimizer-unscale-and-check-inf: 15.74 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 97.70 | batch-generator: 8.66
 iteration   117800/  200000 | consumed samples:      3769600 | elapsed time per iteration (ms): 257.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.289525E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.09 | backward-params-all-reduce: 63.62 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.47 | optimizer-unscale-and-check-inf: 15.98 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.68 | batch-generator: 8.72
 iteration   117900/  200000 | consumed samples:      3772800 | elapsed time per iteration (ms): 256.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.315488E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.16 | backward-params-all-reduce: 63.46 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.04 | optimizer-unscale-and-check-inf: 16.49 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 98.77 | batch-generator: 8.51
 iteration   118000/  200000 | consumed samples:      3776000 | elapsed time per iteration (ms): 257.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.307519E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.15 | backward-params-all-reduce: 63.19 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.87 | optimizer-unscale-and-check-inf: 16.70 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.76 | batch-generator: 8.65
--------------------------------------------------------------------------------------------------
 validation loss at iteration 118000 | lm loss value: 3.325253E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 118000, match long value: -0.00787014147906266 | match short value: -0.05488757306234626 
--------------------------------------------------------------------------------------------------------
 iteration   118100/  200000 | consumed samples:      3779200 | elapsed time per iteration (ms): 273.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.314167E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.08 | backward-params-all-reduce: 65.53 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.10 | optimizer-unscale-and-check-inf: 16.35 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.74 | batch-generator: 18.61
 iteration   118200/  200000 | consumed samples:      3782400 | elapsed time per iteration (ms): 272.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.310967E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.86 | backward-params-all-reduce: 65.56 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.85 | optimizer-unscale-and-check-inf: 15.78 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 14.20 | optimizer: 99.52 | batch-generator: 8.62
 iteration   118300/  200000 | consumed samples:      3785600 | elapsed time per iteration (ms): 258.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.311481E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.16 | backward-params-all-reduce: 64.15 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.40 | optimizer-unscale-and-check-inf: 17.01 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.64 | batch-generator: 8.49
 iteration   118400/  200000 | consumed samples:      3788800 | elapsed time per iteration (ms): 258.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.332829E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.28 | backward-params-all-reduce: 64.04 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.71 | optimizer-unscale-and-check-inf: 15.77 | optimizer-clip-main-grad: 7.50 | optimizer-copy-main-to-model-params: 12.38 | optimizer: 98.03 | batch-generator: 8.54
 iteration   118500/  200000 | consumed samples:      3792000 | elapsed time per iteration (ms): 257.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.320137E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.06 | backward-params-all-reduce: 64.40 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.00 | optimizer-unscale-and-check-inf: 17.06 | optimizer-clip-main-grad: 7.53 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 99.18 | batch-generator: 8.49
--------------------------------------------------------------------------------------------------
 validation loss at iteration 118500 | lm loss value: 3.084537E-05 | lm loss PPL: 1.000031E+00 | 
 at iteration 118500, match long value: 0.0003388876710626328 | match short value: -0.023448045774907375 
----------------------------------------------------------------------------------------------------------
 iteration   118600/  200000 | consumed samples:      3795200 | elapsed time per iteration (ms): 270.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.327950E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.23 | backward-params-all-reduce: 64.02 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.73 | optimizer-unscale-and-check-inf: 16.89 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.93 | batch-generator: 15.29
 iteration   118700/  200000 | consumed samples:      3798400 | elapsed time per iteration (ms): 258.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.313561E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.44 | backward-params-all-reduce: 62.93 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 14.85 | optimizer-unscale-and-check-inf: 15.14 | optimizer-clip-main-grad: 7.70 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 99.41 | batch-generator: 8.71
 iteration   118800/  200000 | consumed samples:      3801600 | elapsed time per iteration (ms): 264.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.300469E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.66 | backward-params-all-reduce: 63.82 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.74 | optimizer-unscale-and-check-inf: 17.03 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.16 | batch-generator: 14.59
 iteration   118900/  200000 | consumed samples:      3804800 | elapsed time per iteration (ms): 273.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.261094E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.00 | backward-params-all-reduce: 64.69 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.29 | optimizer-unscale-and-check-inf: 17.39 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 14.27 | optimizer: 100.71 | batch-generator: 8.58
 iteration   119000/  200000 | consumed samples:      3808000 | elapsed time per iteration (ms): 261.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.270238E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.34 | backward-params-all-reduce: 64.03 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.00 | optimizer-unscale-and-check-inf: 16.43 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 98.68 | batch-generator: 8.46
--------------------------------------------------------------------------------------------------
 validation loss at iteration 119000 | lm loss value: 3.354798E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 119000, match long value: -5.761144733360288e-05 | match short value: -0.03043552113199402 
----------------------------------------------------------------------------------------------------------
 iteration   119100/  200000 | consumed samples:      3811200 | elapsed time per iteration (ms): 272.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.284743E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.30 | backward-params-all-reduce: 63.80 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.67 | optimizer-unscale-and-check-inf: 17.29 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.24 | batch-generator: 16.00
 iteration   119200/  200000 | consumed samples:      3814400 | elapsed time per iteration (ms): 259.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.267909E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.26 | backward-params-all-reduce: 63.25 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.84 | optimizer-unscale-and-check-inf: 16.60 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.68 | batch-generator: 8.56
 iteration   119300/  200000 | consumed samples:      3817600 | elapsed time per iteration (ms): 259.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.273216E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.08 | backward-params-all-reduce: 63.48 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.85 | optimizer-unscale-and-check-inf: 17.28 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.40 | batch-generator: 8.57
 iteration   119400/  200000 | consumed samples:      3820800 | elapsed time per iteration (ms): 258.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.268731E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.84 | backward-params-all-reduce: 63.67 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.78 | optimizer-unscale-and-check-inf: 16.21 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.21 | batch-generator: 8.55
 iteration   119500/  200000 | consumed samples:      3824000 | elapsed time per iteration (ms): 262.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.264674E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.29 | backward-params-all-reduce: 64.31 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.87 | optimizer-unscale-and-check-inf: 17.57 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 99.75 | batch-generator: 8.53
--------------------------------------------------------------------------------------------------
 validation loss at iteration 119500 | lm loss value: 3.274368E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 119500, match long value: -0.0013932831899540728 | match short value: -0.03838601900769208 
----------------------------------------------------------------------------------------------------------
 iteration   119600/  200000 | consumed samples:      3827200 | elapsed time per iteration (ms): 281.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.266097E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.92 | backward-params-all-reduce: 64.22 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.97 | optimizer-unscale-and-check-inf: 16.61 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 98.84 | batch-generator: 21.19
 iteration   119700/  200000 | consumed samples:      3830400 | elapsed time per iteration (ms): 274.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.261279E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.73 | backward-params-all-reduce: 65.79 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.30 | optimizer-unscale-and-check-inf: 18.20 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 14.22 | optimizer: 101.50 | batch-generator: 8.55
 iteration   119800/  200000 | consumed samples:      3833600 | elapsed time per iteration (ms): 261.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.283129E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.38 | backward-params-all-reduce: 65.48 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.60 | optimizer-unscale-and-check-inf: 17.34 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.16 | batch-generator: 8.54
 iteration   119900/  200000 | consumed samples:      3836800 | elapsed time per iteration (ms): 257.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.265670E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.48 | backward-params-all-reduce: 62.55 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.40 | optimizer-unscale-and-check-inf: 16.15 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 97.83 | batch-generator: 8.63
 iteration   120000/  200000 | consumed samples:      3840000 | elapsed time per iteration (ms): 259.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.283703E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.31 | backward-params-all-reduce: 63.19 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.87 | optimizer-unscale-and-check-inf: 16.21 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 98.39 | batch-generator: 8.68
--------------------------------------------------------------------------------------------------
 validation loss at iteration 120000 | lm loss value: 3.256853E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 120000, match long value: 0.0003138016018267254 | match short value: -0.034741873362200394 
----------------------------------------------------------------------------------------------------------
saving checkpoint at iteration  120000 to verify1
  successfully saved checkpoint at iteration  120000 to verify1
time (ms) | save-checkpoint: 25064.17
 iteration   120100/  200000 | consumed samples:      3843200 | elapsed time per iteration (ms): 525.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.300626E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.81 | backward-params-all-reduce: 64.92 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.64 | optimizer-unscale-and-check-inf: 16.84 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.45 | optimizer: 98.62 | batch-generator: 15.98
 iteration   120200/  200000 | consumed samples:      3846400 | elapsed time per iteration (ms): 259.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.295080E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.93 | backward-params-all-reduce: 62.99 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.10 | optimizer-unscale-and-check-inf: 15.99 | optimizer-clip-main-grad: 7.75 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.51 | batch-generator: 8.70
 iteration   120300/  200000 | consumed samples:      3849600 | elapsed time per iteration (ms): 260.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.280798E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.21 | backward-params-all-reduce: 63.76 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.15 | optimizer-unscale-and-check-inf: 17.02 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 98.43 | batch-generator: 8.60
 iteration   120400/  200000 | consumed samples:      3852800 | elapsed time per iteration (ms): 275.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.313214E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.08 | backward-params-all-reduce: 64.39 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.22 | optimizer-unscale-and-check-inf: 15.77 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 14.29 | optimizer: 99.02 | batch-generator: 8.55
 iteration   120500/  200000 | consumed samples:      3856000 | elapsed time per iteration (ms): 259.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.301099E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.08 | backward-params-all-reduce: 64.41 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.65 | optimizer-unscale-and-check-inf: 16.13 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 98.13 | batch-generator: 8.58
--------------------------------------------------------------------------------------------------
 validation loss at iteration 120500 | lm loss value: 3.276315E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 120500, match long value: -0.002564732161458318 | match short value: -0.04996245424878945 
---------------------------------------------------------------------------------------------------------
 iteration   120600/  200000 | consumed samples:      3859200 | elapsed time per iteration (ms): 272.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.293548E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.18 | backward-params-all-reduce: 64.70 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.31 | optimizer-unscale-and-check-inf: 16.44 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.03 | batch-generator: 16.11
 iteration   120700/  200000 | consumed samples:      3862400 | elapsed time per iteration (ms): 259.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.310259E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.92 | backward-params-all-reduce: 63.24 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.57 | optimizer-unscale-and-check-inf: 16.17 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 97.99 | batch-generator: 8.52
 iteration   120800/  200000 | consumed samples:      3865600 | elapsed time per iteration (ms): 258.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.307032E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.24 | backward-params-all-reduce: 63.30 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.17 | optimizer-unscale-and-check-inf: 15.82 | optimizer-clip-main-grad: 7.54 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 97.17 | batch-generator: 8.62
 iteration   120900/  200000 | consumed samples:      3868800 | elapsed time per iteration (ms): 259.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.306473E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.09 | backward-params-all-reduce: 63.81 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.64 | optimizer-unscale-and-check-inf: 15.65 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 97.57 | batch-generator: 8.54
 iteration   121000/  200000 | consumed samples:      3872000 | elapsed time per iteration (ms): 256.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.305799E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.42 | backward-params-all-reduce: 64.16 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.23 | optimizer-unscale-and-check-inf: 14.48 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 95.93 | batch-generator: 8.59
--------------------------------------------------------------------------------------------------
 validation loss at iteration 121000 | lm loss value: 3.280175E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 121000, match long value: -0.0019306284912924603 | match short value: -0.04879053125056253 
----------------------------------------------------------------------------------------------------------
 iteration   121100/  200000 | consumed samples:      3875200 | elapsed time per iteration (ms): 290.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.298829E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.74 | backward-params-all-reduce: 65.31 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.35 | optimizer-unscale-and-check-inf: 15.26 | optimizer-clip-main-grad: 7.54 | optimizer-copy-main-to-model-params: 14.18 | optimizer: 98.49 | batch-generator: 20.79
 iteration   121200/  200000 | consumed samples:      3878400 | elapsed time per iteration (ms): 261.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.314839E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.45 | backward-params-all-reduce: 63.15 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.49 | optimizer-unscale-and-check-inf: 15.41 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 97.13 | batch-generator: 8.50
 iteration   121300/  200000 | consumed samples:      3881600 | elapsed time per iteration (ms): 258.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.309939E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.06 | backward-params-all-reduce: 63.69 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 12.38 | optimizer-unscale-and-check-inf: 15.78 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 97.35 | batch-generator: 8.48
 iteration   121400/  200000 | consumed samples:      3884800 | elapsed time per iteration (ms): 257.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.308580E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.14 | backward-params-all-reduce: 63.80 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.69 | optimizer-unscale-and-check-inf: 15.47 | optimizer-clip-main-grad: 7.48 | optimizer-copy-main-to-model-params: 12.25 | optimizer: 96.08 | batch-generator: 8.62
 iteration   121500/  200000 | consumed samples:      3888000 | elapsed time per iteration (ms): 256.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.328717E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.18 | backward-params-all-reduce: 63.39 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.25 | optimizer-unscale-and-check-inf: 15.09 | optimizer-clip-main-grad: 7.43 | optimizer-copy-main-to-model-params: 12.22 | optimizer: 95.16 | batch-generator: 8.52
--------------------------------------------------------------------------------------------------
 validation loss at iteration 121500 | lm loss value: 3.277744E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 121500, match long value: -0.0008930133174043189 | match short value: -0.032484703078689885 
-----------------------------------------------------------------------------------------------------------
 iteration   121600/  200000 | consumed samples:      3891200 | elapsed time per iteration (ms): 271.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.314268E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.04 | backward-params-all-reduce: 63.35 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.84 | optimizer-unscale-and-check-inf: 15.15 | optimizer-clip-main-grad: 7.54 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 97.19 | batch-generator: 15.54
 iteration   121700/  200000 | consumed samples:      3894400 | elapsed time per iteration (ms): 258.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.309185E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.20 | backward-params-all-reduce: 64.35 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.48 | optimizer-unscale-and-check-inf: 15.01 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 96.70 | batch-generator: 8.64
 iteration   121800/  200000 | consumed samples:      3897600 | elapsed time per iteration (ms): 261.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.331000E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.53 | backward-params-all-reduce: 63.31 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.53 | optimizer-unscale-and-check-inf: 14.67 | optimizer-clip-main-grad: 7.54 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 96.39 | batch-generator: 8.64
 iteration   121900/  200000 | consumed samples:      3900800 | elapsed time per iteration (ms): 276.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.311786E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.77 | backward-params-all-reduce: 63.83 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.25 | optimizer-unscale-and-check-inf: 16.97 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 14.24 | optimizer: 100.33 | batch-generator: 14.41
 iteration   122000/  200000 | consumed samples:      3904000 | elapsed time per iteration (ms): 260.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.264720E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.26 | backward-params-all-reduce: 63.89 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.34 | optimizer-unscale-and-check-inf: 16.63 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.25 | batch-generator: 8.68
--------------------------------------------------------------------------------------------------
 validation loss at iteration 122000 | lm loss value: 3.147273E-05 | lm loss PPL: 1.000031E+00 | 
 at iteration 122000, match long value: 0.0031314648969407418 | match short value: -0.01978766078697756 
---------------------------------------------------------------------------------------------------------
 iteration   122100/  200000 | consumed samples:      3907200 | elapsed time per iteration (ms): 271.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.265085E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.44 | backward-params-all-reduce: 63.54 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.60 | optimizer-unscale-and-check-inf: 16.56 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.45 | batch-generator: 18.23
 iteration   122200/  200000 | consumed samples:      3910400 | elapsed time per iteration (ms): 260.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.268030E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.60 | backward-params-all-reduce: 64.59 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.01 | optimizer-unscale-and-check-inf: 17.45 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.68 | batch-generator: 8.62
 iteration   122300/  200000 | consumed samples:      3913600 | elapsed time per iteration (ms): 260.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.258812E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.19 | backward-params-all-reduce: 62.98 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.94 | optimizer-unscale-and-check-inf: 17.03 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.19 | batch-generator: 8.57
 iteration   122400/  200000 | consumed samples:      3916800 | elapsed time per iteration (ms): 259.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.253050E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.25 | backward-params-all-reduce: 65.36 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.24 | optimizer-unscale-and-check-inf: 16.16 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 97.58 | batch-generator: 8.65
 iteration   122500/  200000 | consumed samples:      3920000 | elapsed time per iteration (ms): 259.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.254072E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.56 | backward-params-all-reduce: 63.54 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.79 | optimizer-unscale-and-check-inf: 16.38 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.39 | batch-generator: 8.61
--------------------------------------------------------------------------------------------------
 validation loss at iteration 122500 | lm loss value: 3.277826E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 122500, match long value: -0.002032298308203576 | match short value: -0.0319471326751037 
--------------------------------------------------------------------------------------------------------
 iteration   122600/  200000 | consumed samples:      3923200 | elapsed time per iteration (ms): 292.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.260124E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.02 | backward-params-all-reduce: 63.68 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.27 | optimizer-unscale-and-check-inf: 17.09 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 14.27 | optimizer: 100.44 | batch-generator: 20.74
 iteration   122700/  200000 | consumed samples:      3926400 | elapsed time per iteration (ms): 258.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.254923E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.08 | backward-params-all-reduce: 63.36 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.53 | optimizer-unscale-and-check-inf: 17.71 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.52 | batch-generator: 8.64
 iteration   122800/  200000 | consumed samples:      3929600 | elapsed time per iteration (ms): 260.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.268511E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.12 | backward-params-all-reduce: 63.42 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.50 | optimizer-unscale-and-check-inf: 17.03 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.75 | batch-generator: 8.59
 iteration   122900/  200000 | consumed samples:      3932800 | elapsed time per iteration (ms): 260.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.283466E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.84 | backward-params-all-reduce: 63.92 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.31 | optimizer-unscale-and-check-inf: 17.26 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.82 | batch-generator: 8.60
 iteration   123000/  200000 | consumed samples:      3936000 | elapsed time per iteration (ms): 260.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.310157E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.15 | backward-params-all-reduce: 64.04 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.60 | optimizer-unscale-and-check-inf: 16.26 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 12.62 | optimizer: 98.35 | batch-generator: 8.80
--------------------------------------------------------------------------------------------------
 validation loss at iteration 123000 | lm loss value: 3.190754E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 123000, match long value: -0.0034291188006241417 | match short value: -0.02964743008826257 
----------------------------------------------------------------------------------------------------------
 iteration   123100/  200000 | consumed samples:      3939200 | elapsed time per iteration (ms): 270.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.276814E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.09 | backward-params-all-reduce: 63.13 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.24 | optimizer-unscale-and-check-inf: 18.16 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 99.72 | batch-generator: 15.91
 iteration   123200/  200000 | consumed samples:      3942400 | elapsed time per iteration (ms): 260.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.279616E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.42 | backward-params-all-reduce: 63.18 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.29 | optimizer-unscale-and-check-inf: 18.84 | optimizer-clip-main-grad: 7.73 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 100.53 | batch-generator: 8.63
 iteration   123300/  200000 | consumed samples:      3945600 | elapsed time per iteration (ms): 275.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.278558E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.70 | backward-params-all-reduce: 64.73 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.05 | optimizer-unscale-and-check-inf: 20.00 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 14.36 | optimizer: 103.22 | batch-generator: 8.53
 iteration   123400/  200000 | consumed samples:      3948800 | elapsed time per iteration (ms): 262.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.295152E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.14 | backward-params-all-reduce: 63.82 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.12 | optimizer-unscale-and-check-inf: 19.81 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 101.22 | batch-generator: 8.48
 iteration   123500/  200000 | consumed samples:      3952000 | elapsed time per iteration (ms): 331.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.278414E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.95 | backward-params-all-reduce: 62.96 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.14 | optimizer-unscale-and-check-inf: 18.52 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.94 | batch-generator: 81.75
--------------------------------------------------------------------------------------------------
 validation loss at iteration 123500 | lm loss value: 3.335043E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 123500, match long value: -0.0014244538911128374 | match short value: -0.017722280646010686 
-----------------------------------------------------------------------------------------------------------
 iteration   123600/  200000 | consumed samples:      3955200 | elapsed time per iteration (ms): 270.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.312717E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.12 | backward-params-all-reduce: 63.68 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.22 | optimizer-unscale-and-check-inf: 18.78 | optimizer-clip-main-grad: 7.51 | optimizer-copy-main-to-model-params: 12.32 | optimizer: 99.01 | batch-generator: 16.36
 iteration   123700/  200000 | consumed samples:      3958400 | elapsed time per iteration (ms): 261.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.274206E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.11 | backward-params-all-reduce: 65.57 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.06 | optimizer-unscale-and-check-inf: 18.74 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 100.12 | batch-generator: 8.71
 iteration   123800/  200000 | consumed samples:      3961600 | elapsed time per iteration (ms): 260.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.302311E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.40 | backward-params-all-reduce: 62.87 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.14 | optimizer-unscale-and-check-inf: 19.32 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 100.80 | batch-generator: 8.60
 iteration   123900/  200000 | consumed samples:      3964800 | elapsed time per iteration (ms): 259.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.311155E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.94 | backward-params-all-reduce: 63.38 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.02 | optimizer-unscale-and-check-inf: 18.85 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 100.22 | batch-generator: 8.66
 iteration   124000/  200000 | consumed samples:      3968000 | elapsed time per iteration (ms): 265.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.289638E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.52 | backward-params-all-reduce: 64.69 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.09 | optimizer-unscale-and-check-inf: 18.13 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.47 | batch-generator: 8.65
--------------------------------------------------------------------------------------------------
 validation loss at iteration 124000 | lm loss value: 3.278172E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 124000, match long value: -0.001621450137163022 | match short value: -0.023348109939053376 
----------------------------------------------------------------------------------------------------------
 iteration   124100/  200000 | consumed samples:      3971200 | elapsed time per iteration (ms): 278.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.300376E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.41 | backward-params-all-reduce: 63.45 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.89 | optimizer-unscale-and-check-inf: 17.11 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.55 | optimizer: 100.66 | batch-generator: 14.50
 iteration   124200/  200000 | consumed samples:      3974400 | elapsed time per iteration (ms): 258.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.305835E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.14 | backward-params-all-reduce: 63.12 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.69 | optimizer-unscale-and-check-inf: 17.39 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 99.23 | batch-generator: 8.47
 iteration   124300/  200000 | consumed samples:      3977600 | elapsed time per iteration (ms): 259.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.308074E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.09 | backward-params-all-reduce: 64.60 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.45 | optimizer-unscale-and-check-inf: 17.65 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 99.39 | batch-generator: 8.57
 iteration   124400/  200000 | consumed samples:      3980800 | elapsed time per iteration (ms): 259.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.305531E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.07 | backward-params-all-reduce: 64.20 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.53 | optimizer-unscale-and-check-inf: 18.40 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 100.24 | batch-generator: 8.56
 iteration   124500/  200000 | consumed samples:      3984000 | elapsed time per iteration (ms): 260.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.316787E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.14 | backward-params-all-reduce: 64.41 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.68 | optimizer-unscale-and-check-inf: 18.55 | optimizer-clip-main-grad: 7.70 | optimizer-copy-main-to-model-params: 12.58 | optimizer: 100.67 | batch-generator: 8.66
--------------------------------------------------------------------------------------------------
 validation loss at iteration 124500 | lm loss value: 3.317308E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 124500, match long value: -0.0010940063009877703 | match short value: -0.03256848058997927 
----------------------------------------------------------------------------------------------------------
 iteration   124600/  200000 | consumed samples:      3987200 | elapsed time per iteration (ms): 276.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.315009E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.14 | backward-params-all-reduce: 63.64 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.60 | optimizer-unscale-and-check-inf: 17.77 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 99.70 | batch-generator: 22.08
 iteration   124700/  200000 | consumed samples:      3990400 | elapsed time per iteration (ms): 260.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.326234E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.82 | backward-params-all-reduce: 64.96 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.95 | optimizer-unscale-and-check-inf: 17.53 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.77 | batch-generator: 8.58
 iteration   124800/  200000 | consumed samples:      3993600 | elapsed time per iteration (ms): 272.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.315939E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.21 | backward-params-all-reduce: 63.87 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.39 | optimizer-unscale-and-check-inf: 17.68 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 14.17 | optimizer: 101.00 | batch-generator: 8.57
 iteration   124900/  200000 | consumed samples:      3996800 | elapsed time per iteration (ms): 260.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.315853E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.39 | backward-params-all-reduce: 64.62 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.03 | optimizer-unscale-and-check-inf: 16.80 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.55 | optimizer: 99.11 | batch-generator: 8.35
 iteration   125000/  200000 | consumed samples:      4000000 | elapsed time per iteration (ms): 258.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.326341E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.18 | backward-params-all-reduce: 63.88 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.48 | optimizer-unscale-and-check-inf: 16.90 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.65 | batch-generator: 8.52
--------------------------------------------------------------------------------------------------
 validation loss at iteration 125000 | lm loss value: 3.161733E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 125000, match long value: 0.0006410512989609748 | match short value: -0.032284722129096145 
----------------------------------------------------------------------------------------------------------
 iteration   125100/  200000 | consumed samples:      4003200 | elapsed time per iteration (ms): 277.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.251957E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.70 | backward-params-all-reduce: 65.09 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.23 | optimizer-unscale-and-check-inf: 17.04 | optimizer-clip-main-grad: 7.54 | optimizer-copy-main-to-model-params: 12.35 | optimizer: 98.83 | batch-generator: 19.88
 iteration   125200/  200000 | consumed samples:      4006400 | elapsed time per iteration (ms): 261.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.260733E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.28 | backward-params-all-reduce: 64.73 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.90 | optimizer-unscale-and-check-inf: 16.69 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.80 | batch-generator: 8.40
 iteration   125300/  200000 | consumed samples:      4009600 | elapsed time per iteration (ms): 257.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.250541E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.24 | backward-params-all-reduce: 63.94 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.07 | optimizer-unscale-and-check-inf: 16.54 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 98.86 | batch-generator: 8.46
 iteration   125400/  200000 | consumed samples:      4012800 | elapsed time per iteration (ms): 260.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.258280E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.76 | backward-params-all-reduce: 64.32 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.31 | optimizer-unscale-and-check-inf: 16.81 | optimizer-clip-main-grad: 7.52 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 99.23 | batch-generator: 8.34
 iteration   125500/  200000 | consumed samples:      4016000 | elapsed time per iteration (ms): 268.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.256207E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.93 | backward-params-all-reduce: 63.92 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.23 | optimizer-unscale-and-check-inf: 17.08 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 14.23 | optimizer: 101.27 | batch-generator: 8.38
--------------------------------------------------------------------------------------------------
 validation loss at iteration 125500 | lm loss value: 3.259300E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 125500, match long value: 0.0040422561220227905 | match short value: -0.009968120089985032 
----------------------------------------------------------------------------------------------------------
 iteration   125600/  200000 | consumed samples:      4019200 | elapsed time per iteration (ms): 269.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.253681E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.45 | backward-params-all-reduce: 63.29 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.80 | optimizer-unscale-and-check-inf: 17.67 | optimizer-clip-main-grad: 7.52 | optimizer-copy-main-to-model-params: 12.31 | optimizer: 98.94 | batch-generator: 15.68
 iteration   125700/  200000 | consumed samples:      4022400 | elapsed time per iteration (ms): 257.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.269508E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.19 | backward-params-all-reduce: 63.30 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.51 | optimizer-unscale-and-check-inf: 17.78 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.52 | batch-generator: 8.59
 iteration   125800/  200000 | consumed samples:      4025600 | elapsed time per iteration (ms): 259.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.253410E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.26 | backward-params-all-reduce: 64.48 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.08 | optimizer-unscale-and-check-inf: 17.22 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.58 | batch-generator: 8.60
 iteration   125900/  200000 | consumed samples:      4028800 | elapsed time per iteration (ms): 258.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.265722E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.65 | backward-params-all-reduce: 63.43 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.23 | optimizer-unscale-and-check-inf: 16.68 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.17 | batch-generator: 8.52
 iteration   126000/  200000 | consumed samples:      4032000 | elapsed time per iteration (ms): 258.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.274034E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.96 | backward-params-all-reduce: 64.69 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.64 | optimizer-unscale-and-check-inf: 16.87 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.74 | batch-generator: 8.52
--------------------------------------------------------------------------------------------------
 validation loss at iteration 126000 | lm loss value: 3.270606E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 126000, match long value: -0.00048061698572414717 | match short value: 0.00036625289441696656 
-------------------------------------------------------------------------------------------------------------
 iteration   126100/  200000 | consumed samples:      4035200 | elapsed time per iteration (ms): 276.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.272234E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.63 | backward-params-all-reduce: 63.68 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.94 | optimizer-unscale-and-check-inf: 16.96 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.14 | batch-generator: 21.64
 iteration   126200/  200000 | consumed samples:      4038400 | elapsed time per iteration (ms): 266.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.287900E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.79 | backward-params-all-reduce: 63.78 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.70 | optimizer-unscale-and-check-inf: 16.60 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 14.40 | optimizer: 101.57 | batch-generator: 8.66
 iteration   126300/  200000 | consumed samples:      4041600 | elapsed time per iteration (ms): 266.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.266283E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.51 | backward-params-all-reduce: 63.12 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.61 | optimizer-unscale-and-check-inf: 16.60 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.41 | batch-generator: 8.69
 iteration   126400/  200000 | consumed samples:      4044800 | elapsed time per iteration (ms): 259.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.280166E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.03 | backward-params-all-reduce: 65.10 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.86 | optimizer-unscale-and-check-inf: 17.68 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 99.81 | batch-generator: 8.64
 iteration   126500/  200000 | consumed samples:      4048000 | elapsed time per iteration (ms): 259.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.288531E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.17 | backward-params-all-reduce: 63.90 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.36 | optimizer-unscale-and-check-inf: 16.04 | optimizer-clip-main-grad: 7.71 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.76 | batch-generator: 8.80
--------------------------------------------------------------------------------------------------
 validation loss at iteration 126500 | lm loss value: 3.272185E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 126500, match long value: -0.00027992658437930564 | match short value: -0.041682384545124074 
------------------------------------------------------------------------------------------------------------
 iteration   126600/  200000 | consumed samples:      4051200 | elapsed time per iteration (ms): 272.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.278959E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.38 | backward-params-all-reduce: 64.21 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.57 | optimizer-unscale-and-check-inf: 16.62 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.46 | batch-generator: 15.35
 iteration   126700/  200000 | consumed samples:      4054400 | elapsed time per iteration (ms): 261.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.274369E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.07 | backward-params-all-reduce: 64.06 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.27 | optimizer-unscale-and-check-inf: 17.75 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.36 | batch-generator: 8.62
 iteration   126800/  200000 | consumed samples:      4057600 | elapsed time per iteration (ms): 260.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.309648E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.27 | backward-params-all-reduce: 63.84 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.72 | optimizer-unscale-and-check-inf: 17.58 | optimizer-clip-main-grad: 7.73 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.69 | batch-generator: 8.65
 iteration   126900/  200000 | consumed samples:      4060800 | elapsed time per iteration (ms): 260.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.288285E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.18 | backward-params-all-reduce: 64.06 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.25 | optimizer-unscale-and-check-inf: 17.54 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 12.54 | optimizer: 99.16 | batch-generator: 8.65
 iteration   127000/  200000 | consumed samples:      4064000 | elapsed time per iteration (ms): 276.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.291156E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.24 | backward-params-all-reduce: 65.83 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.39 | optimizer-unscale-and-check-inf: 16.63 | optimizer-clip-main-grad: 7.71 | optimizer-copy-main-to-model-params: 14.37 | optimizer: 100.27 | batch-generator: 8.77
--------------------------------------------------------------------------------------------------
 validation loss at iteration 127000 | lm loss value: 3.293006E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 127000, match long value: -0.001548373160891493 | match short value: -0.022526544430355004 
----------------------------------------------------------------------------------------------------------
 iteration   127100/  200000 | consumed samples:      4067200 | elapsed time per iteration (ms): 272.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.298073E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.23 | backward-params-all-reduce: 62.81 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.09 | optimizer-unscale-and-check-inf: 17.43 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.77 | batch-generator: 16.06
 iteration   127200/  200000 | consumed samples:      4070400 | elapsed time per iteration (ms): 259.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.291467E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.42 | backward-params-all-reduce: 63.66 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.03 | optimizer-unscale-and-check-inf: 17.26 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 98.52 | batch-generator: 8.61
 iteration   127300/  200000 | consumed samples:      4073600 | elapsed time per iteration (ms): 260.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.317723E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.22 | backward-params-all-reduce: 63.70 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.62 | optimizer-unscale-and-check-inf: 18.27 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.37 | optimizer: 99.51 | batch-generator: 8.67
 iteration   127400/  200000 | consumed samples:      4076800 | elapsed time per iteration (ms): 259.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.323703E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.40 | backward-params-all-reduce: 62.85 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.17 | optimizer-unscale-and-check-inf: 17.80 | optimizer-clip-main-grad: 7.50 | optimizer-copy-main-to-model-params: 12.34 | optimizer: 98.48 | batch-generator: 8.63
 iteration   127500/  200000 | consumed samples:      4080000 | elapsed time per iteration (ms): 259.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.323787E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.30 | backward-params-all-reduce: 62.95 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.17 | optimizer-unscale-and-check-inf: 18.55 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 100.03 | batch-generator: 8.67
--------------------------------------------------------------------------------------------------
 validation loss at iteration 127500 | lm loss value: 3.279733E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 127500, match long value: 0.000494935681925016 | match short value: 0.009676689981971841 
--------------------------------------------------------------------------------------------------------
 iteration   127600/  200000 | consumed samples:      4083200 | elapsed time per iteration (ms): 277.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.300943E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.46 | backward-params-all-reduce: 63.71 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.67 | optimizer-unscale-and-check-inf: 17.00 | optimizer-clip-main-grad: 7.86 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 99.27 | batch-generator: 20.52
 iteration   127700/  200000 | consumed samples:      4086400 | elapsed time per iteration (ms): 274.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.302558E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.35 | backward-params-all-reduce: 63.17 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.38 | optimizer-unscale-and-check-inf: 16.96 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 14.23 | optimizer: 100.51 | batch-generator: 8.75
 iteration   127800/  200000 | consumed samples:      4089600 | elapsed time per iteration (ms): 263.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.303011E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.09 | backward-params-all-reduce: 63.52 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.72 | optimizer-unscale-and-check-inf: 17.40 | optimizer-clip-main-grad: 7.78 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.57 | batch-generator: 8.74
 iteration   127900/  200000 | consumed samples:      4092800 | elapsed time per iteration (ms): 260.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.315674E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.28 | backward-params-all-reduce: 63.15 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.54 | optimizer-unscale-and-check-inf: 17.39 | optimizer-clip-main-grad: 7.75 | optimizer-copy-main-to-model-params: 12.57 | optimizer: 99.45 | batch-generator: 8.73
 iteration   128000/  200000 | consumed samples:      4096000 | elapsed time per iteration (ms): 259.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.308709E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.35 | backward-params-all-reduce: 64.01 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.27 | optimizer-unscale-and-check-inf: 17.01 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.54 | batch-generator: 8.61
--------------------------------------------------------------------------------------------------
 validation loss at iteration 128000 | lm loss value: 3.215116E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 128000, match long value: -0.0015184191289370557 | match short value: -0.034297443963964495 
-----------------------------------------------------------------------------------------------------------
 iteration   128100/  200000 | consumed samples:      4099200 | elapsed time per iteration (ms): 270.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.321316E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.18 | backward-params-all-reduce: 63.60 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.31 | optimizer-unscale-and-check-inf: 17.13 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 98.79 | batch-generator: 14.93
 iteration   128200/  200000 | consumed samples:      4102400 | elapsed time per iteration (ms): 267.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.267932E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.78 | backward-params-all-reduce: 64.19 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.83 | optimizer-unscale-and-check-inf: 17.63 | optimizer-clip-main-grad: 7.71 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.85 | batch-generator: 14.15
 iteration   128300/  200000 | consumed samples:      4105600 | elapsed time per iteration (ms): 259.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.243545E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.19 | backward-params-all-reduce: 63.34 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.72 | optimizer-unscale-and-check-inf: 17.24 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.59 | optimizer: 99.32 | batch-generator: 8.62
 iteration   128400/  200000 | consumed samples:      4108800 | elapsed time per iteration (ms): 268.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.250149E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.65 | backward-params-all-reduce: 64.36 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.50 | optimizer-unscale-and-check-inf: 16.68 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 14.23 | optimizer: 100.17 | batch-generator: 8.57
 iteration   128500/  200000 | consumed samples:      4112000 | elapsed time per iteration (ms): 267.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.247511E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.08 | backward-params-all-reduce: 64.44 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.47 | optimizer-unscale-and-check-inf: 16.47 | optimizer-clip-main-grad: 7.54 | optimizer-copy-main-to-model-params: 12.36 | optimizer: 97.50 | batch-generator: 8.45
--------------------------------------------------------------------------------------------------
 validation loss at iteration 128500 | lm loss value: 3.212409E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 128500, match long value: -0.0006774119853507489 | match short value: -0.017094631892176124 
-----------------------------------------------------------------------------------------------------------
 iteration   128600/  200000 | consumed samples:      4115200 | elapsed time per iteration (ms): 273.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.263601E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.57 | backward-params-all-reduce: 64.27 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.67 | optimizer-unscale-and-check-inf: 15.87 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 97.76 | batch-generator: 17.13
 iteration   128700/  200000 | consumed samples:      4118400 | elapsed time per iteration (ms): 261.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.258482E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.32 | backward-params-all-reduce: 63.85 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.83 | optimizer-unscale-and-check-inf: 15.45 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.54 | optimizer: 97.52 | batch-generator: 8.72
 iteration   128800/  200000 | consumed samples:      4121600 | elapsed time per iteration (ms): 259.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.270172E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.67 | backward-params-all-reduce: 63.28 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.36 | optimizer-unscale-and-check-inf: 15.74 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 97.37 | batch-generator: 8.66
 iteration   128900/  200000 | consumed samples:      4124800 | elapsed time per iteration (ms): 258.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.261279E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.03 | backward-params-all-reduce: 63.51 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.42 | optimizer-unscale-and-check-inf: 15.30 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 96.95 | batch-generator: 8.61
 iteration   129000/  200000 | consumed samples:      4128000 | elapsed time per iteration (ms): 257.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.270944E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.21 | backward-params-all-reduce: 62.11 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.64 | optimizer-unscale-and-check-inf: 16.17 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 98.12 | batch-generator: 8.58
--------------------------------------------------------------------------------------------------
 validation loss at iteration 129000 | lm loss value: 3.140470E-05 | lm loss PPL: 1.000031E+00 | 
 at iteration 129000, match long value: 0.0006059347591453024 | match short value: -0.024585812760110022 
----------------------------------------------------------------------------------------------------------
 iteration   129100/  200000 | consumed samples:      4131200 | elapsed time per iteration (ms): 277.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.270701E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.44 | backward-params-all-reduce: 64.42 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.62 | optimizer-unscale-and-check-inf: 15.68 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 97.65 | batch-generator: 19.85
 iteration   129200/  200000 | consumed samples:      4134400 | elapsed time per iteration (ms): 277.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.277421E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.69 | backward-params-all-reduce: 63.85 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.99 | optimizer-unscale-and-check-inf: 15.09 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 14.34 | optimizer: 99.14 | batch-generator: 8.65
 iteration   129300/  200000 | consumed samples:      4137600 | elapsed time per iteration (ms): 259.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.281478E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.56 | backward-params-all-reduce: 62.80 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.36 | optimizer-unscale-and-check-inf: 15.64 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 97.23 | batch-generator: 8.71
 iteration   129400/  200000 | consumed samples:      4140800 | elapsed time per iteration (ms): 260.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.266121E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.74 | backward-params-all-reduce: 64.60 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.39 | optimizer-unscale-and-check-inf: 15.68 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 97.32 | batch-generator: 8.64
 iteration   129500/  200000 | consumed samples:      4144000 | elapsed time per iteration (ms): 260.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.274621E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.53 | backward-params-all-reduce: 63.67 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.47 | optimizer-unscale-and-check-inf: 16.63 | optimizer-clip-main-grad: 7.42 | optimizer-copy-main-to-model-params: 12.23 | optimizer: 96.90 | batch-generator: 8.52
--------------------------------------------------------------------------------------------------
 validation loss at iteration 129500 | lm loss value: 3.318847E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 129500, match long value: 0.000568865054555517 | match short value: -0.015426557856432196 
---------------------------------------------------------------------------------------------------------
 iteration   129600/  200000 | consumed samples:      4147200 | elapsed time per iteration (ms): 272.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.288871E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.33 | backward-params-all-reduce: 63.38 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.39 | optimizer-unscale-and-check-inf: 15.65 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 97.38 | batch-generator: 15.50
 iteration   129700/  200000 | consumed samples:      4150400 | elapsed time per iteration (ms): 259.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.286457E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.90 | backward-params-all-reduce: 64.61 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.07 | optimizer-unscale-and-check-inf: 15.00 | optimizer-clip-main-grad: 7.50 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 96.18 | batch-generator: 8.68
 iteration   129800/  200000 | consumed samples:      4153600 | elapsed time per iteration (ms): 258.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.277335E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.86 | backward-params-all-reduce: 63.46 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.33 | optimizer-unscale-and-check-inf: 15.00 | optimizer-clip-main-grad: 7.54 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 96.49 | batch-generator: 8.56
 iteration   129900/  200000 | consumed samples:      4156800 | elapsed time per iteration (ms): 272.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.293073E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.28 | backward-params-all-reduce: 64.45 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.17 | optimizer-unscale-and-check-inf: 14.85 | optimizer-clip-main-grad: 7.52 | optimizer-copy-main-to-model-params: 14.28 | optimizer: 97.95 | batch-generator: 8.65
 iteration   130000/  200000 | consumed samples:      4160000 | elapsed time per iteration (ms): 261.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.289173E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.43 | backward-params-all-reduce: 62.66 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.66 | optimizer-unscale-and-check-inf: 16.37 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.36 | batch-generator: 8.64
--------------------------------------------------------------------------------------------------
 validation loss at iteration 130000 | lm loss value: 3.354950E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 130000, match long value: 0.0008812876618144515 | match short value: -0.02450677475485088 
---------------------------------------------------------------------------------------------------------
 iteration   130100/  200000 | consumed samples:      4163200 | elapsed time per iteration (ms): 271.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.286097E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.20 | backward-params-all-reduce: 63.94 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.39 | optimizer-unscale-and-check-inf: 15.98 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 97.65 | batch-generator: 16.93
 iteration   130200/  200000 | consumed samples:      4166400 | elapsed time per iteration (ms): 260.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.302307E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.01 | backward-params-all-reduce: 64.31 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.62 | optimizer-unscale-and-check-inf: 16.09 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.38 | optimizer: 97.30 | batch-generator: 8.62
 iteration   130300/  200000 | consumed samples:      4169600 | elapsed time per iteration (ms): 261.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.311326E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.27 | backward-params-all-reduce: 64.55 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.45 | optimizer-unscale-and-check-inf: 16.75 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 98.51 | batch-generator: 8.63
 iteration   130400/  200000 | consumed samples:      4172800 | elapsed time per iteration (ms): 260.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.294303E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.09 | backward-params-all-reduce: 64.78 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.02 | optimizer-unscale-and-check-inf: 16.93 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.45 | optimizer: 98.11 | batch-generator: 8.58
 iteration   130500/  200000 | consumed samples:      4176000 | elapsed time per iteration (ms): 262.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.305698E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.87 | backward-params-all-reduce: 62.78 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.81 | optimizer-unscale-and-check-inf: 16.33 | optimizer-clip-main-grad: 7.71 | optimizer-copy-main-to-model-params: 12.56 | optimizer: 98.56 | batch-generator: 8.68
--------------------------------------------------------------------------------------------------
 validation loss at iteration 130500 | lm loss value: 3.302336E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 130500, match long value: -0.0036229813296279633 | match short value: -0.019222452881329614 
-----------------------------------------------------------------------------------------------------------
 iteration   130600/  200000 | consumed samples:      4179200 | elapsed time per iteration (ms): 282.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.291264E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.76 | backward-params-all-reduce: 65.29 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.03 | optimizer-unscale-and-check-inf: 15.26 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 14.22 | optimizer: 98.23 | batch-generator: 19.06
 iteration   130700/  200000 | consumed samples:      4182400 | elapsed time per iteration (ms): 269.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.318421E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.60 | backward-params-all-reduce: 64.50 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.52 | optimizer-unscale-and-check-inf: 16.89 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.67 | batch-generator: 8.66
 iteration   130800/  200000 | consumed samples:      4185600 | elapsed time per iteration (ms): 260.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.292134E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.35 | backward-params-all-reduce: 63.74 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.83 | optimizer-unscale-and-check-inf: 15.99 | optimizer-clip-main-grad: 7.78 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 98.32 | batch-generator: 8.85
 iteration   130900/  200000 | consumed samples:      4188800 | elapsed time per iteration (ms): 259.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.309119E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.00 | backward-params-all-reduce: 63.49 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.15 | optimizer-unscale-and-check-inf: 16.78 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.19 | batch-generator: 8.63
 iteration   131000/  200000 | consumed samples:      4192000 | elapsed time per iteration (ms): 260.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.333372E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.34 | backward-params-all-reduce: 64.17 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.27 | optimizer-unscale-and-check-inf: 16.05 | optimizer-clip-main-grad: 7.54 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 97.51 | batch-generator: 8.71
--------------------------------------------------------------------------------------------------
 validation loss at iteration 131000 | lm loss value: 3.409735E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 131000, match long value: 4.4564442772769033e-05 | match short value: -0.04452943052050381 
----------------------------------------------------------------------------------------------------------
 iteration   131100/  200000 | consumed samples:      4195200 | elapsed time per iteration (ms): 271.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.299787E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.01 | backward-params-all-reduce: 64.01 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.33 | optimizer-unscale-and-check-inf: 17.00 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.38 | optimizer: 97.93 | batch-generator: 15.96
 iteration   131200/  200000 | consumed samples:      4198400 | elapsed time per iteration (ms): 259.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.311641E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.22 | backward-params-all-reduce: 63.53 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.38 | optimizer-unscale-and-check-inf: 16.92 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.57 | optimizer: 98.62 | batch-generator: 8.65
 iteration   131300/  200000 | consumed samples:      4201600 | elapsed time per iteration (ms): 266.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.277848E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.68 | backward-params-all-reduce: 64.34 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.40 | optimizer-unscale-and-check-inf: 15.69 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 97.43 | batch-generator: 14.58
 iteration   131400/  200000 | consumed samples:      4204800 | elapsed time per iteration (ms): 277.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.243439E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.99 | backward-params-all-reduce: 63.69 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.12 | optimizer-unscale-and-check-inf: 15.12 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 14.17 | optimizer: 99.17 | batch-generator: 8.57
 iteration   131500/  200000 | consumed samples:      4208000 | elapsed time per iteration (ms): 259.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.238119E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.40 | backward-params-all-reduce: 63.49 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.63 | optimizer-unscale-and-check-inf: 15.41 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 97.23 | batch-generator: 8.56
--------------------------------------------------------------------------------------------------
 validation loss at iteration 131500 | lm loss value: 3.207360E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 131500, match long value: -0.0020293983435641784 | match short value: -0.029619828901287903 
-----------------------------------------------------------------------------------------------------------
 iteration   131600/  200000 | consumed samples:      4211200 | elapsed time per iteration (ms): 272.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.257540E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.12 | backward-params-all-reduce: 62.92 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.55 | optimizer-unscale-and-check-inf: 17.55 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.44 | batch-generator: 16.16
 iteration   131700/  200000 | consumed samples:      4214400 | elapsed time per iteration (ms): 259.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.244064E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.82 | backward-params-all-reduce: 63.98 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.25 | optimizer-unscale-and-check-inf: 18.18 | optimizer-clip-main-grad: 7.72 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.81 | batch-generator: 8.67
 iteration   131800/  200000 | consumed samples:      4217600 | elapsed time per iteration (ms): 261.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.247124E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.11 | backward-params-all-reduce: 64.29 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.02 | optimizer-unscale-and-check-inf: 19.02 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 12.54 | optimizer: 100.45 | batch-generator: 8.74
 iteration   131900/  200000 | consumed samples:      4220800 | elapsed time per iteration (ms): 262.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.253017E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.31 | backward-params-all-reduce: 65.15 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 11.99 | optimizer-unscale-and-check-inf: 19.65 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 100.84 | batch-generator: 8.67
 iteration   132000/  200000 | consumed samples:      4224000 | elapsed time per iteration (ms): 336.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.251730E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.31 | backward-params-all-reduce: 64.01 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.29 | optimizer-unscale-and-check-inf: 19.33 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 100.98 | batch-generator: 83.37
--------------------------------------------------------------------------------------------------
 validation loss at iteration 132000 | lm loss value: 3.164646E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 132000, match long value: -0.001303321333158161 | match short value: -0.03387065876106817 
---------------------------------------------------------------------------------------------------------
 iteration   132100/  200000 | consumed samples:      4227200 | elapsed time per iteration (ms): 292.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.255516E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.03 | backward-params-all-reduce: 63.03 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.34 | optimizer-unscale-and-check-inf: 20.22 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 14.26 | optimizer: 103.69 | batch-generator: 19.95
 iteration   132200/  200000 | consumed samples:      4230400 | elapsed time per iteration (ms): 260.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.278761E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.85 | backward-params-all-reduce: 63.71 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.20 | optimizer-unscale-and-check-inf: 18.91 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.56 | optimizer: 100.48 | batch-generator: 8.58
 iteration   132300/  200000 | consumed samples:      4233600 | elapsed time per iteration (ms): 258.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.274719E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.34 | backward-params-all-reduce: 63.18 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.71 | optimizer-unscale-and-check-inf: 18.79 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 100.92 | batch-generator: 8.63
 iteration   132400/  200000 | consumed samples:      4236800 | elapsed time per iteration (ms): 260.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.265139E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.54 | backward-params-all-reduce: 63.47 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.81 | optimizer-unscale-and-check-inf: 18.74 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.54 | optimizer: 100.92 | batch-generator: 8.63
 iteration   132500/  200000 | consumed samples:      4240000 | elapsed time per iteration (ms): 261.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.276770E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.68 | backward-params-all-reduce: 64.74 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.21 | optimizer-unscale-and-check-inf: 18.70 | optimizer-clip-main-grad: 7.70 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 100.28 | batch-generator: 8.68
--------------------------------------------------------------------------------------------------
 validation loss at iteration 132500 | lm loss value: 3.247302E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 132500, match long value: -0.007242377062977583 | match short value: -0.027200379322908416 
----------------------------------------------------------------------------------------------------------
 iteration   132600/  200000 | consumed samples:      4243200 | elapsed time per iteration (ms): 272.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.275546E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.40 | backward-params-all-reduce: 64.66 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.07 | optimizer-unscale-and-check-inf: 18.30 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 100.75 | batch-generator: 15.64
 iteration   132700/  200000 | consumed samples:      4246400 | elapsed time per iteration (ms): 260.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.278997E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.23 | backward-params-all-reduce: 63.36 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.69 | optimizer-unscale-and-check-inf: 17.86 | optimizer-clip-main-grad: 7.75 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 100.03 | batch-generator: 8.74
 iteration   132800/  200000 | consumed samples:      4249600 | elapsed time per iteration (ms): 269.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.284530E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.56 | backward-params-all-reduce: 64.80 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.86 | optimizer-unscale-and-check-inf: 16.76 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 14.28 | optimizer: 100.62 | batch-generator: 8.64
 iteration   132900/  200000 | consumed samples:      4252800 | elapsed time per iteration (ms): 264.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.265592E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.18 | backward-params-all-reduce: 64.21 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.67 | optimizer-unscale-and-check-inf: 17.62 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 99.56 | batch-generator: 8.60
 iteration   133000/  200000 | consumed samples:      4256000 | elapsed time per iteration (ms): 259.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.294373E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.82 | backward-params-all-reduce: 63.87 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.01 | optimizer-unscale-and-check-inf: 17.00 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 99.29 | batch-generator: 8.63
--------------------------------------------------------------------------------------------------
 validation loss at iteration 133000 | lm loss value: 3.354808E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 133000, match long value: -0.003797926227485861 | match short value: -0.03274741606068985 
---------------------------------------------------------------------------------------------------------
 iteration   133100/  200000 | consumed samples:      4259200 | elapsed time per iteration (ms): 268.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.288563E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   4 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.33 | backward-params-all-reduce: 63.85 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.40 | optimizer-unscale-and-check-inf: 17.56 | optimizer-clip-main-grad: 7.37 | optimizer-copy-main-to-model-params: 11.99 | optimizer: 97.53 | batch-generator: 15.19
 iteration   133200/  200000 | consumed samples:      4262400 | elapsed time per iteration (ms): 259.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.301981E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.28 | backward-params-all-reduce: 62.75 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.87 | optimizer-unscale-and-check-inf: 17.84 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.57 | optimizer: 100.07 | batch-generator: 8.66
 iteration   133300/  200000 | consumed samples:      4265600 | elapsed time per iteration (ms): 258.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.314586E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.03 | backward-params-all-reduce: 62.98 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.32 | optimizer-unscale-and-check-inf: 17.07 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.66 | batch-generator: 8.67
 iteration   133400/  200000 | consumed samples:      4268800 | elapsed time per iteration (ms): 259.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.305898E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.19 | backward-params-all-reduce: 64.11 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.73 | optimizer-unscale-and-check-inf: 16.57 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.60 | batch-generator: 8.63
 iteration   133500/  200000 | consumed samples:      4272000 | elapsed time per iteration (ms): 259.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.306515E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.43 | backward-params-all-reduce: 64.18 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.50 | optimizer-unscale-and-check-inf: 17.39 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.16 | batch-generator: 8.68
--------------------------------------------------------------------------------------------------
 validation loss at iteration 133500 | lm loss value: 3.256466E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 133500, match long value: -0.00019556271895387038 | match short value: -0.008158895514061326 
------------------------------------------------------------------------------------------------------------
 iteration   133600/  200000 | consumed samples:      4275200 | elapsed time per iteration (ms): 290.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.302325E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.02 | backward-params-all-reduce: 65.35 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.80 | optimizer-unscale-and-check-inf: 16.56 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 14.26 | optimizer: 100.43 | batch-generator: 19.43
 iteration   133700/  200000 | consumed samples:      4278400 | elapsed time per iteration (ms): 259.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.312084E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.95 | backward-params-all-reduce: 64.09 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.93 | optimizer-unscale-and-check-inf: 16.28 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.54 | optimizer: 98.53 | batch-generator: 8.80
 iteration   133800/  200000 | consumed samples:      4281600 | elapsed time per iteration (ms): 260.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.293769E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.15 | backward-params-all-reduce: 64.61 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.56 | optimizer-unscale-and-check-inf: 15.17 | optimizer-clip-main-grad: 7.71 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 98.14 | batch-generator: 8.75
 iteration   133900/  200000 | consumed samples:      4284800 | elapsed time per iteration (ms): 259.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.292979E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.75 | backward-params-all-reduce: 62.66 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.46 | optimizer-unscale-and-check-inf: 16.59 | optimizer-clip-main-grad: 7.70 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 99.42 | batch-generator: 8.74
 iteration   134000/  200000 | consumed samples:      4288000 | elapsed time per iteration (ms): 258.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.291772E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.05 | backward-params-all-reduce: 63.52 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.16 | optimizer-unscale-and-check-inf: 16.60 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.00 | batch-generator: 8.50
--------------------------------------------------------------------------------------------------
 validation loss at iteration 134000 | lm loss value: 3.294667E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 134000, match long value: -0.002757132414563622 | match short value: -0.0035258999169357854 
-----------------------------------------------------------------------------------------------------------
 iteration   134100/  200000 | consumed samples:      4291200 | elapsed time per iteration (ms): 272.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.296151E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.93 | backward-params-all-reduce: 64.40 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.50 | optimizer-unscale-and-check-inf: 16.25 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.13 | batch-generator: 15.82
 iteration   134200/  200000 | consumed samples:      4294400 | elapsed time per iteration (ms): 259.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.302675E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.23 | backward-params-all-reduce: 63.52 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.72 | optimizer-unscale-and-check-inf: 15.13 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 98.12 | batch-generator: 8.71
 iteration   134300/  200000 | consumed samples:      4297600 | elapsed time per iteration (ms): 270.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.307473E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.94 | backward-params-all-reduce: 62.73 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.68 | optimizer-unscale-and-check-inf: 16.50 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 14.11 | optimizer: 100.02 | batch-generator: 8.49
 iteration   134400/  200000 | consumed samples:      4300800 | elapsed time per iteration (ms): 265.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.305756E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.31 | backward-params-all-reduce: 64.08 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.81 | optimizer-unscale-and-check-inf: 15.42 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 97.56 | batch-generator: 14.62
 iteration   134500/  200000 | consumed samples:      4304000 | elapsed time per iteration (ms): 259.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.227917E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.22 | backward-params-all-reduce: 64.37 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.91 | optimizer-unscale-and-check-inf: 18.33 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 100.53 | batch-generator: 8.56
--------------------------------------------------------------------------------------------------
 validation loss at iteration 134500 | lm loss value: 3.258451E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 134500, match long value: 0.0004923134665496399 | match short value: -0.025812638884460157 
----------------------------------------------------------------------------------------------------------
 iteration   134600/  200000 | consumed samples:      4307200 | elapsed time per iteration (ms): 272.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.241027E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.43 | backward-params-all-reduce: 64.89 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.90 | optimizer-unscale-and-check-inf: 16.76 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.92 | batch-generator: 14.71
 iteration   134700/  200000 | consumed samples:      4310400 | elapsed time per iteration (ms): 261.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.264752E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 54.54 | backward-params-all-reduce: 64.63 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.98 | optimizer-unscale-and-check-inf: 16.87 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.15 | batch-generator: 8.62
 iteration   134800/  200000 | consumed samples:      4313600 | elapsed time per iteration (ms): 261.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.250745E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.63 | backward-params-all-reduce: 63.80 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.99 | optimizer-unscale-and-check-inf: 16.67 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 98.89 | batch-generator: 8.65
 iteration   134900/  200000 | consumed samples:      4316800 | elapsed time per iteration (ms): 259.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.263693E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.08 | backward-params-all-reduce: 64.41 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.63 | optimizer-unscale-and-check-inf: 16.68 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.64 | batch-generator: 8.77
 iteration   135000/  200000 | consumed samples:      4320000 | elapsed time per iteration (ms): 271.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.243463E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.01 | backward-params-all-reduce: 64.37 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.14 | optimizer-unscale-and-check-inf: 17.14 | optimizer-clip-main-grad: 7.72 | optimizer-copy-main-to-model-params: 14.33 | optimizer: 101.48 | batch-generator: 8.62
--------------------------------------------------------------------------------------------------
 validation loss at iteration 135000 | lm loss value: 3.307894E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 135000, match long value: -0.0038563147546282783 | match short value: -0.026945607415179644 
-----------------------------------------------------------------------------------------------------------
 iteration   135100/  200000 | consumed samples:      4323200 | elapsed time per iteration (ms): 276.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.239717E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.44 | backward-params-all-reduce: 64.01 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.23 | optimizer-unscale-and-check-inf: 16.97 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.42 | batch-generator: 18.90
 iteration   135200/  200000 | consumed samples:      4326400 | elapsed time per iteration (ms): 260.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.254557E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.60 | backward-params-all-reduce: 65.01 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.09 | optimizer-unscale-and-check-inf: 16.47 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 98.87 | batch-generator: 8.54
 iteration   135300/  200000 | consumed samples:      4329600 | elapsed time per iteration (ms): 258.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.270778E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.59 | backward-params-all-reduce: 64.22 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.62 | optimizer-unscale-and-check-inf: 16.38 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 99.31 | batch-generator: 8.64
 iteration   135400/  200000 | consumed samples:      4332800 | elapsed time per iteration (ms): 260.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.262703E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.33 | backward-params-all-reduce: 65.32 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.13 | optimizer-unscale-and-check-inf: 17.57 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.91 | batch-generator: 8.73
 iteration   135500/  200000 | consumed samples:      4336000 | elapsed time per iteration (ms): 262.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.254197E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.35 | backward-params-all-reduce: 65.98 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.10 | optimizer-unscale-and-check-inf: 16.69 | optimizer-clip-main-grad: 7.73 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.21 | batch-generator: 8.90
--------------------------------------------------------------------------------------------------
 validation loss at iteration 135500 | lm loss value: 3.213296E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 135500, match long value: -0.0024156271485281772 | match short value: -0.019157794818382666 
-----------------------------------------------------------------------------------------------------------
 iteration   135600/  200000 | consumed samples:      4339200 | elapsed time per iteration (ms): 269.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.257092E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.90 | backward-params-all-reduce: 65.06 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.47 | optimizer-unscale-and-check-inf: 16.80 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 99.48 | batch-generator: 15.41
 iteration   135700/  200000 | consumed samples:      4342400 | elapsed time per iteration (ms): 258.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.264330E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.31 | backward-params-all-reduce: 63.62 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.44 | optimizer-unscale-and-check-inf: 17.03 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.76 | batch-generator: 8.70
 iteration   135800/  200000 | consumed samples:      4345600 | elapsed time per iteration (ms): 273.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.268241E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.05 | backward-params-all-reduce: 64.05 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.87 | optimizer-unscale-and-check-inf: 16.85 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 14.38 | optimizer: 100.91 | batch-generator: 8.65
 iteration   135900/  200000 | consumed samples:      4348800 | elapsed time per iteration (ms): 259.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.286760E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.02 | backward-params-all-reduce: 63.20 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.19 | optimizer-unscale-and-check-inf: 17.08 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 98.60 | batch-generator: 8.62
 iteration   136000/  200000 | consumed samples:      4352000 | elapsed time per iteration (ms): 259.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.277471E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.63 | backward-params-all-reduce: 63.13 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.73 | optimizer-unscale-and-check-inf: 17.42 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.44 | batch-generator: 8.59
--------------------------------------------------------------------------------------------------
 validation loss at iteration 136000 | lm loss value: 3.309777E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 136000, match long value: -0.003196230747885831 | match short value: 0.005237488493248024 
---------------------------------------------------------------------------------------------------------
 iteration   136100/  200000 | consumed samples:      4355200 | elapsed time per iteration (ms): 271.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.268336E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.06 | backward-params-all-reduce: 63.63 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.36 | optimizer-unscale-and-check-inf: 16.74 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.32 | batch-generator: 17.32
 iteration   136200/  200000 | consumed samples:      4358400 | elapsed time per iteration (ms): 261.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.270696E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.15 | backward-params-all-reduce: 64.11 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.39 | optimizer-unscale-and-check-inf: 16.89 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.54 | batch-generator: 8.69
 iteration   136300/  200000 | consumed samples:      4361600 | elapsed time per iteration (ms): 258.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.260179E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.25 | backward-params-all-reduce: 62.67 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.45 | optimizer-unscale-and-check-inf: 16.74 | optimizer-clip-main-grad: 7.54 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 98.33 | batch-generator: 8.55
 iteration   136400/  200000 | consumed samples:      4364800 | elapsed time per iteration (ms): 259.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.281250E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.04 | backward-params-all-reduce: 62.63 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.26 | optimizer-unscale-and-check-inf: 16.62 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 98.08 | batch-generator: 8.67
 iteration   136500/  200000 | consumed samples:      4368000 | elapsed time per iteration (ms): 274.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.283951E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.93 | backward-params-all-reduce: 63.74 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.31 | optimizer-unscale-and-check-inf: 15.60 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 14.18 | optimizer: 98.80 | batch-generator: 8.72
--------------------------------------------------------------------------------------------------
 validation loss at iteration 136500 | lm loss value: 3.122172E-05 | lm loss PPL: 1.000031E+00 | 
 at iteration 136500, match long value: 0.0019191312728626033 | match short value: -0.03355854375955075 
---------------------------------------------------------------------------------------------------------
 iteration   136600/  200000 | consumed samples:      4371200 | elapsed time per iteration (ms): 277.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.283480E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.93 | backward-params-all-reduce: 64.24 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.52 | optimizer-unscale-and-check-inf: 17.08 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.90 | batch-generator: 20.46
 iteration   136700/  200000 | consumed samples:      4374400 | elapsed time per iteration (ms): 259.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.298100E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.14 | backward-params-all-reduce: 63.35 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.18 | optimizer-unscale-and-check-inf: 17.28 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.75 | batch-generator: 8.57
 iteration   136800/  200000 | consumed samples:      4377600 | elapsed time per iteration (ms): 259.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.271347E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.14 | backward-params-all-reduce: 63.98 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.34 | optimizer-unscale-and-check-inf: 17.02 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.55 | batch-generator: 8.58
 iteration   136900/  200000 | consumed samples:      4380800 | elapsed time per iteration (ms): 258.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.311765E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.21 | backward-params-all-reduce: 64.00 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.56 | optimizer-unscale-and-check-inf: 17.10 | optimizer-clip-main-grad: 7.53 | optimizer-copy-main-to-model-params: 12.36 | optimizer: 98.19 | batch-generator: 8.55
 iteration   137000/  200000 | consumed samples:      4384000 | elapsed time per iteration (ms): 259.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.297856E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.27 | backward-params-all-reduce: 64.21 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.50 | optimizer-unscale-and-check-inf: 17.09 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 98.80 | batch-generator: 8.62
--------------------------------------------------------------------------------------------------
 validation loss at iteration 137000 | lm loss value: 3.246350E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 137000, match long value: -0.0033080600340546244 | match short value: -0.01833815898408041 
----------------------------------------------------------------------------------------------------------
 iteration   137100/  200000 | consumed samples:      4387200 | elapsed time per iteration (ms): 275.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.301109E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.39 | backward-params-all-reduce: 64.69 | backward-embedding-all-reduce: 0.05 | optimizer-copy-to-main-grad: 12.56 | optimizer-unscale-and-check-inf: 17.22 | optimizer-clip-main-grad: 7.73 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 99.22 | batch-generator: 17.16
 iteration   137200/  200000 | consumed samples:      4390400 | elapsed time per iteration (ms): 272.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.300190E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.18 | backward-params-all-reduce: 63.54 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.10 | optimizer-unscale-and-check-inf: 16.48 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 14.17 | optimizer: 99.47 | batch-generator: 8.87
 iteration   137300/  200000 | consumed samples:      4393600 | elapsed time per iteration (ms): 260.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.302778E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.32 | backward-params-all-reduce: 63.21 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.27 | optimizer-unscale-and-check-inf: 16.16 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 97.70 | batch-generator: 8.69
 iteration   137400/  200000 | consumed samples:      4396800 | elapsed time per iteration (ms): 260.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.307323E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.00 | backward-params-all-reduce: 64.77 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.48 | optimizer-unscale-and-check-inf: 17.58 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.40 | optimizer: 98.75 | batch-generator: 8.72
 iteration   137500/  200000 | consumed samples:      4400000 | elapsed time per iteration (ms): 258.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.321306E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.21 | backward-params-all-reduce: 63.41 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 11.92 | optimizer-unscale-and-check-inf: 15.87 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 96.97 | batch-generator: 8.65
--------------------------------------------------------------------------------------------------
 validation loss at iteration 137500 | lm loss value: 3.264815E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 137500, match long value: -0.0021819908058355307 | match short value: -0.037345527356366925 
-----------------------------------------------------------------------------------------------------------
 iteration   137600/  200000 | consumed samples:      4403200 | elapsed time per iteration (ms): 278.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.228525E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.11 | backward-params-all-reduce: 64.85 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.76 | optimizer-unscale-and-check-inf: 18.74 | optimizer-clip-main-grad: 7.72 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 100.91 | batch-generator: 21.47
 iteration   137700/  200000 | consumed samples:      4406400 | elapsed time per iteration (ms): 261.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.230353E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.19 | backward-params-all-reduce: 65.41 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.97 | optimizer-unscale-and-check-inf: 17.55 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.54 | optimizer: 99.93 | batch-generator: 8.63
 iteration   137800/  200000 | consumed samples:      4409600 | elapsed time per iteration (ms): 261.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.232276E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.32 | backward-params-all-reduce: 64.63 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.71 | optimizer-unscale-and-check-inf: 17.49 | optimizer-clip-main-grad: 7.72 | optimizer-copy-main-to-model-params: 12.54 | optimizer: 99.64 | batch-generator: 8.67
 iteration   137900/  200000 | consumed samples:      4412800 | elapsed time per iteration (ms): 262.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.238766E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.57 | backward-params-all-reduce: 63.51 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.88 | optimizer-unscale-and-check-inf: 16.96 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 12.54 | optimizer: 99.25 | batch-generator: 8.74
 iteration   138000/  200000 | consumed samples:      4416000 | elapsed time per iteration (ms): 274.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.245513E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.89 | backward-params-all-reduce: 64.73 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.56 | optimizer-unscale-and-check-inf: 16.90 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 14.29 | optimizer: 100.55 | batch-generator: 8.73
--------------------------------------------------------------------------------------------------
 validation loss at iteration 138000 | lm loss value: 3.344427E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 138000, match long value: 0.0003548096389047502 | match short value: -0.02681718097423695 
---------------------------------------------------------------------------------------------------------
 iteration   138100/  200000 | consumed samples:      4419200 | elapsed time per iteration (ms): 277.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.249034E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.23 | backward-params-all-reduce: 63.86 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.34 | optimizer-unscale-and-check-inf: 17.49 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 99.04 | batch-generator: 22.03
 iteration   138200/  200000 | consumed samples:      4422400 | elapsed time per iteration (ms): 263.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.255566E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.28 | backward-params-all-reduce: 65.04 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.07 | optimizer-unscale-and-check-inf: 16.20 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 98.58 | batch-generator: 8.53
 iteration   138300/  200000 | consumed samples:      4425600 | elapsed time per iteration (ms): 260.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.250583E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.71 | backward-params-all-reduce: 63.27 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.90 | optimizer-unscale-and-check-inf: 17.64 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.87 | batch-generator: 8.62
 iteration   138400/  200000 | consumed samples:      4428800 | elapsed time per iteration (ms): 260.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.257740E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.82 | backward-params-all-reduce: 63.43 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.53 | optimizer-unscale-and-check-inf: 16.67 | optimizer-clip-main-grad: 7.52 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 98.30 | batch-generator: 8.41
 iteration   138500/  200000 | consumed samples:      4432000 | elapsed time per iteration (ms): 259.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.268896E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 54.72 | backward-params-all-reduce: 62.67 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.02 | optimizer-unscale-and-check-inf: 16.17 | optimizer-clip-main-grad: 7.51 | optimizer-copy-main-to-model-params: 12.38 | optimizer: 97.74 | batch-generator: 8.58
--------------------------------------------------------------------------------------------------
 validation loss at iteration 138500 | lm loss value: 3.342877E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 138500, match long value: 0.0031543075383484677 | match short value: -0.0013064457975505627 
-----------------------------------------------------------------------------------------------------------
 iteration   138600/  200000 | consumed samples:      4435200 | elapsed time per iteration (ms): 269.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.257422E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.81 | backward-params-all-reduce: 64.27 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.48 | optimizer-unscale-and-check-inf: 15.81 | optimizer-clip-main-grad: 7.45 | optimizer-copy-main-to-model-params: 12.36 | optimizer: 96.75 | batch-generator: 15.61
 iteration   138700/  200000 | consumed samples:      4438400 | elapsed time per iteration (ms): 274.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.265859E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.36 | backward-params-all-reduce: 63.10 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.27 | optimizer-unscale-and-check-inf: 16.10 | optimizer-clip-main-grad: 7.50 | optimizer-copy-main-to-model-params: 14.16 | optimizer: 98.69 | batch-generator: 8.66
 iteration   138800/  200000 | consumed samples:      4441600 | elapsed time per iteration (ms): 258.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.259040E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.00 | backward-params-all-reduce: 63.88 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.50 | optimizer-unscale-and-check-inf: 16.52 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.30 | batch-generator: 8.46
 iteration   138900/  200000 | consumed samples:      4444800 | elapsed time per iteration (ms): 257.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.270734E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.24 | backward-params-all-reduce: 64.01 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.50 | optimizer-unscale-and-check-inf: 16.47 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 98.18 | batch-generator: 8.52
 iteration   139000/  200000 | consumed samples:      4448000 | elapsed time per iteration (ms): 257.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.284140E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.03 | backward-params-all-reduce: 62.96 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.20 | optimizer-unscale-and-check-inf: 16.81 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 98.21 | batch-generator: 8.50
--------------------------------------------------------------------------------------------------
 validation loss at iteration 139000 | lm loss value: 3.213299E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 139000, match long value: 0.002869638507311556 | match short value: -0.04512787400410556 
--------------------------------------------------------------------------------------------------------
 iteration   139100/  200000 | consumed samples:      4451200 | elapsed time per iteration (ms): 272.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.281647E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.07 | backward-params-all-reduce: 64.79 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.41 | optimizer-unscale-and-check-inf: 14.98 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 96.60 | batch-generator: 17.44
 iteration   139200/  200000 | consumed samples:      4454400 | elapsed time per iteration (ms): 259.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.283052E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.16 | backward-params-all-reduce: 63.82 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.37 | optimizer-unscale-and-check-inf: 15.94 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.57 | optimizer: 97.66 | batch-generator: 8.74
 iteration   139300/  200000 | consumed samples:      4457600 | elapsed time per iteration (ms): 257.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.283821E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.42 | backward-params-all-reduce: 62.49 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.48 | optimizer-unscale-and-check-inf: 15.82 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 97.62 | batch-generator: 8.65
 iteration   139400/  200000 | consumed samples:      4460800 | elapsed time per iteration (ms): 273.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.275625E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.87 | backward-params-all-reduce: 64.09 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.53 | optimizer-unscale-and-check-inf: 15.45 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 14.30 | optimizer: 99.07 | batch-generator: 8.70
 iteration   139500/  200000 | consumed samples:      4464000 | elapsed time per iteration (ms): 258.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.281568E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.21 | backward-params-all-reduce: 63.10 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.13 | optimizer-unscale-and-check-inf: 15.05 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 96.34 | batch-generator: 8.61
--------------------------------------------------------------------------------------------------
 validation loss at iteration 139500 | lm loss value: 3.250935E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 139500, match long value: 0.000528822377275806 | match short value: -0.02177759320198408 
--------------------------------------------------------------------------------------------------------
 iteration   139600/  200000 | consumed samples:      4467200 | elapsed time per iteration (ms): 270.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.288243E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.08 | backward-params-all-reduce: 62.33 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.18 | optimizer-unscale-and-check-inf: 16.90 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.29 | batch-generator: 16.00
 iteration   139700/  200000 | consumed samples:      4470400 | elapsed time per iteration (ms): 258.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.297066E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.63 | backward-params-all-reduce: 63.40 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.40 | optimizer-unscale-and-check-inf: 16.21 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 97.79 | batch-generator: 8.62
 iteration   139800/  200000 | consumed samples:      4473600 | elapsed time per iteration (ms): 260.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.304355E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.19 | backward-params-all-reduce: 64.81 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.14 | optimizer-unscale-and-check-inf: 17.05 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.39 | batch-generator: 8.52
 iteration   139900/  200000 | consumed samples:      4476800 | elapsed time per iteration (ms): 260.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.290132E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.22 | backward-params-all-reduce: 63.62 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.30 | optimizer-unscale-and-check-inf: 16.46 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 98.10 | batch-generator: 8.68
 iteration   140000/  200000 | consumed samples:      4480000 | elapsed time per iteration (ms): 259.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.294306E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.21 | backward-params-all-reduce: 63.47 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.41 | optimizer-unscale-and-check-inf: 16.71 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 98.36 | batch-generator: 8.57
--------------------------------------------------------------------------------------------------
 validation loss at iteration 140000 | lm loss value: 3.250982E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 140000, match long value: -0.00117364672921233 | match short value: -0.016131554100340048 
---------------------------------------------------------------------------------------------------------
saving checkpoint at iteration  140000 to verify1
  successfully saved checkpoint at iteration  140000 to verify1
time (ms) | save-checkpoint: 23523.36
 iteration   140100/  200000 | consumed samples:      4483200 | elapsed time per iteration (ms): 517.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.295996E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.45 | backward-params-all-reduce: 64.44 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.57 | optimizer-unscale-and-check-inf: 18.31 | optimizer-clip-main-grad: 7.73 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 100.26 | batch-generator: 22.58
 iteration   140200/  200000 | consumed samples:      4486400 | elapsed time per iteration (ms): 272.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.292445E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.64 | backward-params-all-reduce: 64.11 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.02 | optimizer-unscale-and-check-inf: 18.48 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 14.14 | optimizer: 101.35 | batch-generator: 8.41
 iteration   140300/  200000 | consumed samples:      4489600 | elapsed time per iteration (ms): 262.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.298661E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.25 | backward-params-all-reduce: 64.71 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.12 | optimizer-unscale-and-check-inf: 19.12 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 100.54 | batch-generator: 8.43
 iteration   140400/  200000 | consumed samples:      4492800 | elapsed time per iteration (ms): 338.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.307845E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.43 | backward-params-all-reduce: 64.26 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.05 | optimizer-unscale-and-check-inf: 17.48 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.55 | optimizer: 98.82 | batch-generator: 85.70
 iteration   140500/  200000 | consumed samples:      4496000 | elapsed time per iteration (ms): 258.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.304831E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.15 | backward-params-all-reduce: 63.04 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.15 | optimizer-unscale-and-check-inf: 17.50 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.98 | batch-generator: 8.53
--------------------------------------------------------------------------------------------------
 validation loss at iteration 140500 | lm loss value: 3.277534E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 140500, match long value: -0.003387971157819841 | match short value: -0.035566469580964474 
----------------------------------------------------------------------------------------------------------
 iteration   140600/  200000 | consumed samples:      4499200 | elapsed time per iteration (ms): 272.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.325665E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.23 | backward-params-all-reduce: 64.13 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.00 | optimizer-unscale-and-check-inf: 18.63 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.95 | batch-generator: 16.04
 iteration   140700/  200000 | consumed samples:      4502400 | elapsed time per iteration (ms): 264.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.265235E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.15 | backward-params-all-reduce: 63.93 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.25 | optimizer-unscale-and-check-inf: 16.84 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 99.42 | batch-generator: 14.25
 iteration   140800/  200000 | consumed samples:      4505600 | elapsed time per iteration (ms): 264.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.235678E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.67 | backward-params-all-reduce: 64.43 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.89 | optimizer-unscale-and-check-inf: 18.55 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 100.74 | batch-generator: 8.50
 iteration   140900/  200000 | consumed samples:      4508800 | elapsed time per iteration (ms): 271.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.237945E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.09 | backward-params-all-reduce: 64.70 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.21 | optimizer-unscale-and-check-inf: 16.25 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 14.35 | optimizer: 100.68 | batch-generator: 8.74
 iteration   141000/  200000 | consumed samples:      4512000 | elapsed time per iteration (ms): 261.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.232173E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.58 | backward-params-all-reduce: 64.52 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.78 | optimizer-unscale-and-check-inf: 17.09 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.08 | batch-generator: 8.55
--------------------------------------------------------------------------------------------------
 validation loss at iteration 141000 | lm loss value: 3.303422E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 141000, match long value: -0.002530054993449105 | match short value: -0.014011956426050071 
----------------------------------------------------------------------------------------------------------
 iteration   141100/  200000 | consumed samples:      4515200 | elapsed time per iteration (ms): 273.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.248765E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.52 | backward-params-all-reduce: 64.89 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.71 | optimizer-unscale-and-check-inf: 17.51 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.47 | batch-generator: 15.64
 iteration   141200/  200000 | consumed samples:      4518400 | elapsed time per iteration (ms): 262.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.253100E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.30 | backward-params-all-reduce: 64.63 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.86 | optimizer-unscale-and-check-inf: 17.06 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 100.28 | batch-generator: 8.54
 iteration   141300/  200000 | consumed samples:      4521600 | elapsed time per iteration (ms): 258.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.242789E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.26 | backward-params-all-reduce: 63.71 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.61 | optimizer-unscale-and-check-inf: 17.93 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.82 | batch-generator: 8.50
 iteration   141400/  200000 | consumed samples:      4524800 | elapsed time per iteration (ms): 261.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.259124E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.21 | backward-params-all-reduce: 65.22 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.21 | optimizer-unscale-and-check-inf: 17.47 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.87 | batch-generator: 8.43
 iteration   141500/  200000 | consumed samples:      4528000 | elapsed time per iteration (ms): 261.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.241848E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.35 | backward-params-all-reduce: 64.55 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.39 | optimizer-unscale-and-check-inf: 17.67 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 100.34 | batch-generator: 8.55
--------------------------------------------------------------------------------------------------
 validation loss at iteration 141500 | lm loss value: 3.331624E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 141500, match long value: -0.0010190784399656645 | match short value: -0.0287362149634243 
---------------------------------------------------------------------------------------------------------
 iteration   141600/  200000 | consumed samples:      4531200 | elapsed time per iteration (ms): 291.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.259468E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.82 | backward-params-all-reduce: 64.49 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.35 | optimizer-unscale-and-check-inf: 16.97 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 14.27 | optimizer: 101.37 | batch-generator: 21.80
 iteration   141700/  200000 | consumed samples:      4534400 | elapsed time per iteration (ms): 261.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.252126E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.31 | backward-params-all-reduce: 64.40 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.82 | optimizer-unscale-and-check-inf: 16.65 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.30 | optimizer: 98.59 | batch-generator: 8.73
 iteration   141800/  200000 | consumed samples:      4537600 | elapsed time per iteration (ms): 260.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.248506E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.98 | backward-params-all-reduce: 65.02 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.99 | optimizer-unscale-and-check-inf: 17.29 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.38 | optimizer: 98.92 | batch-generator: 8.64
 iteration   141900/  200000 | consumed samples:      4540800 | elapsed time per iteration (ms): 259.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.258545E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.25 | backward-params-all-reduce: 64.76 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.00 | optimizer-unscale-and-check-inf: 17.44 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.68 | batch-generator: 8.49
 iteration   142000/  200000 | consumed samples:      4544000 | elapsed time per iteration (ms): 258.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.257805E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.09 | backward-params-all-reduce: 63.58 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.66 | optimizer-unscale-and-check-inf: 16.54 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.54 | batch-generator: 8.56
--------------------------------------------------------------------------------------------------
 validation loss at iteration 142000 | lm loss value: 3.254427E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 142000, match long value: -0.000213445390421934 | match short value: -0.012896910173418877 
----------------------------------------------------------------------------------------------------------
 iteration   142100/  200000 | consumed samples:      4547200 | elapsed time per iteration (ms): 269.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.270593E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.99 | backward-params-all-reduce: 64.04 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.98 | optimizer-unscale-and-check-inf: 16.82 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.13 | batch-generator: 14.53
 iteration   142200/  200000 | consumed samples:      4550400 | elapsed time per iteration (ms): 259.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.287931E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.30 | backward-params-all-reduce: 63.75 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 14.57 | optimizer-unscale-and-check-inf: 15.50 | optimizer-clip-main-grad: 7.78 | optimizer-copy-main-to-model-params: 12.55 | optimizer: 99.58 | batch-generator: 8.62
 iteration   142300/  200000 | consumed samples:      4553600 | elapsed time per iteration (ms): 264.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.273840E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.57 | backward-params-all-reduce: 64.56 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.33 | optimizer-unscale-and-check-inf: 16.57 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.16 | batch-generator: 8.52
 iteration   142400/  200000 | consumed samples:      4556800 | elapsed time per iteration (ms): 270.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.263175E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.05 | backward-params-all-reduce: 63.89 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.15 | optimizer-unscale-and-check-inf: 16.97 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 14.27 | optimizer: 101.11 | batch-generator: 8.57
 iteration   142500/  200000 | consumed samples:      4560000 | elapsed time per iteration (ms): 259.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.280384E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.23 | backward-params-all-reduce: 64.15 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.19 | optimizer-unscale-and-check-inf: 16.64 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 99.11 | batch-generator: 8.58
--------------------------------------------------------------------------------------------------
 validation loss at iteration 142500 | lm loss value: 3.224845E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 142500, match long value: 3.6731346807797536e-06 | match short value: -0.009763760174248173 
-----------------------------------------------------------------------------------------------------------
 iteration   142600/  200000 | consumed samples:      4563200 | elapsed time per iteration (ms): 270.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.285507E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.06 | backward-params-all-reduce: 63.73 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.49 | optimizer-unscale-and-check-inf: 17.00 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.45 | optimizer: 99.63 | batch-generator: 14.40
 iteration   142700/  200000 | consumed samples:      4566400 | elapsed time per iteration (ms): 259.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.272741E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.33 | backward-params-all-reduce: 62.97 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.32 | optimizer-unscale-and-check-inf: 17.25 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.54 | optimizer: 99.87 | batch-generator: 8.54
 iteration   142800/  200000 | consumed samples:      4569600 | elapsed time per iteration (ms): 259.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.276713E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.15 | backward-params-all-reduce: 63.38 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.24 | optimizer-unscale-and-check-inf: 17.23 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 99.69 | batch-generator: 8.52
 iteration   142900/  200000 | consumed samples:      4572800 | elapsed time per iteration (ms): 259.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.282727E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.28 | backward-params-all-reduce: 64.39 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.19 | optimizer-unscale-and-check-inf: 16.53 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.91 | batch-generator: 8.48
 iteration   143000/  200000 | consumed samples:      4576000 | elapsed time per iteration (ms): 259.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.301895E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.40 | backward-params-all-reduce: 63.48 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.81 | optimizer-unscale-and-check-inf: 16.52 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.39 | optimizer: 97.94 | batch-generator: 8.52
--------------------------------------------------------------------------------------------------
 validation loss at iteration 143000 | lm loss value: 3.321947E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 143000, match long value: 0.00014722920982097908 | match short value: -0.002530956374195237 
-----------------------------------------------------------------------------------------------------------
 iteration   143100/  200000 | consumed samples:      4579200 | elapsed time per iteration (ms): 286.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.289964E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.61 | backward-params-all-reduce: 63.51 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.99 | optimizer-unscale-and-check-inf: 16.04 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 14.12 | optimizer: 99.84 | batch-generator: 19.62
 iteration   143200/  200000 | consumed samples:      4582400 | elapsed time per iteration (ms): 259.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.293504E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.26 | backward-params-all-reduce: 63.19 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.21 | optimizer-unscale-and-check-inf: 16.69 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.11 | batch-generator: 8.47
 iteration   143300/  200000 | consumed samples:      4585600 | elapsed time per iteration (ms): 258.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.279029E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.12 | backward-params-all-reduce: 63.06 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 14.09 | optimizer-unscale-and-check-inf: 16.35 | optimizer-clip-main-grad: 7.70 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 99.91 | batch-generator: 8.59
 iteration   143400/  200000 | consumed samples:      4588800 | elapsed time per iteration (ms): 258.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.296953E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.32 | backward-params-all-reduce: 62.88 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.13 | optimizer-unscale-and-check-inf: 16.64 | optimizer-clip-main-grad: 7.71 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 99.21 | batch-generator: 8.61
 iteration   143500/  200000 | consumed samples:      4592000 | elapsed time per iteration (ms): 260.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.308035E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.06 | backward-params-all-reduce: 65.02 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 14.10 | optimizer-unscale-and-check-inf: 15.36 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 98.82 | batch-generator: 8.56
--------------------------------------------------------------------------------------------------
 validation loss at iteration 143500 | lm loss value: 3.176601E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 143500, match long value: -0.0014362211838643447 | match short value: -0.028651633206342855 
-----------------------------------------------------------------------------------------------------------
 iteration   143600/  200000 | consumed samples:      4595200 | elapsed time per iteration (ms): 269.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.291767E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.50 | backward-params-all-reduce: 63.37 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.66 | optimizer-unscale-and-check-inf: 15.46 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.44 | batch-generator: 14.88
 iteration   143700/  200000 | consumed samples:      4598400 | elapsed time per iteration (ms): 257.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.301012E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.11 | backward-params-all-reduce: 64.29 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.41 | optimizer-unscale-and-check-inf: 16.45 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.37 | optimizer: 98.54 | batch-generator: 8.53
 iteration   143800/  200000 | consumed samples:      4601600 | elapsed time per iteration (ms): 276.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.255346E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.65 | backward-params-all-reduce: 64.88 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.99 | optimizer-unscale-and-check-inf: 17.25 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 14.22 | optimizer: 101.25 | batch-generator: 14.05
 iteration   143900/  200000 | consumed samples:      4604800 | elapsed time per iteration (ms): 262.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.233854E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.43 | backward-params-all-reduce: 64.83 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.42 | optimizer-unscale-and-check-inf: 16.26 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.54 | optimizer: 98.99 | batch-generator: 8.67
 iteration   144000/  200000 | consumed samples:      4608000 | elapsed time per iteration (ms): 262.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.244823E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.61 | backward-params-all-reduce: 63.59 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.52 | optimizer-unscale-and-check-inf: 19.42 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 101.23 | batch-generator: 8.51
--------------------------------------------------------------------------------------------------
 validation loss at iteration 144000 | lm loss value: 3.258809E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 144000, match long value: 0.00024204138930172212 | match short value: -0.02513978621988031 
----------------------------------------------------------------------------------------------------------
 iteration   144100/  200000 | consumed samples:      4611200 | elapsed time per iteration (ms): 273.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.240600E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.20 | backward-params-all-reduce: 64.98 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.68 | optimizer-unscale-and-check-inf: 18.25 | optimizer-clip-main-grad: 7.81 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 100.43 | batch-generator: 16.49
 iteration   144200/  200000 | consumed samples:      4614400 | elapsed time per iteration (ms): 261.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.218267E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.41 | backward-params-all-reduce: 63.44 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.54 | optimizer-unscale-and-check-inf: 20.12 | optimizer-clip-main-grad: 7.71 | optimizer-copy-main-to-model-params: 12.55 | optimizer: 102.08 | batch-generator: 8.70
 iteration   144300/  200000 | consumed samples:      4617600 | elapsed time per iteration (ms): 261.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.247921E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.70 | backward-params-all-reduce: 64.24 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.42 | optimizer-unscale-and-check-inf: 16.73 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.49 | batch-generator: 8.73
 iteration   144400/  200000 | consumed samples:      4620800 | elapsed time per iteration (ms): 262.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.235679E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.36 | backward-params-all-reduce: 64.40 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.64 | optimizer-unscale-and-check-inf: 18.38 | optimizer-clip-main-grad: 7.71 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 100.45 | batch-generator: 8.77
 iteration   144500/  200000 | consumed samples:      4624000 | elapsed time per iteration (ms): 266.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.241366E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.71 | backward-params-all-reduce: 64.61 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.42 | optimizer-unscale-and-check-inf: 18.11 | optimizer-clip-main-grad: 7.73 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.92 | batch-generator: 8.62
--------------------------------------------------------------------------------------------------
 validation loss at iteration 144500 | lm loss value: 3.345690E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 144500, match long value: 0.000533442005675229 | match short value: -0.009204454005703088 
---------------------------------------------------------------------------------------------------------
 iteration   144600/  200000 | consumed samples:      4627200 | elapsed time per iteration (ms): 288.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.234152E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.01 | backward-params-all-reduce: 64.11 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.47 | optimizer-unscale-and-check-inf: 18.54 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 101.66 | batch-generator: 20.94
 iteration   144700/  200000 | consumed samples:      4630400 | elapsed time per iteration (ms): 261.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.239995E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.37 | backward-params-all-reduce: 64.55 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.48 | optimizer-unscale-and-check-inf: 18.92 | optimizer-clip-main-grad: 7.73 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 100.82 | batch-generator: 8.60
 iteration   144800/  200000 | consumed samples:      4633600 | elapsed time per iteration (ms): 261.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.236668E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.95 | backward-params-all-reduce: 65.44 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.15 | optimizer-unscale-and-check-inf: 19.15 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 100.52 | batch-generator: 8.52
 iteration   144900/  200000 | consumed samples:      4636800 | elapsed time per iteration (ms): 263.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.261583E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.49 | backward-params-all-reduce: 64.45 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.34 | optimizer-unscale-and-check-inf: 18.85 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 100.45 | batch-generator: 8.59
 iteration   145000/  200000 | consumed samples:      4640000 | elapsed time per iteration (ms): 260.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.260321E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.28 | backward-params-all-reduce: 64.47 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.27 | optimizer-unscale-and-check-inf: 18.64 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 100.19 | batch-generator: 8.54
--------------------------------------------------------------------------------------------------
 validation loss at iteration 145000 | lm loss value: 3.213992E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 145000, match long value: -0.0006684631883382166 | match short value: -0.01848727249324568 
----------------------------------------------------------------------------------------------------------
 iteration   145100/  200000 | consumed samples:      4643200 | elapsed time per iteration (ms): 275.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.266643E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.04 | backward-params-all-reduce: 64.75 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.50 | optimizer-unscale-and-check-inf: 17.99 | optimizer-clip-main-grad: 7.81 | optimizer-copy-main-to-model-params: 12.40 | optimizer: 99.40 | batch-generator: 15.39
 iteration   145200/  200000 | consumed samples:      4646400 | elapsed time per iteration (ms): 263.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.252836E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.24 | backward-params-all-reduce: 63.18 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.55 | optimizer-unscale-and-check-inf: 17.14 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 12.54 | optimizer: 99.16 | batch-generator: 8.72
 iteration   145300/  200000 | consumed samples:      4649600 | elapsed time per iteration (ms): 273.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.274765E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.90 | backward-params-all-reduce: 63.15 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.10 | optimizer-unscale-and-check-inf: 18.73 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 14.40 | optimizer: 102.09 | batch-generator: 8.59
 iteration   145400/  200000 | consumed samples:      4652800 | elapsed time per iteration (ms): 260.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.260892E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.26 | backward-params-all-reduce: 63.02 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.31 | optimizer-unscale-and-check-inf: 18.22 | optimizer-clip-main-grad: 7.73 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.91 | batch-generator: 8.56
 iteration   145500/  200000 | consumed samples:      4656000 | elapsed time per iteration (ms): 260.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.280263E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.98 | backward-params-all-reduce: 63.75 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.13 | optimizer-unscale-and-check-inf: 18.27 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.68 | batch-generator: 8.52
--------------------------------------------------------------------------------------------------
 validation loss at iteration 145500 | lm loss value: 3.335815E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 145500, match long value: -0.0009384306340335655 | match short value: -0.011494328297768624 
-----------------------------------------------------------------------------------------------------------
 iteration   145600/  200000 | consumed samples:      4659200 | elapsed time per iteration (ms): 275.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.283062E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.12 | backward-params-all-reduce: 65.49 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.10 | optimizer-unscale-and-check-inf: 17.50 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.92 | batch-generator: 16.46
 iteration   145700/  200000 | consumed samples:      4662400 | elapsed time per iteration (ms): 258.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.267813E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.18 | backward-params-all-reduce: 63.81 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.27 | optimizer-unscale-and-check-inf: 16.67 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 98.11 | batch-generator: 8.54
 iteration   145800/  200000 | consumed samples:      4665600 | elapsed time per iteration (ms): 258.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.288657E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.35 | backward-params-all-reduce: 63.65 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.11 | optimizer-unscale-and-check-inf: 17.54 | optimizer-clip-main-grad: 7.44 | optimizer-copy-main-to-model-params: 12.22 | optimizer: 97.48 | batch-generator: 8.54
 iteration   145900/  200000 | consumed samples:      4668800 | elapsed time per iteration (ms): 258.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.267426E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.41 | backward-params-all-reduce: 62.74 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.34 | optimizer-unscale-and-check-inf: 17.28 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 98.84 | batch-generator: 8.47
 iteration   146000/  200000 | consumed samples:      4672000 | elapsed time per iteration (ms): 272.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.300549E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.92 | backward-params-all-reduce: 63.33 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.08 | optimizer-unscale-and-check-inf: 17.03 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 14.34 | optimizer: 100.17 | batch-generator: 8.52
--------------------------------------------------------------------------------------------------
 validation loss at iteration 146000 | lm loss value: 3.141045E-05 | lm loss PPL: 1.000031E+00 | 
 at iteration 146000, match long value: -0.0035833731609656253 | match short value: -0.03471639241164772 
----------------------------------------------------------------------------------------------------------
 iteration   146100/  200000 | consumed samples:      4675200 | elapsed time per iteration (ms): 279.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.278495E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.70 | backward-params-all-reduce: 63.22 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.40 | optimizer-unscale-and-check-inf: 17.50 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 12.56 | optimizer: 99.33 | batch-generator: 19.56
 iteration   146200/  200000 | consumed samples:      4678400 | elapsed time per iteration (ms): 262.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.284317E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.03 | backward-params-all-reduce: 64.42 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.21 | optimizer-unscale-and-check-inf: 18.58 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 100.06 | batch-generator: 8.61
 iteration   146300/  200000 | consumed samples:      4681600 | elapsed time per iteration (ms): 262.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.275240E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.35 | backward-params-all-reduce: 64.32 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.22 | optimizer-unscale-and-check-inf: 18.21 | optimizer-clip-main-grad: 7.75 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 99.88 | batch-generator: 8.78
 iteration   146400/  200000 | consumed samples:      4684800 | elapsed time per iteration (ms): 259.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.301234E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.59 | backward-params-all-reduce: 63.88 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.35 | optimizer-unscale-and-check-inf: 16.37 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 97.99 | batch-generator: 8.56
 iteration   146500/  200000 | consumed samples:      4688000 | elapsed time per iteration (ms): 258.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.295234E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.40 | backward-params-all-reduce: 63.04 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 11.92 | optimizer-unscale-and-check-inf: 17.35 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 98.50 | batch-generator: 8.54
--------------------------------------------------------------------------------------------------
 validation loss at iteration 146500 | lm loss value: 3.295310E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 146500, match long value: -0.0033864347093947875 | match short value: -0.014258292299927384 
-----------------------------------------------------------------------------------------------------------
 iteration   146600/  200000 | consumed samples:      4691200 | elapsed time per iteration (ms): 274.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.296298E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.30 | backward-params-all-reduce: 64.68 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.81 | optimizer-unscale-and-check-inf: 16.22 | optimizer-clip-main-grad: 7.70 | optimizer-copy-main-to-model-params: 12.54 | optimizer: 98.51 | batch-generator: 16.59
 iteration   146700/  200000 | consumed samples:      4694400 | elapsed time per iteration (ms): 265.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.289994E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.24 | backward-params-all-reduce: 63.34 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.13 | optimizer-unscale-and-check-inf: 17.35 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.77 | batch-generator: 8.53
 iteration   146800/  200000 | consumed samples:      4697600 | elapsed time per iteration (ms): 270.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.298320E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.78 | backward-params-all-reduce: 63.71 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.13 | optimizer-unscale-and-check-inf: 17.19 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 14.37 | optimizer: 100.49 | batch-generator: 8.53
 iteration   146900/  200000 | consumed samples:      4700800 | elapsed time per iteration (ms): 265.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.269016E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.48 | backward-params-all-reduce: 63.70 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.38 | optimizer-unscale-and-check-inf: 17.05 | optimizer-clip-main-grad: 7.75 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 98.79 | batch-generator: 13.99
 iteration   147000/  200000 | consumed samples:      4704000 | elapsed time per iteration (ms): 259.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.219096E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.33 | backward-params-all-reduce: 63.78 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.06 | optimizer-unscale-and-check-inf: 15.32 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 97.52 | batch-generator: 8.46
--------------------------------------------------------------------------------------------------
 validation loss at iteration 147000 | lm loss value: 3.346762E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 147000, match long value: 0.0009517104268862909 | match short value: -0.015510997461201896 
----------------------------------------------------------------------------------------------------------
 iteration   147100/  200000 | consumed samples:      4707200 | elapsed time per iteration (ms): 273.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.223444E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.78 | backward-params-all-reduce: 65.17 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.68 | optimizer-unscale-and-check-inf: 16.09 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 97.96 | batch-generator: 16.63
 iteration   147200/  200000 | consumed samples:      4710400 | elapsed time per iteration (ms): 260.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.249241E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.30 | backward-params-all-reduce: 64.17 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.70 | optimizer-unscale-and-check-inf: 14.82 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 96.73 | batch-generator: 8.46
 iteration   147300/  200000 | consumed samples:      4713600 | elapsed time per iteration (ms): 259.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.240355E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.14 | backward-params-all-reduce: 63.97 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.00 | optimizer-unscale-and-check-inf: 15.01 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 97.34 | batch-generator: 8.56
 iteration   147400/  200000 | consumed samples:      4716800 | elapsed time per iteration (ms): 265.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.238547E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.43 | backward-params-all-reduce: 64.83 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.63 | optimizer-unscale-and-check-inf: 15.78 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 97.69 | batch-generator: 8.54
 iteration   147500/  200000 | consumed samples:      4720000 | elapsed time per iteration (ms): 276.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.229972E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.00 | backward-params-all-reduce: 64.35 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.63 | optimizer-unscale-and-check-inf: 14.94 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 14.24 | optimizer: 98.55 | batch-generator: 8.57
--------------------------------------------------------------------------------------------------
 validation loss at iteration 147500 | lm loss value: 3.288045E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 147500, match long value: 0.0006936730823842429 | match short value: -0.012018929915848449 
----------------------------------------------------------------------------------------------------------
 iteration   147600/  200000 | consumed samples:      4723200 | elapsed time per iteration (ms): 275.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.229701E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.96 | backward-params-all-reduce: 63.83 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.45 | optimizer-unscale-and-check-inf: 14.82 | optimizer-clip-main-grad: 7.53 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 96.41 | batch-generator: 20.96
 iteration   147700/  200000 | consumed samples:      4726400 | elapsed time per iteration (ms): 258.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.243550E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.66 | backward-params-all-reduce: 62.73 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.62 | optimizer-unscale-and-check-inf: 15.69 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 97.56 | batch-generator: 8.50
 iteration   147800/  200000 | consumed samples:      4729600 | elapsed time per iteration (ms): 259.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.248390E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.21 | backward-params-all-reduce: 63.45 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.44 | optimizer-unscale-and-check-inf: 15.46 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 97.16 | batch-generator: 8.52
 iteration   147900/  200000 | consumed samples:      4732800 | elapsed time per iteration (ms): 258.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.243514E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.24 | backward-params-all-reduce: 62.78 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.19 | optimizer-unscale-and-check-inf: 15.30 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 96.64 | batch-generator: 8.40
 iteration   148000/  200000 | consumed samples:      4736000 | elapsed time per iteration (ms): 260.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.274376E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.29 | backward-params-all-reduce: 63.66 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.35 | optimizer-unscale-and-check-inf: 15.52 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.55 | optimizer: 97.17 | batch-generator: 8.50
--------------------------------------------------------------------------------------------------
 validation loss at iteration 148000 | lm loss value: 3.372406E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 148000, match long value: -0.0005251043926004356 | match short value: -0.021902103707868575 
-----------------------------------------------------------------------------------------------------------
 iteration   148100/  200000 | consumed samples:      4739200 | elapsed time per iteration (ms): 272.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.260820E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.98 | backward-params-all-reduce: 64.98 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.42 | optimizer-unscale-and-check-inf: 15.34 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 97.06 | batch-generator: 15.84
 iteration   148200/  200000 | consumed samples:      4742400 | elapsed time per iteration (ms): 273.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.246045E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.97 | backward-params-all-reduce: 63.92 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.47 | optimizer-unscale-and-check-inf: 15.71 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 14.48 | optimizer: 99.44 | batch-generator: 8.52
 iteration   148300/  200000 | consumed samples:      4745600 | elapsed time per iteration (ms): 261.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.268275E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.38 | backward-params-all-reduce: 63.88 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.58 | optimizer-unscale-and-check-inf: 14.73 | optimizer-clip-main-grad: 7.52 | optimizer-copy-main-to-model-params: 12.55 | optimizer: 96.52 | batch-generator: 8.54
 iteration   148400/  200000 | consumed samples:      4748800 | elapsed time per iteration (ms): 258.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.246073E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.30 | backward-params-all-reduce: 62.54 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.28 | optimizer-unscale-and-check-inf: 14.78 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 96.28 | batch-generator: 8.55
 iteration   148500/  200000 | consumed samples:      4752000 | elapsed time per iteration (ms): 257.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.260129E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.31 | backward-params-all-reduce: 62.54 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.12 | optimizer-unscale-and-check-inf: 15.37 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 96.71 | batch-generator: 8.53
--------------------------------------------------------------------------------------------------
 validation loss at iteration 148500 | lm loss value: 3.223316E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 148500, match long value: -0.0016878948177189827 | match short value: -0.011612313155158325 
-----------------------------------------------------------------------------------------------------------
 iteration   148600/  200000 | consumed samples:      4755200 | elapsed time per iteration (ms): 273.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.261604E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.19 | backward-params-all-reduce: 63.78 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.25 | optimizer-unscale-and-check-inf: 18.78 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 100.31 | batch-generator: 15.87
 iteration   148700/  200000 | consumed samples:      4758400 | elapsed time per iteration (ms): 261.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.271515E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.25 | backward-params-all-reduce: 63.81 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.09 | optimizer-unscale-and-check-inf: 19.68 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 101.11 | batch-generator: 8.49
 iteration   148800/  200000 | consumed samples:      4761600 | elapsed time per iteration (ms): 259.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.268732E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.59 | backward-params-all-reduce: 65.65 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 11.93 | optimizer-unscale-and-check-inf: 17.36 | optimizer-clip-main-grad: 7.49 | optimizer-copy-main-to-model-params: 12.35 | optimizer: 97.76 | batch-generator: 8.40
 iteration   148900/  200000 | consumed samples:      4764800 | elapsed time per iteration (ms): 346.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.289065E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.26 | backward-params-all-reduce: 63.70 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.00 | optimizer-unscale-and-check-inf: 17.79 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.02 | batch-generator: 88.62
 iteration   149000/  200000 | consumed samples:      4768000 | elapsed time per iteration (ms): 268.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.262849E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.77 | backward-params-all-reduce: 63.77 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.07 | optimizer-unscale-and-check-inf: 17.95 | optimizer-clip-main-grad: 7.48 | optimizer-copy-main-to-model-params: 13.96 | optimizer: 99.62 | batch-generator: 8.45
--------------------------------------------------------------------------------------------------
 validation loss at iteration 149000 | lm loss value: 3.323076E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 149000, match long value: -0.0017612724145535468 | match short value: -0.006433430154864216 
-----------------------------------------------------------------------------------------------------------
 iteration   149100/  200000 | consumed samples:      4771200 | elapsed time per iteration (ms): 276.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.272240E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.00 | backward-params-all-reduce: 62.74 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.42 | optimizer-unscale-and-check-inf: 18.77 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 100.44 | batch-generator: 19.13
 iteration   149200/  200000 | consumed samples:      4774400 | elapsed time per iteration (ms): 259.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.277317E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.47 | backward-params-all-reduce: 64.22 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.31 | optimizer-unscale-and-check-inf: 18.70 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 100.28 | batch-generator: 8.46
 iteration   149300/  200000 | consumed samples:      4777600 | elapsed time per iteration (ms): 258.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.269621E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.25 | backward-params-all-reduce: 63.84 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.16 | optimizer-unscale-and-check-inf: 17.54 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 98.87 | batch-generator: 8.48
 iteration   149400/  200000 | consumed samples:      4780800 | elapsed time per iteration (ms): 259.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.283091E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.20 | backward-params-all-reduce: 63.94 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.75 | optimizer-unscale-and-check-inf: 17.34 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.37 | batch-generator: 8.52
 iteration   149500/  200000 | consumed samples:      4784000 | elapsed time per iteration (ms): 259.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.281565E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.96 | backward-params-all-reduce: 63.66 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.25 | optimizer-unscale-and-check-inf: 17.94 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.41 | batch-generator: 8.39
--------------------------------------------------------------------------------------------------
 validation loss at iteration 149500 | lm loss value: 3.260230E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 149500, match long value: -0.003937008288978446 | match short value: -0.014841595632184198 
----------------------------------------------------------------------------------------------------------
 iteration   149600/  200000 | consumed samples:      4787200 | elapsed time per iteration (ms): 269.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.281508E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.19 | backward-params-all-reduce: 63.69 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.50 | optimizer-unscale-and-check-inf: 16.65 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.36 | batch-generator: 14.87
 iteration   149700/  200000 | consumed samples:      4790400 | elapsed time per iteration (ms): 269.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.268571E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.93 | backward-params-all-reduce: 63.84 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.19 | optimizer-unscale-and-check-inf: 17.23 | optimizer-clip-main-grad: 7.53 | optimizer-copy-main-to-model-params: 14.20 | optimizer: 100.28 | batch-generator: 8.53
 iteration   149800/  200000 | consumed samples:      4793600 | elapsed time per iteration (ms): 259.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.268338E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.33 | backward-params-all-reduce: 64.09 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.24 | optimizer-unscale-and-check-inf: 17.82 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 99.22 | batch-generator: 8.51
 iteration   149900/  200000 | consumed samples:      4796800 | elapsed time per iteration (ms): 257.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.282138E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.16 | backward-params-all-reduce: 62.67 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.36 | optimizer-unscale-and-check-inf: 17.70 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.31 | batch-generator: 8.57
 iteration   150000/  200000 | consumed samples:      4800000 | elapsed time per iteration (ms): 259.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.287811E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.23 | backward-params-all-reduce: 64.04 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.49 | optimizer-unscale-and-check-inf: 17.70 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.43 | batch-generator: 8.41
--------------------------------------------------------------------------------------------------
 validation loss at iteration 150000 | lm loss value: 3.219318E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 150000, match long value: -0.001144513869419475 | match short value: -0.008002307056649194 
----------------------------------------------------------------------------------------------------------
 iteration   150100/  200000 | consumed samples:      4803200 | elapsed time per iteration (ms): 275.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.230918E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.59 | backward-params-all-reduce: 63.96 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.84 | optimizer-unscale-and-check-inf: 17.60 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 99.62 | batch-generator: 20.12
 iteration   150200/  200000 | consumed samples:      4806400 | elapsed time per iteration (ms): 258.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.230886E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.50 | backward-params-all-reduce: 64.00 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.31 | optimizer-unscale-and-check-inf: 16.93 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 99.48 | batch-generator: 8.49
 iteration   150300/  200000 | consumed samples:      4809600 | elapsed time per iteration (ms): 260.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.225736E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.71 | backward-params-all-reduce: 65.85 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.29 | optimizer-unscale-and-check-inf: 17.25 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 99.78 | batch-generator: 8.49
 iteration   150400/  200000 | consumed samples:      4812800 | elapsed time per iteration (ms): 269.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.225662E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.70 | backward-params-all-reduce: 65.03 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.37 | optimizer-unscale-and-check-inf: 16.27 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 14.27 | optimizer: 100.72 | batch-generator: 8.70
 iteration   150500/  200000 | consumed samples:      4816000 | elapsed time per iteration (ms): 261.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.229158E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.84 | backward-params-all-reduce: 64.95 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.28 | optimizer-unscale-and-check-inf: 16.11 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.54 | optimizer: 98.79 | batch-generator: 8.52
--------------------------------------------------------------------------------------------------
 validation loss at iteration 150500 | lm loss value: 3.238952E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 150500, match long value: 0.0018433561099994555 | match short value: -0.028467248172761216 
----------------------------------------------------------------------------------------------------------
 iteration   150600/  200000 | consumed samples:      4819200 | elapsed time per iteration (ms): 276.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.231188E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.42 | backward-params-all-reduce: 65.94 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.19 | optimizer-unscale-and-check-inf: 16.71 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.18 | batch-generator: 18.95
 iteration   150700/  200000 | consumed samples:      4822400 | elapsed time per iteration (ms): 258.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.239235E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.54 | backward-params-all-reduce: 63.49 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.89 | optimizer-unscale-and-check-inf: 16.21 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.39 | batch-generator: 8.58
 iteration   150800/  200000 | consumed samples:      4825600 | elapsed time per iteration (ms): 261.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.234962E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.69 | backward-params-all-reduce: 64.42 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.79 | optimizer-unscale-and-check-inf: 16.53 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.54 | optimizer: 99.64 | batch-generator: 8.54
 iteration   150900/  200000 | consumed samples:      4828800 | elapsed time per iteration (ms): 258.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.223402E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.19 | backward-params-all-reduce: 64.42 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.87 | optimizer-unscale-and-check-inf: 16.51 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 99.69 | batch-generator: 8.59
 iteration   151000/  200000 | consumed samples:      4832000 | elapsed time per iteration (ms): 258.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.250361E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.53 | backward-params-all-reduce: 64.84 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.39 | optimizer-unscale-and-check-inf: 17.00 | optimizer-clip-main-grad: 7.53 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 99.59 | batch-generator: 8.45
--------------------------------------------------------------------------------------------------
 validation loss at iteration 151000 | lm loss value: 3.399533E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 151000, match long value: -0.001888185541562589 | match short value: -0.027399704388299948 
----------------------------------------------------------------------------------------------------------
 iteration   151100/  200000 | consumed samples:      4835200 | elapsed time per iteration (ms): 279.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.241185E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.75 | backward-params-all-reduce: 64.30 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.69 | optimizer-unscale-and-check-inf: 17.25 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 14.32 | optimizer: 102.05 | batch-generator: 15.44
 iteration   151200/  200000 | consumed samples:      4838400 | elapsed time per iteration (ms): 265.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.246981E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 54.03 | backward-params-all-reduce: 63.93 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.32 | optimizer-unscale-and-check-inf: 16.37 | optimizer-clip-main-grad: 7.53 | optimizer-copy-main-to-model-params: 12.36 | optimizer: 98.25 | batch-generator: 8.53
 iteration   151300/  200000 | consumed samples:      4841600 | elapsed time per iteration (ms): 258.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.222535E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.16 | backward-params-all-reduce: 64.02 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.38 | optimizer-unscale-and-check-inf: 17.05 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.67 | batch-generator: 8.57
 iteration   151400/  200000 | consumed samples:      4844800 | elapsed time per iteration (ms): 258.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.253492E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.00 | backward-params-all-reduce: 65.20 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.61 | optimizer-unscale-and-check-inf: 17.13 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.40 | optimizer: 99.46 | batch-generator: 8.67
 iteration   151500/  200000 | consumed samples:      4848000 | elapsed time per iteration (ms): 257.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.247277E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.16 | backward-params-all-reduce: 63.37 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.49 | optimizer-unscale-and-check-inf: 15.93 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 98.67 | batch-generator: 8.57
--------------------------------------------------------------------------------------------------
 validation loss at iteration 151500 | lm loss value: 3.156808E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 151500, match long value: 0.004391436311607381 | match short value: -0.010085500411761752 
---------------------------------------------------------------------------------------------------------
 iteration   151600/  200000 | consumed samples:      4851200 | elapsed time per iteration (ms): 270.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.256529E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.06 | backward-params-all-reduce: 63.30 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 14.65 | optimizer-unscale-and-check-inf: 15.25 | optimizer-clip-main-grad: 7.72 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.29 | batch-generator: 14.81
 iteration   151700/  200000 | consumed samples:      4854400 | elapsed time per iteration (ms): 259.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.254355E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.32 | backward-params-all-reduce: 64.18 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.65 | optimizer-unscale-and-check-inf: 16.65 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 98.55 | batch-generator: 8.45
 iteration   151800/  200000 | consumed samples:      4857600 | elapsed time per iteration (ms): 260.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.265709E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.43 | backward-params-all-reduce: 64.00 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 14.34 | optimizer-unscale-and-check-inf: 15.37 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.54 | optimizer: 99.05 | batch-generator: 8.70
 iteration   151900/  200000 | consumed samples:      4860800 | elapsed time per iteration (ms): 270.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.274287E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.21 | backward-params-all-reduce: 64.11 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.53 | optimizer-unscale-and-check-inf: 15.19 | optimizer-clip-main-grad: 7.54 | optimizer-copy-main-to-model-params: 14.35 | optimizer: 99.77 | batch-generator: 8.51
 iteration   152000/  200000 | consumed samples:      4864000 | elapsed time per iteration (ms): 261.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.246864E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.31 | backward-params-all-reduce: 66.14 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.48 | optimizer-unscale-and-check-inf: 16.22 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.89 | batch-generator: 8.53
--------------------------------------------------------------------------------------------------
 validation loss at iteration 152000 | lm loss value: 3.396273E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 152000, match long value: 0.0009089434024083136 | match short value: -0.021514724200281495 
----------------------------------------------------------------------------------------------------------
 iteration   152100/  200000 | consumed samples:      4867200 | elapsed time per iteration (ms): 274.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.264120E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.46 | backward-params-all-reduce: 63.47 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.09 | optimizer-unscale-and-check-inf: 15.64 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.00 | batch-generator: 20.64
 iteration   152200/  200000 | consumed samples:      4870400 | elapsed time per iteration (ms): 257.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.259987E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.96 | backward-params-all-reduce: 62.55 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.89 | optimizer-unscale-and-check-inf: 15.65 | optimizer-clip-main-grad: 7.52 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 97.69 | batch-generator: 8.51
 iteration   152300/  200000 | consumed samples:      4873600 | elapsed time per iteration (ms): 256.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.269020E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.40 | backward-params-all-reduce: 63.58 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.59 | optimizer-unscale-and-check-inf: 15.70 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.49 | batch-generator: 8.50
 iteration   152400/  200000 | consumed samples:      4876800 | elapsed time per iteration (ms): 259.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.254416E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.07 | backward-params-all-reduce: 64.41 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.40 | optimizer-unscale-and-check-inf: 15.60 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.36 | batch-generator: 8.49
 iteration   152500/  200000 | consumed samples:      4880000 | elapsed time per iteration (ms): 259.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.277009E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.55 | backward-params-all-reduce: 63.25 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.85 | optimizer-unscale-and-check-inf: 16.49 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 98.60 | batch-generator: 8.56
--------------------------------------------------------------------------------------------------
 validation loss at iteration 152500 | lm loss value: 3.253066E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 152500, match long value: -0.0010706031766661236 | match short value: -0.029945236006235207 
-----------------------------------------------------------------------------------------------------------
 iteration   152600/  200000 | consumed samples:      4883200 | elapsed time per iteration (ms): 280.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.272333E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.86 | backward-params-all-reduce: 65.16 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.22 | optimizer-unscale-and-check-inf: 15.53 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 14.28 | optimizer: 99.79 | batch-generator: 14.25
 iteration   152700/  200000 | consumed samples:      4886400 | elapsed time per iteration (ms): 261.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.267603E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.31 | backward-params-all-reduce: 64.33 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.29 | optimizer-unscale-and-check-inf: 17.65 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.42 | optimizer: 98.57 | batch-generator: 8.55
 iteration   152800/  200000 | consumed samples:      4889600 | elapsed time per iteration (ms): 259.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.289123E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.26 | backward-params-all-reduce: 63.27 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.39 | optimizer-unscale-and-check-inf: 17.11 | optimizer-clip-main-grad: 7.84 | optimizer-copy-main-to-model-params: 12.56 | optimizer: 99.10 | batch-generator: 8.66
 iteration   152900/  200000 | consumed samples:      4892800 | elapsed time per iteration (ms): 261.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.295189E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.20 | backward-params-all-reduce: 64.01 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.41 | optimizer-unscale-and-check-inf: 17.03 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.70 | batch-generator: 8.52
 iteration   153000/  200000 | consumed samples:      4896000 | elapsed time per iteration (ms): 260.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.272424E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.20 | backward-params-all-reduce: 64.11 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.18 | optimizer-unscale-and-check-inf: 18.02 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.47 | batch-generator: 8.57
--------------------------------------------------------------------------------------------------
 validation loss at iteration 153000 | lm loss value: 3.339747E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 153000, match long value: -0.0018882984027271856 | match short value: -0.02363995441968513 
----------------------------------------------------------------------------------------------------------
 iteration   153100/  200000 | consumed samples:      4899200 | elapsed time per iteration (ms): 272.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.300478E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.40 | backward-params-all-reduce: 63.61 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.54 | optimizer-unscale-and-check-inf: 16.73 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.59 | batch-generator: 15.81
 iteration   153200/  200000 | consumed samples:      4902400 | elapsed time per iteration (ms): 266.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.241463E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.24 | backward-params-all-reduce: 64.98 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.71 | optimizer-unscale-and-check-inf: 16.84 | optimizer-clip-main-grad: 7.73 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.94 | batch-generator: 14.41
 iteration   153300/  200000 | consumed samples:      4905600 | elapsed time per iteration (ms): 268.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.219276E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.50 | backward-params-all-reduce: 65.34 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.34 | optimizer-unscale-and-check-inf: 17.88 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 14.22 | optimizer: 101.29 | batch-generator: 8.61
 iteration   153400/  200000 | consumed samples:      4908800 | elapsed time per iteration (ms): 268.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.227735E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.10 | backward-params-all-reduce: 64.14 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.54 | optimizer-unscale-and-check-inf: 17.04 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 98.84 | batch-generator: 8.50
 iteration   153500/  200000 | consumed samples:      4912000 | elapsed time per iteration (ms): 261.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.229997E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.25 | backward-params-all-reduce: 63.97 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.30 | optimizer-unscale-and-check-inf: 16.93 | optimizer-clip-main-grad: 7.80 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 99.72 | batch-generator: 8.57
--------------------------------------------------------------------------------------------------
 validation loss at iteration 153500 | lm loss value: 3.287456E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 153500, match long value: -0.0017279739174635537 | match short value: -0.03246198931072117 
----------------------------------------------------------------------------------------------------------
 iteration   153600/  200000 | consumed samples:      4915200 | elapsed time per iteration (ms): 280.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.215859E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.30 | backward-params-all-reduce: 64.88 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.17 | optimizer-unscale-and-check-inf: 16.55 | optimizer-clip-main-grad: 7.75 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 99.23 | batch-generator: 21.58
 iteration   153700/  200000 | consumed samples:      4918400 | elapsed time per iteration (ms): 258.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.222169E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.09 | backward-params-all-reduce: 63.67 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.81 | optimizer-unscale-and-check-inf: 18.10 | optimizer-clip-main-grad: 7.51 | optimizer-copy-main-to-model-params: 12.22 | optimizer: 98.81 | batch-generator: 8.43
 iteration   153800/  200000 | consumed samples:      4921600 | elapsed time per iteration (ms): 260.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.240519E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.92 | backward-params-all-reduce: 64.34 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.24 | optimizer-unscale-and-check-inf: 17.66 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.17 | batch-generator: 8.49
 iteration   153900/  200000 | consumed samples:      4924800 | elapsed time per iteration (ms): 262.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.234311E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.09 | backward-params-all-reduce: 64.72 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.80 | optimizer-unscale-and-check-inf: 17.25 | optimizer-clip-main-grad: 7.72 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.44 | batch-generator: 8.61
 iteration   154000/  200000 | consumed samples:      4928000 | elapsed time per iteration (ms): 261.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.232383E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.61 | backward-params-all-reduce: 63.38 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.26 | optimizer-unscale-and-check-inf: 17.88 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.43 | batch-generator: 8.48
--------------------------------------------------------------------------------------------------
 validation loss at iteration 154000 | lm loss value: 3.272175E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 154000, match long value: -0.0006147201485794253 | match short value: -0.017647402130015914 
-----------------------------------------------------------------------------------------------------------
 iteration   154100/  200000 | consumed samples:      4931200 | elapsed time per iteration (ms): 286.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.232243E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.96 | backward-params-all-reduce: 65.01 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.14 | optimizer-unscale-and-check-inf: 16.87 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 14.24 | optimizer: 99.98 | batch-generator: 15.15
 iteration   154200/  200000 | consumed samples:      4934400 | elapsed time per iteration (ms): 260.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.256179E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.21 | backward-params-all-reduce: 64.21 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.02 | optimizer-unscale-and-check-inf: 17.42 | optimizer-clip-main-grad: 7.72 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.86 | batch-generator: 8.38
 iteration   154300/  200000 | consumed samples:      4937600 | elapsed time per iteration (ms): 259.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.255256E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.48 | backward-params-all-reduce: 63.01 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.43 | optimizer-unscale-and-check-inf: 17.48 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.16 | batch-generator: 8.48
 iteration   154400/  200000 | consumed samples:      4940800 | elapsed time per iteration (ms): 261.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.258312E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.56 | backward-params-all-reduce: 63.69 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.49 | optimizer-unscale-and-check-inf: 17.34 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.11 | batch-generator: 8.47
 iteration   154500/  200000 | consumed samples:      4944000 | elapsed time per iteration (ms): 257.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.246642E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.18 | backward-params-all-reduce: 63.94 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.09 | optimizer-unscale-and-check-inf: 16.57 | optimizer-clip-main-grad: 7.53 | optimizer-copy-main-to-model-params: 12.39 | optimizer: 97.23 | batch-generator: 8.59
--------------------------------------------------------------------------------------------------
 validation loss at iteration 154500 | lm loss value: 3.284547E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 154500, match long value: 0.0006601730723763774 | match short value: -0.026486758869020483 
----------------------------------------------------------------------------------------------------------
 iteration   154600/  200000 | consumed samples:      4947200 | elapsed time per iteration (ms): 274.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.249681E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.52 | backward-params-all-reduce: 65.15 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.59 | optimizer-unscale-and-check-inf: 16.07 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 97.93 | batch-generator: 15.22
 iteration   154700/  200000 | consumed samples:      4950400 | elapsed time per iteration (ms): 263.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.252349E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.96 | backward-params-all-reduce: 64.76 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.43 | optimizer-unscale-and-check-inf: 16.33 | optimizer-clip-main-grad: 7.92 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 98.32 | batch-generator: 8.87
 iteration   154800/  200000 | consumed samples:      4953600 | elapsed time per iteration (ms): 271.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.251328E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 54.44 | backward-params-all-reduce: 62.11 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.72 | optimizer-unscale-and-check-inf: 16.30 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 14.26 | optimizer: 100.06 | batch-generator: 8.60
 iteration   154900/  200000 | consumed samples:      4956800 | elapsed time per iteration (ms): 261.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.267425E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.30 | backward-params-all-reduce: 63.32 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.76 | optimizer-unscale-and-check-inf: 16.46 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 98.47 | batch-generator: 8.63
 iteration   155000/  200000 | consumed samples:      4960000 | elapsed time per iteration (ms): 258.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.256491E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.21 | backward-params-all-reduce: 62.61 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.31 | optimizer-unscale-and-check-inf: 16.52 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.12 | batch-generator: 8.67
--------------------------------------------------------------------------------------------------
 validation loss at iteration 155000 | lm loss value: 3.255711E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 155000, match long value: -0.0038074045931064554 | match short value: -0.008476994151113599 
-----------------------------------------------------------------------------------------------------------
 iteration   155100/  200000 | consumed samples:      4963200 | elapsed time per iteration (ms): 273.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.256013E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.21 | backward-params-all-reduce: 63.64 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.57 | optimizer-unscale-and-check-inf: 15.90 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 97.73 | batch-generator: 15.76
 iteration   155200/  200000 | consumed samples:      4966400 | elapsed time per iteration (ms): 260.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.272479E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.52 | backward-params-all-reduce: 63.10 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.31 | optimizer-unscale-and-check-inf: 17.72 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.24 | batch-generator: 8.63
 iteration   155300/  200000 | consumed samples:      4969600 | elapsed time per iteration (ms): 260.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.253224E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.20 | backward-params-all-reduce: 63.77 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.36 | optimizer-unscale-and-check-inf: 17.20 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.54 | optimizer: 98.91 | batch-generator: 8.72
 iteration   155400/  200000 | consumed samples:      4972800 | elapsed time per iteration (ms): 259.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.263961E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.21 | backward-params-all-reduce: 63.49 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.74 | optimizer-unscale-and-check-inf: 17.00 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.39 | optimizer: 98.40 | batch-generator: 8.61
 iteration   155500/  200000 | consumed samples:      4976000 | elapsed time per iteration (ms): 266.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.263263E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.65 | backward-params-all-reduce: 63.89 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.17 | optimizer-unscale-and-check-inf: 16.88 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.70 | batch-generator: 8.65
--------------------------------------------------------------------------------------------------
 validation loss at iteration 155500 | lm loss value: 3.335672E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 155500, match long value: 0.0007668789588491392 | match short value: -0.007644358785534166 
----------------------------------------------------------------------------------------------------------
 iteration   155600/  200000 | consumed samples:      4979200 | elapsed time per iteration (ms): 285.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.259435E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.24 | backward-params-all-reduce: 62.73 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.42 | optimizer-unscale-and-check-inf: 15.93 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 97.66 | batch-generator: 21.35
 iteration   155700/  200000 | consumed samples:      4982400 | elapsed time per iteration (ms): 260.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.277752E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.25 | backward-params-all-reduce: 63.81 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.38 | optimizer-unscale-and-check-inf: 16.48 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.13 | batch-generator: 8.59
 iteration   155800/  200000 | consumed samples:      4985600 | elapsed time per iteration (ms): 260.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.298561E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.34 | backward-params-all-reduce: 63.55 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.61 | optimizer-unscale-and-check-inf: 16.14 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.58 | optimizer: 98.11 | batch-generator: 8.56
 iteration   155900/  200000 | consumed samples:      4988800 | elapsed time per iteration (ms): 258.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.276287E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.33 | backward-params-all-reduce: 62.76 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.21 | optimizer-unscale-and-check-inf: 15.69 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 97.16 | batch-generator: 8.56
 iteration   156000/  200000 | consumed samples:      4992000 | elapsed time per iteration (ms): 259.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.268461E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.22 | backward-params-all-reduce: 63.46 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.13 | optimizer-unscale-and-check-inf: 16.83 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.29 | batch-generator: 8.73
--------------------------------------------------------------------------------------------------
 validation loss at iteration 156000 | lm loss value: 3.226357E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 156000, match long value: 0.0023165457077642423 | match short value: -0.008848753229791336 
----------------------------------------------------------------------------------------------------------
 iteration   156100/  200000 | consumed samples:      4995200 | elapsed time per iteration (ms): 273.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.270556E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.32 | backward-params-all-reduce: 64.27 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.12 | optimizer-unscale-and-check-inf: 16.50 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.54 | optimizer: 97.96 | batch-generator: 15.60
 iteration   156200/  200000 | consumed samples:      4998400 | elapsed time per iteration (ms): 260.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.275432E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.36 | backward-params-all-reduce: 62.87 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.14 | optimizer-unscale-and-check-inf: 16.17 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 97.54 | batch-generator: 8.56
 iteration   156300/  200000 | consumed samples:      5001600 | elapsed time per iteration (ms): 282.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.260223E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.85 | backward-params-all-reduce: 63.94 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.42 | optimizer-unscale-and-check-inf: 16.08 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 14.25 | optimizer: 99.53 | batch-generator: 14.15
 iteration   156400/  200000 | consumed samples:      5004800 | elapsed time per iteration (ms): 259.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.218551E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.57 | backward-params-all-reduce: 63.24 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.71 | optimizer-unscale-and-check-inf: 17.18 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.12 | batch-generator: 8.57
 iteration   156500/  200000 | consumed samples:      5008000 | elapsed time per iteration (ms): 262.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.208749E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.61 | backward-params-all-reduce: 63.15 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.29 | optimizer-unscale-and-check-inf: 16.82 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.37 | batch-generator: 8.59
--------------------------------------------------------------------------------------------------
 validation loss at iteration 156500 | lm loss value: 3.287757E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 156500, match long value: -0.0010534225250242325 | match short value: -0.004814704646833721 
-----------------------------------------------------------------------------------------------------------
 iteration   156600/  200000 | consumed samples:      5011200 | elapsed time per iteration (ms): 271.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.221626E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.30 | backward-params-all-reduce: 64.03 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.20 | optimizer-unscale-and-check-inf: 18.01 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 99.44 | batch-generator: 16.41
 iteration   156700/  200000 | consumed samples:      5014400 | elapsed time per iteration (ms): 258.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.227851E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.39 | backward-params-all-reduce: 62.93 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.60 | optimizer-unscale-and-check-inf: 16.69 | optimizer-clip-main-grad: 7.71 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.70 | batch-generator: 8.72
 iteration   156800/  200000 | consumed samples:      5017600 | elapsed time per iteration (ms): 259.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.214450E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.58 | backward-params-all-reduce: 63.09 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.63 | optimizer-unscale-and-check-inf: 16.66 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.61 | batch-generator: 8.63
 iteration   156900/  200000 | consumed samples:      5020800 | elapsed time per iteration (ms): 260.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.234057E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.40 | backward-params-all-reduce: 63.01 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.12 | optimizer-unscale-and-check-inf: 17.49 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.92 | batch-generator: 8.57
 iteration   157000/  200000 | consumed samples:      5024000 | elapsed time per iteration (ms): 271.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.228744E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.63 | backward-params-all-reduce: 62.68 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.60 | optimizer-unscale-and-check-inf: 16.69 | optimizer-clip-main-grad: 7.76 | optimizer-copy-main-to-model-params: 14.28 | optimizer: 100.51 | batch-generator: 8.72
--------------------------------------------------------------------------------------------------
 validation loss at iteration 157000 | lm loss value: 3.164820E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 157000, match long value: -0.00138513630173534 | match short value: -0.013707036371171542 
---------------------------------------------------------------------------------------------------------
 iteration   157100/  200000 | consumed samples:      5027200 | elapsed time per iteration (ms): 280.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.228694E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.63 | backward-params-all-reduce: 63.39 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.87 | optimizer-unscale-and-check-inf: 18.30 | optimizer-clip-main-grad: 7.76 | optimizer-copy-main-to-model-params: 12.58 | optimizer: 100.70 | batch-generator: 21.65
 iteration   157200/  200000 | consumed samples:      5030400 | elapsed time per iteration (ms): 262.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.243780E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.42 | backward-params-all-reduce: 64.82 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.50 | optimizer-unscale-and-check-inf: 19.51 | optimizer-clip-main-grad: 7.78 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 101.49 | batch-generator: 8.84
 iteration   157300/  200000 | consumed samples:      5033600 | elapsed time per iteration (ms): 262.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.246085E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.72 | backward-params-all-reduce: 63.84 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.29 | optimizer-unscale-and-check-inf: 19.05 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 100.64 | batch-generator: 8.60
 iteration   157400/  200000 | consumed samples:      5036800 | elapsed time per iteration (ms): 344.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.230619E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.70 | backward-params-all-reduce: 62.87 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.11 | optimizer-unscale-and-check-inf: 19.28 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 100.65 | batch-generator: 91.00
 iteration   157500/  200000 | consumed samples:      5040000 | elapsed time per iteration (ms): 260.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.237502E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.87 | backward-params-all-reduce: 64.38 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.60 | optimizer-unscale-and-check-inf: 17.43 | optimizer-clip-main-grad: 7.72 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.43 | batch-generator: 8.58
--------------------------------------------------------------------------------------------------
 validation loss at iteration 157500 | lm loss value: 3.392551E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 157500, match long value: -0.001747520427553444 | match short value: -0.03489742029338646 
---------------------------------------------------------------------------------------------------------
 iteration   157600/  200000 | consumed samples:      5043200 | elapsed time per iteration (ms): 276.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.239241E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.56 | backward-params-all-reduce: 66.41 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.51 | optimizer-unscale-and-check-inf: 18.09 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.94 | batch-generator: 15.93
 iteration   157700/  200000 | consumed samples:      5046400 | elapsed time per iteration (ms): 267.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.250908E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.23 | backward-params-all-reduce: 63.59 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.00 | optimizer-unscale-and-check-inf: 17.48 | optimizer-clip-main-grad: 7.73 | optimizer-copy-main-to-model-params: 14.35 | optimizer: 101.73 | batch-generator: 8.63
 iteration   157800/  200000 | consumed samples:      5049600 | elapsed time per iteration (ms): 269.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.245378E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.51 | backward-params-all-reduce: 63.95 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.58 | optimizer-unscale-and-check-inf: 19.30 | optimizer-clip-main-grad: 7.47 | optimizer-copy-main-to-model-params: 12.22 | optimizer: 99.73 | batch-generator: 8.48
 iteration   157900/  200000 | consumed samples:      5052800 | elapsed time per iteration (ms): 261.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.247906E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.93 | backward-params-all-reduce: 63.54 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.52 | optimizer-unscale-and-check-inf: 16.93 | optimizer-clip-main-grad: 7.84 | optimizer-copy-main-to-model-params: 12.55 | optimizer: 100.04 | batch-generator: 8.77
 iteration   158000/  200000 | consumed samples:      5056000 | elapsed time per iteration (ms): 258.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.256022E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.18 | backward-params-all-reduce: 62.56 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.90 | optimizer-unscale-and-check-inf: 18.01 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 100.17 | batch-generator: 8.53
--------------------------------------------------------------------------------------------------
 validation loss at iteration 158000 | lm loss value: 3.358043E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 158000, match long value: -0.0016448625698926688 | match short value: -0.02529847718273754 
----------------------------------------------------------------------------------------------------------
 iteration   158100/  200000 | consumed samples:      5059200 | elapsed time per iteration (ms): 270.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.248990E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.21 | backward-params-all-reduce: 63.25 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.37 | optimizer-unscale-and-check-inf: 17.30 | optimizer-clip-main-grad: 7.70 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 100.03 | batch-generator: 15.77
 iteration   158200/  200000 | consumed samples:      5062400 | elapsed time per iteration (ms): 258.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.271527E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.16 | backward-params-all-reduce: 63.63 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.49 | optimizer-unscale-and-check-inf: 17.40 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 100.19 | batch-generator: 8.52
 iteration   158300/  200000 | consumed samples:      5065600 | elapsed time per iteration (ms): 258.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.260759E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.06 | backward-params-all-reduce: 62.86 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.28 | optimizer-unscale-and-check-inf: 16.89 | optimizer-clip-main-grad: 7.75 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 99.65 | batch-generator: 8.62
 iteration   158400/  200000 | consumed samples:      5068800 | elapsed time per iteration (ms): 261.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.259821E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.88 | backward-params-all-reduce: 63.57 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.70 | optimizer-unscale-and-check-inf: 16.10 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.18 | batch-generator: 8.65
 iteration   158500/  200000 | consumed samples:      5072000 | elapsed time per iteration (ms): 271.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.272749E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.66 | backward-params-all-reduce: 64.48 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.40 | optimizer-unscale-and-check-inf: 15.98 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 14.18 | optimizer: 100.39 | batch-generator: 8.60
--------------------------------------------------------------------------------------------------
 validation loss at iteration 158500 | lm loss value: 3.193641E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 158500, match long value: -0.0014501512192666057 | match short value: -0.022937553403138276 
-----------------------------------------------------------------------------------------------------------
 iteration   158600/  200000 | consumed samples:      5075200 | elapsed time per iteration (ms): 275.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.272971E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.37 | backward-params-all-reduce: 63.13 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.83 | optimizer-unscale-and-check-inf: 17.08 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.21 | batch-generator: 21.22
 iteration   158700/  200000 | consumed samples:      5078400 | elapsed time per iteration (ms): 258.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.268994E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.87 | backward-params-all-reduce: 64.50 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.10 | optimizer-unscale-and-check-inf: 16.80 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.13 | batch-generator: 8.49
 iteration   158800/  200000 | consumed samples:      5081600 | elapsed time per iteration (ms): 259.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.268603E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.55 | backward-params-all-reduce: 64.04 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.58 | optimizer-unscale-and-check-inf: 16.53 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 99.45 | batch-generator: 8.49
 iteration   158900/  200000 | consumed samples:      5084800 | elapsed time per iteration (ms): 258.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.277544E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.40 | backward-params-all-reduce: 63.71 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.91 | optimizer-unscale-and-check-inf: 17.13 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.23 | batch-generator: 8.44
 iteration   159000/  200000 | consumed samples:      5088000 | elapsed time per iteration (ms): 258.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.272606E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.23 | backward-params-all-reduce: 62.97 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.78 | optimizer-unscale-and-check-inf: 16.75 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.75 | batch-generator: 8.41
--------------------------------------------------------------------------------------------------
 validation loss at iteration 159000 | lm loss value: 3.373435E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 159000, match long value: 0.0003474760993550453 | match short value: -0.015146401038671242 
----------------------------------------------------------------------------------------------------------
 iteration   159100/  200000 | consumed samples:      5091200 | elapsed time per iteration (ms): 270.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.269322E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.12 | backward-params-all-reduce: 62.39 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.53 | optimizer-unscale-and-check-inf: 16.21 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 99.01 | batch-generator: 14.94
 iteration   159200/  200000 | consumed samples:      5094400 | elapsed time per iteration (ms): 269.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.293704E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.19 | backward-params-all-reduce: 62.15 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.34 | optimizer-unscale-and-check-inf: 16.55 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 14.15 | optimizer: 100.85 | batch-generator: 8.62
 iteration   159300/  200000 | consumed samples:      5097600 | elapsed time per iteration (ms): 258.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.267501E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.28 | backward-params-all-reduce: 63.35 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.54 | optimizer-unscale-and-check-inf: 16.19 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 12.54 | optimizer: 99.21 | batch-generator: 8.57
 iteration   159400/  200000 | consumed samples:      5100800 | elapsed time per iteration (ms): 265.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.270092E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.20 | backward-params-all-reduce: 64.47 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.34 | optimizer-unscale-and-check-inf: 16.81 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 99.41 | batch-generator: 14.61
 iteration   159500/  200000 | consumed samples:      5104000 | elapsed time per iteration (ms): 259.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.217395E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.58 | backward-params-all-reduce: 64.60 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.88 | optimizer-unscale-and-check-inf: 16.86 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.96 | batch-generator: 8.46
--------------------------------------------------------------------------------------------------
 validation loss at iteration 159500 | lm loss value: 3.265621E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 159500, match long value: -0.001916252678470606 | match short value: -0.012637918381750492 
----------------------------------------------------------------------------------------------------------
 iteration   159600/  200000 | consumed samples:      5107200 | elapsed time per iteration (ms): 270.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.205521E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.63 | backward-params-all-reduce: 64.62 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.20 | optimizer-unscale-and-check-inf: 16.07 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 98.68 | batch-generator: 15.27
 iteration   159700/  200000 | consumed samples:      5110400 | elapsed time per iteration (ms): 260.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.217775E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.58 | backward-params-all-reduce: 64.99 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.56 | optimizer-unscale-and-check-inf: 16.72 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 12.55 | optimizer: 99.73 | batch-generator: 8.69
 iteration   159800/  200000 | consumed samples:      5113600 | elapsed time per iteration (ms): 259.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.206438E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.23 | backward-params-all-reduce: 65.43 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.76 | optimizer-unscale-and-check-inf: 16.56 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.27 | optimizer: 98.35 | batch-generator: 8.66
 iteration   159900/  200000 | consumed samples:      5116800 | elapsed time per iteration (ms): 267.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.222833E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.83 | backward-params-all-reduce: 64.79 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.54 | optimizer-unscale-and-check-inf: 15.56 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 14.31 | optimizer: 100.20 | batch-generator: 8.53
 iteration   160000/  200000 | consumed samples:      5120000 | elapsed time per iteration (ms): 266.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.225426E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.76 | backward-params-all-reduce: 63.89 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 14.34 | optimizer-unscale-and-check-inf: 15.11 | optimizer-clip-main-grad: 7.73 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 98.88 | batch-generator: 8.69
--------------------------------------------------------------------------------------------------
 validation loss at iteration 160000 | lm loss value: 3.109222E-05 | lm loss PPL: 1.000031E+00 | 
 at iteration 160000, match long value: -0.0057262625320474245 | match short value: -0.034270274717470246 
-----------------------------------------------------------------------------------------------------------
saving checkpoint at iteration  160000 to verify1
  successfully saved checkpoint at iteration  160000 to verify1
time (ms) | save-checkpoint: 22423.88
 iteration   160100/  200000 | consumed samples:      5123200 | elapsed time per iteration (ms): 500.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.230386E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.30 | backward-params-all-reduce: 64.62 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 14.12 | optimizer-unscale-and-check-inf: 16.15 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 99.55 | batch-generator: 19.69
 iteration   160200/  200000 | consumed samples:      5126400 | elapsed time per iteration (ms): 259.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.233156E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.72 | backward-params-all-reduce: 63.73 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.56 | optimizer-unscale-and-check-inf: 18.72 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 100.62 | batch-generator: 8.51
 iteration   160300/  200000 | consumed samples:      5129600 | elapsed time per iteration (ms): 258.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.240759E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.00 | backward-params-all-reduce: 63.74 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.76 | optimizer-unscale-and-check-inf: 18.04 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 100.15 | batch-generator: 8.51
 iteration   160400/  200000 | consumed samples:      5132800 | elapsed time per iteration (ms): 260.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.223496E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.31 | backward-params-all-reduce: 64.15 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.45 | optimizer-unscale-and-check-inf: 17.58 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.35 | batch-generator: 8.44
 iteration   160500/  200000 | consumed samples:      5136000 | elapsed time per iteration (ms): 264.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.244941E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.35 | backward-params-all-reduce: 64.34 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.55 | optimizer-unscale-and-check-inf: 17.62 | optimizer-clip-main-grad: 7.73 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.59 | batch-generator: 8.61
--------------------------------------------------------------------------------------------------
 validation loss at iteration 160500 | lm loss value: 3.280082E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 160500, match long value: -0.0013389888277156577 | match short value: -0.021190545792339872 
-----------------------------------------------------------------------------------------------------------
 iteration   160600/  200000 | consumed samples:      5139200 | elapsed time per iteration (ms): 275.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.230397E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.19 | backward-params-all-reduce: 64.96 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.33 | optimizer-unscale-and-check-inf: 16.56 | optimizer-clip-main-grad: 7.80 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.36 | batch-generator: 14.08
 iteration   160700/  200000 | consumed samples:      5142400 | elapsed time per iteration (ms): 273.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.250929E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.84 | backward-params-all-reduce: 65.10 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.19 | optimizer-unscale-and-check-inf: 16.46 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 14.34 | optimizer: 99.75 | batch-generator: 8.55
 iteration   160800/  200000 | consumed samples:      5145600 | elapsed time per iteration (ms): 260.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.233353E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.27 | backward-params-all-reduce: 63.72 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.33 | optimizer-unscale-and-check-inf: 16.35 | optimizer-clip-main-grad: 7.51 | optimizer-copy-main-to-model-params: 12.23 | optimizer: 97.59 | batch-generator: 8.51
 iteration   160900/  200000 | consumed samples:      5148800 | elapsed time per iteration (ms): 259.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.242297E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.41 | backward-params-all-reduce: 64.11 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.59 | optimizer-unscale-and-check-inf: 16.75 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.65 | batch-generator: 8.58
 iteration   161000/  200000 | consumed samples:      5152000 | elapsed time per iteration (ms): 261.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.255392E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.32 | backward-params-all-reduce: 63.34 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.50 | optimizer-unscale-and-check-inf: 16.35 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 98.19 | batch-generator: 8.49
--------------------------------------------------------------------------------------------------
 validation loss at iteration 161000 | lm loss value: 3.350492E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 161000, match long value: -0.0005523383216568497 | match short value: -0.006868881947934702 
-----------------------------------------------------------------------------------------------------------
 iteration   161100/  200000 | consumed samples:      5155200 | elapsed time per iteration (ms): 273.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.263076E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.40 | backward-params-all-reduce: 63.47 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.30 | optimizer-unscale-and-check-inf: 17.36 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 98.85 | batch-generator: 19.39
 iteration   161200/  200000 | consumed samples:      5158400 | elapsed time per iteration (ms): 262.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.257017E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.26 | backward-params-all-reduce: 63.62 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.18 | optimizer-unscale-and-check-inf: 17.46 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.00 | batch-generator: 8.76
 iteration   161300/  200000 | consumed samples:      5161600 | elapsed time per iteration (ms): 264.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.257862E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.15 | backward-params-all-reduce: 63.71 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.51 | optimizer-unscale-and-check-inf: 17.94 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.71 | batch-generator: 8.53
 iteration   161400/  200000 | consumed samples:      5164800 | elapsed time per iteration (ms): 275.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.260684E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.97 | backward-params-all-reduce: 64.20 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.35 | optimizer-unscale-and-check-inf: 16.84 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 14.38 | optimizer: 100.43 | batch-generator: 8.61
 iteration   161500/  200000 | consumed samples:      5168000 | elapsed time per iteration (ms): 259.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.241700E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.51 | backward-params-all-reduce: 63.93 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.21 | optimizer-unscale-and-check-inf: 16.64 | optimizer-clip-main-grad: 7.52 | optimizer-copy-main-to-model-params: 12.38 | optimizer: 97.39 | batch-generator: 8.59
--------------------------------------------------------------------------------------------------
 validation loss at iteration 161500 | lm loss value: 3.339214E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 161500, match long value: -0.002130838388097916 | match short value: -0.014053880144570851 
----------------------------------------------------------------------------------------------------------
 iteration   161600/  200000 | consumed samples:      5171200 | elapsed time per iteration (ms): 274.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.255969E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.15 | backward-params-all-reduce: 63.19 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.18 | optimizer-unscale-and-check-inf: 17.63 | optimizer-clip-main-grad: 7.53 | optimizer-copy-main-to-model-params: 12.37 | optimizer: 98.37 | batch-generator: 19.91
 iteration   161700/  200000 | consumed samples:      5174400 | elapsed time per iteration (ms): 258.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.263357E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.31 | backward-params-all-reduce: 63.26 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.40 | optimizer-unscale-and-check-inf: 16.44 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 98.21 | batch-generator: 8.74
 iteration   161800/  200000 | consumed samples:      5177600 | elapsed time per iteration (ms): 261.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.267116E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.25 | backward-params-all-reduce: 65.08 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.37 | optimizer-unscale-and-check-inf: 17.89 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.47 | batch-generator: 8.52
 iteration   161900/  200000 | consumed samples:      5180800 | elapsed time per iteration (ms): 260.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.260545E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.21 | backward-params-all-reduce: 64.57 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.41 | optimizer-unscale-and-check-inf: 16.09 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.55 | optimizer: 97.84 | batch-generator: 8.79
 iteration   162000/  200000 | consumed samples:      5184000 | elapsed time per iteration (ms): 261.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.270196E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.38 | backward-params-all-reduce: 63.41 | backward-embedding-all-reduce: 0.06 | optimizer-copy-to-main-grad: 12.32 | optimizer-unscale-and-check-inf: 16.68 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.25 | batch-generator: 8.66
--------------------------------------------------------------------------------------------------
 validation loss at iteration 162000 | lm loss value: 3.221029E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 162000, match long value: 0.0009934057420043858 | match short value: 0.0063196386250058195 
----------------------------------------------------------------------------------------------------------
 iteration   162100/  200000 | consumed samples:      5187200 | elapsed time per iteration (ms): 279.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.272460E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.45 | backward-params-all-reduce: 64.89 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.40 | optimizer-unscale-and-check-inf: 16.43 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 14.14 | optimizer: 99.68 | batch-generator: 16.17
 iteration   162200/  200000 | consumed samples:      5190400 | elapsed time per iteration (ms): 268.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.262572E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.59 | backward-params-all-reduce: 64.08 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.37 | optimizer-unscale-and-check-inf: 16.73 | optimizer-clip-main-grad: 7.80 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 98.60 | batch-generator: 8.71
 iteration   162300/  200000 | consumed samples:      5193600 | elapsed time per iteration (ms): 260.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.264824E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.64 | backward-params-all-reduce: 64.41 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.21 | optimizer-unscale-and-check-inf: 16.54 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.07 | batch-generator: 8.61
 iteration   162400/  200000 | consumed samples:      5196800 | elapsed time per iteration (ms): 258.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.280967E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.16 | backward-params-all-reduce: 63.51 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.18 | optimizer-unscale-and-check-inf: 17.09 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.54 | optimizer: 98.63 | batch-generator: 8.58
 iteration   162500/  200000 | consumed samples:      5200000 | elapsed time per iteration (ms): 259.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.269312E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.16 | backward-params-all-reduce: 63.74 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 11.93 | optimizer-unscale-and-check-inf: 16.85 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.04 | batch-generator: 8.43
--------------------------------------------------------------------------------------------------
 validation loss at iteration 162500 | lm loss value: 3.318136E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 162500, match long value: -0.0009422905365564312 | match short value: 0.0017144433939804428 
-----------------------------------------------------------------------------------------------------------
 iteration   162600/  200000 | consumed samples:      5203200 | elapsed time per iteration (ms): 282.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.200124E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.34 | backward-params-all-reduce: 66.37 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.17 | optimizer-unscale-and-check-inf: 19.06 | optimizer-clip-main-grad: 7.78 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 101.74 | batch-generator: 22.60
 iteration   162700/  200000 | consumed samples:      5206400 | elapsed time per iteration (ms): 264.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.183944E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.17 | backward-params-all-reduce: 64.34 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.19 | optimizer-unscale-and-check-inf: 19.08 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 100.59 | batch-generator: 8.51
 iteration   162800/  200000 | consumed samples:      5209600 | elapsed time per iteration (ms): 261.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.194130E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.38 | backward-params-all-reduce: 63.39 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.29 | optimizer-unscale-and-check-inf: 19.84 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 101.34 | batch-generator: 8.43
 iteration   162900/  200000 | consumed samples:      5212800 | elapsed time per iteration (ms): 274.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.188399E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.05 | backward-params-all-reduce: 64.36 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.45 | optimizer-unscale-and-check-inf: 17.68 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 14.29 | optimizer: 101.30 | batch-generator: 8.77
 iteration   163000/  200000 | consumed samples:      5216000 | elapsed time per iteration (ms): 264.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.174165E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.19 | backward-params-all-reduce: 65.40 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.29 | optimizer-unscale-and-check-inf: 19.60 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 101.24 | batch-generator: 8.67
--------------------------------------------------------------------------------------------------
 validation loss at iteration 163000 | lm loss value: 3.349687E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 163000, match long value: -0.002021548477318565 | match short value: -0.030690848699359098 
----------------------------------------------------------------------------------------------------------
 iteration   163100/  200000 | consumed samples:      5219200 | elapsed time per iteration (ms): 277.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.205831E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.59 | backward-params-all-reduce: 64.10 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.34 | optimizer-unscale-and-check-inf: 17.97 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.69 | batch-generator: 18.74
 iteration   163200/  200000 | consumed samples:      5222400 | elapsed time per iteration (ms): 263.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.223372E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.50 | backward-params-all-reduce: 64.81 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.49 | optimizer-unscale-and-check-inf: 18.25 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 100.05 | batch-generator: 8.66
 iteration   163300/  200000 | consumed samples:      5225600 | elapsed time per iteration (ms): 260.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.220923E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.84 | backward-params-all-reduce: 63.33 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.58 | optimizer-unscale-and-check-inf: 16.73 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.67 | batch-generator: 8.77
 iteration   163400/  200000 | consumed samples:      5228800 | elapsed time per iteration (ms): 261.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.201921E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.38 | backward-params-all-reduce: 63.89 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.73 | optimizer-unscale-and-check-inf: 17.56 | optimizer-clip-main-grad: 7.74 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.74 | batch-generator: 8.59
 iteration   163500/  200000 | consumed samples:      5232000 | elapsed time per iteration (ms): 261.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.193133E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.32 | backward-params-all-reduce: 63.25 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.06 | optimizer-unscale-and-check-inf: 18.62 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.94 | batch-generator: 8.58
--------------------------------------------------------------------------------------------------
 validation loss at iteration 163500 | lm loss value: 3.359047E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 163500, match long value: -0.002772938473308064 | match short value: -0.032839605221222486 
----------------------------------------------------------------------------------------------------------
 iteration   163600/  200000 | consumed samples:      5235200 | elapsed time per iteration (ms): 285.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.198814E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.84 | backward-params-all-reduce: 63.93 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.37 | optimizer-unscale-and-check-inf: 16.19 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 14.31 | optimizer: 99.67 | batch-generator: 15.84
 iteration   163700/  200000 | consumed samples:      5238400 | elapsed time per iteration (ms): 260.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.188917E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.69 | backward-params-all-reduce: 63.46 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.14 | optimizer-unscale-and-check-inf: 18.25 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.67 | batch-generator: 8.60
 iteration   163800/  200000 | consumed samples:      5241600 | elapsed time per iteration (ms): 261.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.208644E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.20 | backward-params-all-reduce: 63.79 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.32 | optimizer-unscale-and-check-inf: 18.62 | optimizer-clip-main-grad: 7.71 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 100.34 | batch-generator: 8.65
 iteration   163900/  200000 | consumed samples:      5244800 | elapsed time per iteration (ms): 259.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.201002E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.36 | backward-params-all-reduce: 64.21 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.60 | optimizer-unscale-and-check-inf: 16.43 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.55 | optimizer: 98.40 | batch-generator: 8.64
 iteration   164000/  200000 | consumed samples:      5248000 | elapsed time per iteration (ms): 258.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.213497E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.33 | backward-params-all-reduce: 62.71 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.17 | optimizer-unscale-and-check-inf: 17.20 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 98.64 | batch-generator: 8.58
--------------------------------------------------------------------------------------------------
 validation loss at iteration 164000 | lm loss value: 3.199944E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 164000, match long value: 0.0006821093741016879 | match short value: -0.009177706864441214 
----------------------------------------------------------------------------------------------------------
 iteration   164100/  200000 | consumed samples:      5251200 | elapsed time per iteration (ms): 271.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.197492E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.79 | backward-params-all-reduce: 63.90 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.93 | optimizer-unscale-and-check-inf: 15.49 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.54 | optimizer: 97.79 | batch-generator: 16.34
 iteration   164200/  200000 | consumed samples:      5254400 | elapsed time per iteration (ms): 261.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.200137E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.23 | backward-params-all-reduce: 64.13 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.61 | optimizer-unscale-and-check-inf: 16.76 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.62 | batch-generator: 8.50
 iteration   164300/  200000 | consumed samples:      5257600 | elapsed time per iteration (ms): 270.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.195833E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.50 | backward-params-all-reduce: 63.64 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.29 | optimizer-unscale-and-check-inf: 17.24 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 14.33 | optimizer: 100.68 | batch-generator: 8.63
 iteration   164400/  200000 | consumed samples:      5260800 | elapsed time per iteration (ms): 263.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.198243E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.37 | backward-params-all-reduce: 63.53 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.16 | optimizer-unscale-and-check-inf: 16.29 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 97.70 | batch-generator: 8.51
 iteration   164500/  200000 | consumed samples:      5264000 | elapsed time per iteration (ms): 260.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.198264E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.26 | backward-params-all-reduce: 64.14 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.35 | optimizer-unscale-and-check-inf: 16.68 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 98.27 | batch-generator: 8.56
--------------------------------------------------------------------------------------------------
 validation loss at iteration 164500 | lm loss value: 3.361976E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 164500, match long value: -0.0041534453653451694 | match short value: -0.02086270379619553 
----------------------------------------------------------------------------------------------------------
 iteration   164600/  200000 | consumed samples:      5267200 | elapsed time per iteration (ms): 276.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.190588E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.73 | backward-params-all-reduce: 64.41 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.65 | optimizer-unscale-and-check-inf: 16.43 | optimizer-clip-main-grad: 7.47 | optimizer-copy-main-to-model-params: 12.25 | optimizer: 96.98 | batch-generator: 19.59
 iteration   164700/  200000 | consumed samples:      5270400 | elapsed time per iteration (ms): 261.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.211427E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.35 | backward-params-all-reduce: 63.69 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.25 | optimizer-unscale-and-check-inf: 16.91 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.36 | batch-generator: 8.65
 iteration   164800/  200000 | consumed samples:      5273600 | elapsed time per iteration (ms): 258.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.194932E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.19 | backward-params-all-reduce: 63.09 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.41 | optimizer-unscale-and-check-inf: 16.39 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.10 | batch-generator: 8.68
 iteration   164900/  200000 | consumed samples:      5276800 | elapsed time per iteration (ms): 263.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.203455E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.26 | backward-params-all-reduce: 63.03 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.11 | optimizer-unscale-and-check-inf: 17.86 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.23 | batch-generator: 8.50
 iteration   165000/  200000 | consumed samples:      5280000 | elapsed time per iteration (ms): 261.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.202429E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.12 | backward-params-all-reduce: 63.46 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.06 | optimizer-unscale-and-check-inf: 16.62 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 97.96 | batch-generator: 8.52
--------------------------------------------------------------------------------------------------
 validation loss at iteration 165000 | lm loss value: 3.279055E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 165000, match long value: -0.002466508027254018 | match short value: -0.0092967522368297 
--------------------------------------------------------------------------------------------------------
 iteration   165100/  200000 | consumed samples:      5283200 | elapsed time per iteration (ms): 285.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.221389E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.06 | backward-params-all-reduce: 65.19 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.11 | optimizer-unscale-and-check-inf: 15.68 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 14.30 | optimizer: 98.94 | batch-generator: 14.84
 iteration   165200/  200000 | consumed samples:      5286400 | elapsed time per iteration (ms): 258.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.213672E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.58 | backward-params-all-reduce: 62.36 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.13 | optimizer-unscale-and-check-inf: 17.48 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 98.88 | batch-generator: 8.53
 iteration   165300/  200000 | consumed samples:      5289600 | elapsed time per iteration (ms): 258.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.201435E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.66 | backward-params-all-reduce: 62.31 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.18 | optimizer-unscale-and-check-inf: 18.55 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 100.09 | batch-generator: 8.63
 iteration   165400/  200000 | consumed samples:      5292800 | elapsed time per iteration (ms): 260.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.199574E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.59 | backward-params-all-reduce: 62.79 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.48 | optimizer-unscale-and-check-inf: 18.39 | optimizer-clip-main-grad: 7.73 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 100.31 | batch-generator: 8.76
 iteration   165500/  200000 | consumed samples:      5296000 | elapsed time per iteration (ms): 259.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.206182E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.89 | backward-params-all-reduce: 63.92 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.39 | optimizer-unscale-and-check-inf: 17.67 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.38 | batch-generator: 8.63
--------------------------------------------------------------------------------------------------
 validation loss at iteration 165500 | lm loss value: 3.362023E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 165500, match long value: -0.005228937370226833 | match short value: -0.020717462313541945 
----------------------------------------------------------------------------------------------------------
 iteration   165600/  200000 | consumed samples:      5299200 | elapsed time per iteration (ms): 272.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.228100E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.02 | backward-params-all-reduce: 64.68 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.25 | optimizer-unscale-and-check-inf: 18.18 | optimizer-clip-main-grad: 7.51 | optimizer-copy-main-to-model-params: 12.27 | optimizer: 98.40 | batch-generator: 15.19
 iteration   165700/  200000 | consumed samples:      5302400 | elapsed time per iteration (ms): 268.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.169757E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.07 | backward-params-all-reduce: 65.43 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 11.99 | optimizer-unscale-and-check-inf: 17.10 | optimizer-clip-main-grad: 7.72 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.49 | batch-generator: 14.70
 iteration   165800/  200000 | consumed samples:      5305600 | elapsed time per iteration (ms): 360.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.150124E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.94 | backward-params-all-reduce: 65.62 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.09 | optimizer-unscale-and-check-inf: 19.52 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 14.23 | optimizer: 102.64 | batch-generator: 94.35
 iteration   165900/  200000 | consumed samples:      5308800 | elapsed time per iteration (ms): 260.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.146524E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.33 | backward-params-all-reduce: 64.43 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.04 | optimizer-unscale-and-check-inf: 20.01 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 101.32 | batch-generator: 8.42
 iteration   166000/  200000 | consumed samples:      5312000 | elapsed time per iteration (ms): 260.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.154282E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.01 | backward-params-all-reduce: 65.75 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.22 | optimizer-unscale-and-check-inf: 19.21 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 100.67 | batch-generator: 8.47
--------------------------------------------------------------------------------------------------
 validation loss at iteration 166000 | lm loss value: 3.191807E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 166000, match long value: 0.0004551809183105807 | match short value: -0.0003216633493471003 
-----------------------------------------------------------------------------------------------------------
 iteration   166100/  200000 | consumed samples:      5315200 | elapsed time per iteration (ms): 278.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.138475E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.30 | backward-params-all-reduce: 65.31 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.33 | optimizer-unscale-and-check-inf: 19.04 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 100.69 | batch-generator: 20.54
 iteration   166200/  200000 | consumed samples:      5318400 | elapsed time per iteration (ms): 258.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.154160E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.43 | backward-params-all-reduce: 63.30 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.57 | optimizer-unscale-and-check-inf: 18.67 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 100.54 | batch-generator: 8.51
 iteration   166300/  200000 | consumed samples:      5321600 | elapsed time per iteration (ms): 261.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.156959E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.18 | backward-params-all-reduce: 64.59 | backward-embedding-all-reduce: 0.05 | optimizer-copy-to-main-grad: 14.43 | optimizer-unscale-and-check-inf: 15.85 | optimizer-clip-main-grad: 7.78 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 99.77 | batch-generator: 8.64
 iteration   166400/  200000 | consumed samples:      5324800 | elapsed time per iteration (ms): 259.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.137528E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.46 | backward-params-all-reduce: 63.21 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.09 | optimizer-unscale-and-check-inf: 18.06 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 100.39 | batch-generator: 8.47
 iteration   166500/  200000 | consumed samples:      5328000 | elapsed time per iteration (ms): 268.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.154230E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.19 | backward-params-all-reduce: 64.73 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.40 | optimizer-unscale-and-check-inf: 16.74 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 100.84 | batch-generator: 8.69
--------------------------------------------------------------------------------------------------
 validation loss at iteration 166500 | lm loss value: 3.374080E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 166500, match long value: -0.002709512921240163 | match short value: -0.012539052988549714 
----------------------------------------------------------------------------------------------------------
 iteration   166600/  200000 | consumed samples:      5331200 | elapsed time per iteration (ms): 272.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.152955E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.45 | backward-params-all-reduce: 64.68 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.38 | optimizer-unscale-and-check-inf: 18.06 | optimizer-clip-main-grad: 7.41 | optimizer-copy-main-to-model-params: 12.23 | optimizer: 98.28 | batch-generator: 15.07
 iteration   166700/  200000 | consumed samples:      5334400 | elapsed time per iteration (ms): 257.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.140411E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.07 | backward-params-all-reduce: 63.58 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.99 | optimizer-unscale-and-check-inf: 17.88 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.56 | optimizer: 100.16 | batch-generator: 8.47
 iteration   166800/  200000 | consumed samples:      5337600 | elapsed time per iteration (ms): 258.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.142313E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.94 | backward-params-all-reduce: 63.21 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.07 | optimizer-unscale-and-check-inf: 17.42 | optimizer-clip-main-grad: 7.71 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 99.91 | batch-generator: 8.61
 iteration   166900/  200000 | consumed samples:      5340800 | elapsed time per iteration (ms): 259.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.142419E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.46 | backward-params-all-reduce: 64.38 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.21 | optimizer-unscale-and-check-inf: 16.53 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.01 | batch-generator: 8.44
 iteration   167000/  200000 | consumed samples:      5344000 | elapsed time per iteration (ms): 258.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.139507E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.56 | backward-params-all-reduce: 63.65 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.80 | optimizer-unscale-and-check-inf: 17.85 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.93 | batch-generator: 8.45
--------------------------------------------------------------------------------------------------
 validation loss at iteration 167000 | lm loss value: 3.192861E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 167000, match long value: 0.00042449700760966694 | match short value: -0.002390070630822951 
-----------------------------------------------------------------------------------------------------------
 iteration   167100/  200000 | consumed samples:      5347200 | elapsed time per iteration (ms): 269.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.155076E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.52 | backward-params-all-reduce: 63.72 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.21 | optimizer-unscale-and-check-inf: 16.94 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.42 | batch-generator: 14.41
 iteration   167200/  200000 | consumed samples:      5350400 | elapsed time per iteration (ms): 258.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.159779E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.19 | backward-params-all-reduce: 64.28 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.91 | optimizer-unscale-and-check-inf: 16.80 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.55 | optimizer: 99.02 | batch-generator: 8.54
 iteration   167300/  200000 | consumed samples:      5353600 | elapsed time per iteration (ms): 271.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.143094E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.73 | backward-params-all-reduce: 64.77 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.04 | optimizer-unscale-and-check-inf: 15.94 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 14.29 | optimizer: 100.07 | batch-generator: 8.59
 iteration   167400/  200000 | consumed samples:      5356800 | elapsed time per iteration (ms): 258.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.143954E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.63 | backward-params-all-reduce: 64.15 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.26 | optimizer-unscale-and-check-inf: 16.89 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 99.48 | batch-generator: 8.47
 iteration   167500/  200000 | consumed samples:      5360000 | elapsed time per iteration (ms): 258.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.145209E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.27 | backward-params-all-reduce: 64.32 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.36 | optimizer-unscale-and-check-inf: 16.72 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 99.33 | batch-generator: 8.45
--------------------------------------------------------------------------------------------------
 validation loss at iteration 167500 | lm loss value: 3.340949E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 167500, match long value: -0.002354202351457647 | match short value: -0.01807347484531165 
---------------------------------------------------------------------------------------------------------
 iteration   167600/  200000 | consumed samples:      5363200 | elapsed time per iteration (ms): 273.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.142645E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.53 | backward-params-all-reduce: 62.89 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 14.03 | optimizer-unscale-and-check-inf: 16.35 | optimizer-clip-main-grad: 7.53 | optimizer-copy-main-to-model-params: 12.30 | optimizer: 98.41 | batch-generator: 20.41
 iteration   167700/  200000 | consumed samples:      5366400 | elapsed time per iteration (ms): 258.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.146130E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.00 | backward-params-all-reduce: 63.70 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.08 | optimizer-unscale-and-check-inf: 18.01 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 100.40 | batch-generator: 8.44
 iteration   167800/  200000 | consumed samples:      5369600 | elapsed time per iteration (ms): 257.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.146053E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.04 | backward-params-all-reduce: 63.85 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.70 | optimizer-unscale-and-check-inf: 17.72 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 99.59 | batch-generator: 8.36
 iteration   167900/  200000 | consumed samples:      5372800 | elapsed time per iteration (ms): 259.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.152947E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.28 | backward-params-all-reduce: 63.82 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.10 | optimizer-unscale-and-check-inf: 16.49 | optimizer-clip-main-grad: 7.54 | optimizer-copy-main-to-model-params: 12.42 | optimizer: 98.24 | batch-generator: 8.42
 iteration   168000/  200000 | consumed samples:      5376000 | elapsed time per iteration (ms): 270.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.149859E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.22 | backward-params-all-reduce: 63.70 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.35 | optimizer-unscale-and-check-inf: 15.30 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 14.33 | optimizer: 99.71 | batch-generator: 8.42
--------------------------------------------------------------------------------------------------
 validation loss at iteration 168000 | lm loss value: 3.250532E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 168000, match long value: -0.00022101850579164757 | match short value: -0.00528771465775186 
-----------------------------------------------------------------------------------------------------------
 iteration   168100/  200000 | consumed samples:      5379200 | elapsed time per iteration (ms): 267.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.156217E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.88 | backward-params-all-reduce: 63.78 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.16 | optimizer-unscale-and-check-inf: 16.86 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.26 | batch-generator: 13.90
 iteration   168200/  200000 | consumed samples:      5382400 | elapsed time per iteration (ms): 256.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.150264E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.93 | backward-params-all-reduce: 62.70 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.98 | optimizer-unscale-and-check-inf: 16.24 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.39 | batch-generator: 8.45
 iteration   168300/  200000 | consumed samples:      5385600 | elapsed time per iteration (ms): 257.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.168785E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.88 | backward-params-all-reduce: 64.18 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.13 | optimizer-unscale-and-check-inf: 16.41 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 98.69 | batch-generator: 8.40
 iteration   168400/  200000 | consumed samples:      5388800 | elapsed time per iteration (ms): 257.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.167680E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.25 | backward-params-all-reduce: 64.16 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.76 | optimizer-unscale-and-check-inf: 15.78 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 97.72 | batch-generator: 8.50
 iteration   168500/  200000 | consumed samples:      5392000 | elapsed time per iteration (ms): 256.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.154192E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.16 | backward-params-all-reduce: 63.33 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.68 | optimizer-unscale-and-check-inf: 16.20 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 98.06 | batch-generator: 8.49
--------------------------------------------------------------------------------------------------
 validation loss at iteration 168500 | lm loss value: 3.317087E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 168500, match long value: 0.0004790031221682828 | match short value: -0.028300523571510933 
----------------------------------------------------------------------------------------------------------
 iteration   168600/  200000 | consumed samples:      5395200 | elapsed time per iteration (ms): 269.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.149339E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.74 | backward-params-all-reduce: 63.37 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.78 | optimizer-unscale-and-check-inf: 15.28 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.36 | batch-generator: 14.15
 iteration   168700/  200000 | consumed samples:      5398400 | elapsed time per iteration (ms): 267.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.147033E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.90 | backward-params-all-reduce: 64.28 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.32 | optimizer-unscale-and-check-inf: 15.81 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 14.26 | optimizer: 100.09 | batch-generator: 8.41
 iteration   168800/  200000 | consumed samples:      5401600 | elapsed time per iteration (ms): 267.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.136708E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.22 | backward-params-all-reduce: 64.16 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.79 | optimizer-unscale-and-check-inf: 17.31 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 99.31 | batch-generator: 14.08
 iteration   168900/  200000 | consumed samples:      5404800 | elapsed time per iteration (ms): 260.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.115787E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.27 | backward-params-all-reduce: 65.07 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.89 | optimizer-unscale-and-check-inf: 16.03 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.12 | batch-generator: 8.42
 iteration   169000/  200000 | consumed samples:      5408000 | elapsed time per iteration (ms): 264.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.109129E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.67 | backward-params-all-reduce: 65.25 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.87 | optimizer-unscale-and-check-inf: 18.54 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 100.79 | batch-generator: 8.55
--------------------------------------------------------------------------------------------------
 validation loss at iteration 169000 | lm loss value: 3.361731E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 169000, match long value: -0.0025763934322047587 | match short value: -0.021517068573309133 
-----------------------------------------------------------------------------------------------------------
 iteration   169100/  200000 | consumed samples:      5411200 | elapsed time per iteration (ms): 277.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.102906E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.91 | backward-params-all-reduce: 64.11 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.77 | optimizer-unscale-and-check-inf: 16.60 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 98.61 | batch-generator: 21.25
 iteration   169200/  200000 | consumed samples:      5414400 | elapsed time per iteration (ms): 261.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.120349E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.51 | backward-params-all-reduce: 63.80 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.13 | optimizer-unscale-and-check-inf: 16.69 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.07 | batch-generator: 8.44
 iteration   169300/  200000 | consumed samples:      5417600 | elapsed time per iteration (ms): 262.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.124788E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.12 | backward-params-all-reduce: 64.68 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.36 | optimizer-unscale-and-check-inf: 18.36 | optimizer-clip-main-grad: 7.71 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 100.09 | batch-generator: 8.56
 iteration   169400/  200000 | consumed samples:      5420800 | elapsed time per iteration (ms): 264.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.106895E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 54.90 | backward-params-all-reduce: 63.31 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.23 | optimizer-unscale-and-check-inf: 17.06 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.50 | batch-generator: 8.45
 iteration   169500/  200000 | consumed samples:      5424000 | elapsed time per iteration (ms): 269.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.113213E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.12 | backward-params-all-reduce: 62.91 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.50 | optimizer-unscale-and-check-inf: 17.54 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 14.28 | optimizer: 101.15 | batch-generator: 8.48
--------------------------------------------------------------------------------------------------
 validation loss at iteration 169500 | lm loss value: 3.322083E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 169500, match long value: -0.00224873427459942 | match short value: -0.0004527102100868556 
----------------------------------------------------------------------------------------------------------
 iteration   169600/  200000 | consumed samples:      5427200 | elapsed time per iteration (ms): 274.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.104585E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.26 | backward-params-all-reduce: 66.48 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.38 | optimizer-unscale-and-check-inf: 17.17 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 98.86 | batch-generator: 16.03
 iteration   169700/  200000 | consumed samples:      5430400 | elapsed time per iteration (ms): 260.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.118935E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.86 | backward-params-all-reduce: 65.32 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.41 | optimizer-unscale-and-check-inf: 17.18 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 98.79 | batch-generator: 8.47
 iteration   169800/  200000 | consumed samples:      5433600 | elapsed time per iteration (ms): 258.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.127154E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.97 | backward-params-all-reduce: 64.67 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.25 | optimizer-unscale-and-check-inf: 16.20 | optimizer-clip-main-grad: 7.52 | optimizer-copy-main-to-model-params: 12.44 | optimizer: 97.54 | batch-generator: 8.42
 iteration   169900/  200000 | consumed samples:      5436800 | elapsed time per iteration (ms): 258.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.116936E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.63 | backward-params-all-reduce: 63.29 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.86 | optimizer-unscale-and-check-inf: 15.92 | optimizer-clip-main-grad: 7.53 | optimizer-copy-main-to-model-params: 12.25 | optimizer: 96.74 | batch-generator: 8.65
 iteration   170000/  200000 | consumed samples:      5440000 | elapsed time per iteration (ms): 258.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.122816E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.35 | backward-params-all-reduce: 63.68 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.82 | optimizer-unscale-and-check-inf: 15.99 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 97.98 | batch-generator: 8.45
--------------------------------------------------------------------------------------------------
 validation loss at iteration 170000 | lm loss value: 3.216259E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 170000, match long value: -0.0038889425151792853 | match short value: -0.011686376681422205 
-----------------------------------------------------------------------------------------------------------
 iteration   170100/  200000 | consumed samples:      5443200 | elapsed time per iteration (ms): 272.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.120557E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.35 | backward-params-all-reduce: 63.73 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.98 | optimizer-unscale-and-check-inf: 15.71 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 97.96 | batch-generator: 13.78
 iteration   170200/  200000 | consumed samples:      5446400 | elapsed time per iteration (ms): 272.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.122624E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.91 | backward-params-all-reduce: 63.38 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.38 | optimizer-unscale-and-check-inf: 16.03 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 14.27 | optimizer: 99.40 | batch-generator: 8.50
 iteration   170300/  200000 | consumed samples:      5449600 | elapsed time per iteration (ms): 258.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.132311E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.17 | backward-params-all-reduce: 63.11 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.08 | optimizer-unscale-and-check-inf: 16.26 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 97.58 | batch-generator: 8.45
 iteration   170400/  200000 | consumed samples:      5452800 | elapsed time per iteration (ms): 258.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.115992E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.14 | backward-params-all-reduce: 62.99 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.58 | optimizer-unscale-and-check-inf: 16.76 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.69 | batch-generator: 8.51
 iteration   170500/  200000 | consumed samples:      5456000 | elapsed time per iteration (ms): 258.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.116390E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.25 | backward-params-all-reduce: 64.97 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.53 | optimizer-unscale-and-check-inf: 16.65 | optimizer-clip-main-grad: 7.47 | optimizer-copy-main-to-model-params: 12.25 | optimizer: 97.07 | batch-generator: 8.60
--------------------------------------------------------------------------------------------------
 validation loss at iteration 170500 | lm loss value: 3.458656E-05 | lm loss PPL: 1.000035E+00 | 
 at iteration 170500, match long value: -0.0026786928567877196 | match short value: -0.010894351432005248 
-----------------------------------------------------------------------------------------------------------
 iteration   170600/  200000 | consumed samples:      5459200 | elapsed time per iteration (ms): 269.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.132204E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.49 | backward-params-all-reduce: 64.73 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.03 | optimizer-unscale-and-check-inf: 16.77 | optimizer-clip-main-grad: 7.71 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 98.16 | batch-generator: 14.88
 iteration   170700/  200000 | consumed samples:      5462400 | elapsed time per iteration (ms): 259.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.126542E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.70 | backward-params-all-reduce: 64.79 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.96 | optimizer-unscale-and-check-inf: 16.02 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.28 | batch-generator: 8.69
 iteration   170800/  200000 | consumed samples:      5465600 | elapsed time per iteration (ms): 261.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.126570E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.39 | backward-params-all-reduce: 63.34 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.27 | optimizer-unscale-and-check-inf: 16.88 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.45 | batch-generator: 8.76
 iteration   170900/  200000 | consumed samples:      5468800 | elapsed time per iteration (ms): 269.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.120199E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.51 | backward-params-all-reduce: 64.04 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.00 | optimizer-unscale-and-check-inf: 16.64 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 14.19 | optimizer: 99.63 | batch-generator: 8.53
 iteration   171000/  200000 | consumed samples:      5472000 | elapsed time per iteration (ms): 266.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.117489E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.62 | backward-params-all-reduce: 64.66 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.31 | optimizer-unscale-and-check-inf: 16.64 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 98.25 | batch-generator: 8.75
--------------------------------------------------------------------------------------------------
 validation loss at iteration 171000 | lm loss value: 3.350898E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 171000, match long value: -0.00015355641270584918 | match short value: -0.021377922673951194 
------------------------------------------------------------------------------------------------------------
 iteration   171100/  200000 | consumed samples:      5475200 | elapsed time per iteration (ms): 273.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.121830E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.27 | backward-params-all-reduce: 62.89 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.28 | optimizer-unscale-and-check-inf: 16.32 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 97.93 | batch-generator: 18.87
 iteration   171200/  200000 | consumed samples:      5478400 | elapsed time per iteration (ms): 258.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.126299E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.08 | backward-params-all-reduce: 63.68 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.61 | optimizer-unscale-and-check-inf: 16.43 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.31 | batch-generator: 8.48
 iteration   171300/  200000 | consumed samples:      5481600 | elapsed time per iteration (ms): 259.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.123249E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.15 | backward-params-all-reduce: 64.12 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.09 | optimizer-unscale-and-check-inf: 15.88 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 97.25 | batch-generator: 8.60
 iteration   171400/  200000 | consumed samples:      5484800 | elapsed time per iteration (ms): 259.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.116336E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.08 | backward-params-all-reduce: 63.78 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.76 | optimizer-unscale-and-check-inf: 15.12 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 97.20 | batch-generator: 8.79
 iteration   171500/  200000 | consumed samples:      5488000 | elapsed time per iteration (ms): 261.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.123721E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.55 | backward-params-all-reduce: 64.64 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.98 | optimizer-unscale-and-check-inf: 14.74 | optimizer-clip-main-grad: 7.73 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 97.16 | batch-generator: 8.93
--------------------------------------------------------------------------------------------------
 validation loss at iteration 171500 | lm loss value: 3.373755E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 171500, match long value: 0.0007295672403840785 | match short value: -0.008039813325061814 
----------------------------------------------------------------------------------------------------------
 iteration   171600/  200000 | consumed samples:      5491200 | elapsed time per iteration (ms): 274.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.123049E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.01 | backward-params-all-reduce: 64.43 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.48 | optimizer-unscale-and-check-inf: 16.10 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 97.87 | batch-generator: 15.67
 iteration   171700/  200000 | consumed samples:      5494400 | elapsed time per iteration (ms): 271.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.126931E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.17 | backward-params-all-reduce: 64.40 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.62 | optimizer-unscale-and-check-inf: 14.80 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 14.23 | optimizer: 98.49 | batch-generator: 8.77
 iteration   171800/  200000 | consumed samples:      5497600 | elapsed time per iteration (ms): 257.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.140336E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.34 | backward-params-all-reduce: 62.64 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.34 | optimizer-unscale-and-check-inf: 16.22 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 97.92 | batch-generator: 8.70
 iteration   171900/  200000 | consumed samples:      5500800 | elapsed time per iteration (ms): 265.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.107727E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.32 | backward-params-all-reduce: 63.45 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.23 | optimizer-unscale-and-check-inf: 16.43 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 97.93 | batch-generator: 14.73
 iteration   172000/  200000 | consumed samples:      5504000 | elapsed time per iteration (ms): 260.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.101625E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.33 | backward-params-all-reduce: 63.88 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.36 | optimizer-unscale-and-check-inf: 18.30 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 99.88 | batch-generator: 8.67
--------------------------------------------------------------------------------------------------
 validation loss at iteration 172000 | lm loss value: 3.301674E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 172000, match long value: 0.0014419701972040908 | match short value: -0.019576411511948992 
----------------------------------------------------------------------------------------------------------
 iteration   172100/  200000 | consumed samples:      5507200 | elapsed time per iteration (ms): 274.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.101327E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.16 | backward-params-all-reduce: 64.95 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.64 | optimizer-unscale-and-check-inf: 16.79 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.65 | batch-generator: 16.36
 iteration   172200/  200000 | consumed samples:      5510400 | elapsed time per iteration (ms): 262.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.096947E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.07 | backward-params-all-reduce: 63.91 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.41 | optimizer-unscale-and-check-inf: 17.53 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.17 | batch-generator: 8.61
 iteration   172300/  200000 | consumed samples:      5513600 | elapsed time per iteration (ms): 261.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.101330E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.43 | backward-params-all-reduce: 64.18 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.51 | optimizer-unscale-and-check-inf: 17.91 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.68 | batch-generator: 8.64
 iteration   172400/  200000 | consumed samples:      5516800 | elapsed time per iteration (ms): 275.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.101972E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.79 | backward-params-all-reduce: 63.65 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.51 | optimizer-unscale-and-check-inf: 16.02 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 14.33 | optimizer: 100.73 | batch-generator: 8.77
 iteration   172500/  200000 | consumed samples:      5520000 | elapsed time per iteration (ms): 259.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.112982E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.04 | backward-params-all-reduce: 63.24 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.52 | optimizer-unscale-and-check-inf: 16.69 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 98.44 | batch-generator: 8.52
--------------------------------------------------------------------------------------------------
 validation loss at iteration 172500 | lm loss value: 3.312820E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 172500, match long value: -0.0025170502730212955 | match short value: -0.0029934609919223414 
------------------------------------------------------------------------------------------------------------
 iteration   172600/  200000 | consumed samples:      5523200 | elapsed time per iteration (ms): 276.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.110229E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.34 | backward-params-all-reduce: 63.57 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.36 | optimizer-unscale-and-check-inf: 17.34 | optimizer-clip-main-grad: 7.50 | optimizer-copy-main-to-model-params: 12.34 | optimizer: 98.21 | batch-generator: 20.60
 iteration   172700/  200000 | consumed samples:      5526400 | elapsed time per iteration (ms): 259.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.100163E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.39 | backward-params-all-reduce: 63.67 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.46 | optimizer-unscale-and-check-inf: 17.63 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 99.49 | batch-generator: 8.65
 iteration   172800/  200000 | consumed samples:      5529600 | elapsed time per iteration (ms): 260.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.106587E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.08 | backward-params-all-reduce: 63.97 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.46 | optimizer-unscale-and-check-inf: 16.00 | optimizer-clip-main-grad: 7.88 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 99.06 | batch-generator: 8.80
 iteration   172900/  200000 | consumed samples:      5532800 | elapsed time per iteration (ms): 259.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.096238E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.20 | backward-params-all-reduce: 63.37 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.14 | optimizer-unscale-and-check-inf: 16.71 | optimizer-clip-main-grad: 7.73 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.26 | batch-generator: 8.67
 iteration   173000/  200000 | consumed samples:      5536000 | elapsed time per iteration (ms): 261.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.098087E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.66 | backward-params-all-reduce: 63.64 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.44 | optimizer-unscale-and-check-inf: 16.81 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.56 | optimizer: 98.63 | batch-generator: 8.69
--------------------------------------------------------------------------------------------------
 validation loss at iteration 173000 | lm loss value: 3.455386E-05 | lm loss PPL: 1.000035E+00 | 
 at iteration 173000, match long value: -0.0019120929504433826 | match short value: -0.026474639953379883 
-----------------------------------------------------------------------------------------------------------
 iteration   173100/  200000 | consumed samples:      5539200 | elapsed time per iteration (ms): 282.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.100422E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.96 | backward-params-all-reduce: 63.98 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.54 | optimizer-unscale-and-check-inf: 16.28 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 14.30 | optimizer: 99.92 | batch-generator: 14.92
 iteration   173200/  200000 | consumed samples:      5542400 | elapsed time per iteration (ms): 264.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.097255E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.38 | backward-params-all-reduce: 65.59 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.84 | optimizer-unscale-and-check-inf: 16.52 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 98.72 | batch-generator: 8.81
 iteration   173300/  200000 | consumed samples:      5545600 | elapsed time per iteration (ms): 258.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.099247E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.27 | backward-params-all-reduce: 63.06 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.50 | optimizer-unscale-and-check-inf: 16.48 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 12.55 | optimizer: 98.36 | batch-generator: 8.70
 iteration   173400/  200000 | consumed samples:      5548800 | elapsed time per iteration (ms): 258.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.101497E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.22 | backward-params-all-reduce: 62.62 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.09 | optimizer-unscale-and-check-inf: 17.87 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 99.16 | batch-generator: 8.47
 iteration   173500/  200000 | consumed samples:      5552000 | elapsed time per iteration (ms): 259.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.113429E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.33 | backward-params-all-reduce: 63.66 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.26 | optimizer-unscale-and-check-inf: 16.64 | optimizer-clip-main-grad: 7.73 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.29 | batch-generator: 8.62
--------------------------------------------------------------------------------------------------
 validation loss at iteration 173500 | lm loss value: 3.180468E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 173500, match long value: -0.0015862274316281657 | match short value: -0.01347106820027157 
----------------------------------------------------------------------------------------------------------
 iteration   173600/  200000 | consumed samples:      5555200 | elapsed time per iteration (ms): 272.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.106395E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.33 | backward-params-all-reduce: 63.70 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 12.30 | optimizer-unscale-and-check-inf: 16.64 | optimizer-clip-main-grad: 7.53 | optimizer-copy-main-to-model-params: 12.26 | optimizer: 96.92 | batch-generator: 14.42
 iteration   173700/  200000 | consumed samples:      5558400 | elapsed time per iteration (ms): 262.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.105275E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.79 | backward-params-all-reduce: 63.92 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.52 | optimizer-unscale-and-check-inf: 16.86 | optimizer-clip-main-grad: 7.74 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.81 | batch-generator: 8.68
 iteration   173800/  200000 | consumed samples:      5561600 | elapsed time per iteration (ms): 258.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.105932E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   3 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.38 | backward-params-all-reduce: 62.07 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.43 | optimizer-unscale-and-check-inf: 18.74 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.12 | optimizer: 98.58 | batch-generator: 8.78
 iteration   173900/  200000 | consumed samples:      5564800 | elapsed time per iteration (ms): 273.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.104985E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 55.08 | backward-params-all-reduce: 62.37 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.86 | optimizer-unscale-and-check-inf: 18.50 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 14.30 | optimizer: 102.54 | batch-generator: 8.75
 iteration   174000/  200000 | consumed samples:      5568000 | elapsed time per iteration (ms): 259.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.110082E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.50 | backward-params-all-reduce: 63.06 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.35 | optimizer-unscale-and-check-inf: 18.48 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 100.18 | batch-generator: 8.65
--------------------------------------------------------------------------------------------------
 validation loss at iteration 174000 | lm loss value: 3.359631E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 174000, match long value: -0.002225802695134517 | match short value: -0.0007522552125074439 
-----------------------------------------------------------------------------------------------------------
 iteration   174100/  200000 | consumed samples:      5571200 | elapsed time per iteration (ms): 286.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.119526E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.55 | backward-params-all-reduce: 63.63 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.18 | optimizer-unscale-and-check-inf: 18.91 | optimizer-clip-main-grad: 7.71 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 100.49 | batch-generator: 21.24
 iteration   174200/  200000 | consumed samples:      5574400 | elapsed time per iteration (ms): 259.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.115006E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.13 | backward-params-all-reduce: 63.16 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.80 | optimizer-unscale-and-check-inf: 17.38 | optimizer-clip-main-grad: 7.75 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 99.58 | batch-generator: 8.79
 iteration   174300/  200000 | consumed samples:      5577600 | elapsed time per iteration (ms): 261.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.106532E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.08 | backward-params-all-reduce: 64.14 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.46 | optimizer-unscale-and-check-inf: 19.34 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 101.18 | batch-generator: 8.69
 iteration   174400/  200000 | consumed samples:      5580800 | elapsed time per iteration (ms): 261.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.106603E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.36 | backward-params-all-reduce: 64.06 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.69 | optimizer-unscale-and-check-inf: 17.94 | optimizer-clip-main-grad: 7.80 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 100.12 | batch-generator: 8.62
 iteration   174500/  200000 | consumed samples:      5584000 | elapsed time per iteration (ms): 262.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.113052E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.16 | backward-params-all-reduce: 63.39 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.17 | optimizer-unscale-and-check-inf: 20.61 | optimizer-clip-main-grad: 7.70 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 102.11 | batch-generator: 8.57
--------------------------------------------------------------------------------------------------
 validation loss at iteration 174500 | lm loss value: 3.250664E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 174500, match long value: -0.00016723122245952782 | match short value: -0.018203445349996614 
------------------------------------------------------------------------------------------------------------
 iteration   174600/  200000 | consumed samples:      5587200 | elapsed time per iteration (ms): 376.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.119233E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.08 | backward-params-all-reduce: 64.85 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.80 | optimizer-unscale-and-check-inf: 18.82 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 14.22 | optimizer: 102.66 | batch-generator: 104.75
 iteration   174700/  200000 | consumed samples:      5590400 | elapsed time per iteration (ms): 259.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.097145E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.02 | backward-params-all-reduce: 64.04 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.70 | optimizer-unscale-and-check-inf: 17.18 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.13 | batch-generator: 8.55
 iteration   174800/  200000 | consumed samples:      5593600 | elapsed time per iteration (ms): 259.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.105479E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.03 | backward-params-all-reduce: 63.79 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.92 | optimizer-unscale-and-check-inf: 18.29 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 100.48 | batch-generator: 8.52
 iteration   174900/  200000 | consumed samples:      5596800 | elapsed time per iteration (ms): 259.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.098902E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.25 | backward-params-all-reduce: 64.02 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 13.59 | optimizer-unscale-and-check-inf: 16.68 | optimizer-clip-main-grad: 7.74 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 99.71 | batch-generator: 8.68
 iteration   175000/  200000 | consumed samples:      5600000 | elapsed time per iteration (ms): 258.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.094129E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.86 | backward-params-all-reduce: 63.44 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.20 | optimizer-unscale-and-check-inf: 16.12 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 98.66 | batch-generator: 8.53
--------------------------------------------------------------------------------------------------
 validation loss at iteration 175000 | lm loss value: 3.437868E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 175000, match long value: 0.00033518343706047133 | match short value: -0.008734062535066511 
-----------------------------------------------------------------------------------------------------------
 iteration   175100/  200000 | consumed samples:      5603200 | elapsed time per iteration (ms): 278.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.088611E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.05 | backward-params-all-reduce: 65.98 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.92 | optimizer-unscale-and-check-inf: 17.85 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 100.00 | batch-generator: 21.33
 iteration   175200/  200000 | consumed samples:      5606400 | elapsed time per iteration (ms): 262.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.085587E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.99 | backward-params-all-reduce: 65.46 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.48 | optimizer-unscale-and-check-inf: 17.82 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.56 | batch-generator: 8.42
 iteration   175300/  200000 | consumed samples:      5609600 | elapsed time per iteration (ms): 268.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.087277E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.63 | backward-params-all-reduce: 64.00 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.13 | optimizer-unscale-and-check-inf: 16.76 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 14.24 | optimizer: 99.89 | batch-generator: 8.42
 iteration   175400/  200000 | consumed samples:      5612800 | elapsed time per iteration (ms): 261.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.088968E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.42 | backward-params-all-reduce: 63.98 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.08 | optimizer-unscale-and-check-inf: 16.37 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.66 | batch-generator: 8.45
 iteration   175500/  200000 | consumed samples:      5616000 | elapsed time per iteration (ms): 257.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.094894E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.99 | backward-params-all-reduce: 63.53 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.84 | optimizer-unscale-and-check-inf: 17.86 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.92 | batch-generator: 8.37
--------------------------------------------------------------------------------------------------
 validation loss at iteration 175500 | lm loss value: 3.243732E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 175500, match long value: 1.4742172978558847e-05 | match short value: -0.02205948586418585 
----------------------------------------------------------------------------------------------------------
 iteration   175600/  200000 | consumed samples:      5619200 | elapsed time per iteration (ms): 275.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.086808E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.50 | backward-params-all-reduce: 64.55 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.29 | optimizer-unscale-and-check-inf: 17.09 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.62 | batch-generator: 20.10
 iteration   175700/  200000 | consumed samples:      5622400 | elapsed time per iteration (ms): 259.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.076476E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.13 | backward-params-all-reduce: 65.47 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.74 | optimizer-unscale-and-check-inf: 18.10 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 100.08 | batch-generator: 8.39
 iteration   175800/  200000 | consumed samples:      5625600 | elapsed time per iteration (ms): 257.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.099069E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.11 | backward-params-all-reduce: 63.35 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.00 | optimizer-unscale-and-check-inf: 17.34 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.71 | batch-generator: 8.51
 iteration   175900/  200000 | consumed samples:      5628800 | elapsed time per iteration (ms): 258.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.097855E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.54 | backward-params-all-reduce: 62.17 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.02 | optimizer-unscale-and-check-inf: 17.36 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.64 | batch-generator: 8.41
 iteration   176000/  200000 | consumed samples:      5632000 | elapsed time per iteration (ms): 263.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.091785E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.35 | backward-params-all-reduce: 63.72 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.46 | optimizer-unscale-and-check-inf: 17.40 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 100.11 | batch-generator: 8.42
--------------------------------------------------------------------------------------------------
 validation loss at iteration 176000 | lm loss value: 3.362793E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 176000, match long value: -0.0014354324578996884 | match short value: -0.009177998322484972 
-----------------------------------------------------------------------------------------------------------
 iteration   176100/  200000 | consumed samples:      5635200 | elapsed time per iteration (ms): 276.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.085098E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.98 | backward-params-all-reduce: 64.24 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.26 | optimizer-unscale-and-check-inf: 16.29 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 100.16 | batch-generator: 14.07
 iteration   176200/  200000 | consumed samples:      5638400 | elapsed time per iteration (ms): 259.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.092567E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.22 | backward-params-all-reduce: 66.27 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.89 | optimizer-unscale-and-check-inf: 17.36 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.45 | batch-generator: 8.38
 iteration   176300/  200000 | consumed samples:      5641600 | elapsed time per iteration (ms): 258.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.090799E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.63 | backward-params-all-reduce: 63.75 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.37 | optimizer-unscale-and-check-inf: 16.49 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.06 | batch-generator: 8.43
 iteration   176400/  200000 | consumed samples:      5644800 | elapsed time per iteration (ms): 258.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.078686E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.13 | backward-params-all-reduce: 64.51 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.18 | optimizer-unscale-and-check-inf: 16.94 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.32 | batch-generator: 8.38
 iteration   176500/  200000 | consumed samples:      5648000 | elapsed time per iteration (ms): 260.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.083217E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.51 | backward-params-all-reduce: 63.26 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.83 | optimizer-unscale-and-check-inf: 16.70 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.54 | optimizer: 99.83 | batch-generator: 8.40
--------------------------------------------------------------------------------------------------
 validation loss at iteration 176500 | lm loss value: 3.240815E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 176500, match long value: 2.0456868998630177e-05 | match short value: -8.66947571664501e-05 
-----------------------------------------------------------------------------------------------------------
 iteration   176600/  200000 | consumed samples:      5651200 | elapsed time per iteration (ms): 270.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.089440E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.31 | backward-params-all-reduce: 64.59 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 14.03 | optimizer-unscale-and-check-inf: 15.61 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.92 | batch-generator: 14.19
 iteration   176700/  200000 | consumed samples:      5654400 | elapsed time per iteration (ms): 262.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.091283E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.12 | backward-params-all-reduce: 64.71 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.57 | optimizer-unscale-and-check-inf: 18.13 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 100.00 | batch-generator: 8.43
 iteration   176800/  200000 | consumed samples:      5657600 | elapsed time per iteration (ms): 273.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.092441E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.91 | backward-params-all-reduce: 64.26 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.24 | optimizer-unscale-and-check-inf: 17.27 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 14.32 | optimizer: 100.70 | batch-generator: 8.53
 iteration   176900/  200000 | consumed samples:      5660800 | elapsed time per iteration (ms): 259.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.094401E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.07 | backward-params-all-reduce: 63.96 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.33 | optimizer-unscale-and-check-inf: 17.19 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 98.76 | batch-generator: 8.46
 iteration   177000/  200000 | consumed samples:      5664000 | elapsed time per iteration (ms): 258.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.079465E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.11 | backward-params-all-reduce: 63.01 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.68 | optimizer-unscale-and-check-inf: 17.23 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.44 | optimizer: 99.05 | batch-generator: 8.34
--------------------------------------------------------------------------------------------------
 validation loss at iteration 177000 | lm loss value: 3.350323E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 177000, match long value: 0.0014379401523094445 | match short value: -0.011541575300760117 
----------------------------------------------------------------------------------------------------------
 iteration   177100/  200000 | consumed samples:      5667200 | elapsed time per iteration (ms): 276.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.080363E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.73 | backward-params-all-reduce: 63.82 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.27 | optimizer-unscale-and-check-inf: 17.44 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.45 | optimizer: 98.88 | batch-generator: 19.32
 iteration   177200/  200000 | consumed samples:      5670400 | elapsed time per iteration (ms): 260.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.090878E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.17 | backward-params-all-reduce: 63.72 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.12 | optimizer-unscale-and-check-inf: 17.42 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.87 | batch-generator: 8.48
 iteration   177300/  200000 | consumed samples:      5673600 | elapsed time per iteration (ms): 260.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.086819E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.85 | backward-params-all-reduce: 63.24 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.02 | optimizer-unscale-and-check-inf: 16.77 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.14 | batch-generator: 8.44
 iteration   177400/  200000 | consumed samples:      5676800 | elapsed time per iteration (ms): 259.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.089930E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.14 | backward-params-all-reduce: 62.80 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.49 | optimizer-unscale-and-check-inf: 16.55 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.33 | batch-generator: 8.47
 iteration   177500/  200000 | consumed samples:      5680000 | elapsed time per iteration (ms): 268.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.095410E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.56 | backward-params-all-reduce: 63.71 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.24 | optimizer-unscale-and-check-inf: 16.10 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 14.36 | optimizer: 99.48 | batch-generator: 8.62
--------------------------------------------------------------------------------------------------
 validation loss at iteration 177500 | lm loss value: 3.439225E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 177500, match long value: -0.005209768808743183 | match short value: -0.014639661405516978 
----------------------------------------------------------------------------------------------------------
 iteration   177600/  200000 | consumed samples:      5683200 | elapsed time per iteration (ms): 271.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.095411E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.67 | backward-params-all-reduce: 64.36 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.39 | optimizer-unscale-and-check-inf: 15.83 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 97.44 | batch-generator: 14.97
 iteration   177700/  200000 | consumed samples:      5686400 | elapsed time per iteration (ms): 260.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.080012E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.48 | backward-params-all-reduce: 65.13 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.28 | optimizer-unscale-and-check-inf: 15.44 | optimizer-clip-main-grad: 7.42 | optimizer-copy-main-to-model-params: 12.34 | optimizer: 96.12 | batch-generator: 8.52
 iteration   177800/  200000 | consumed samples:      5689600 | elapsed time per iteration (ms): 259.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.092497E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.42 | backward-params-all-reduce: 62.80 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.32 | optimizer-unscale-and-check-inf: 16.20 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 97.78 | batch-generator: 8.63
 iteration   177900/  200000 | consumed samples:      5692800 | elapsed time per iteration (ms): 259.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.100547E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.03 | backward-params-all-reduce: 63.09 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.24 | optimizer-unscale-and-check-inf: 16.93 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.58 | optimizer: 98.49 | batch-generator: 8.43
 iteration   178000/  200000 | consumed samples:      5696000 | elapsed time per iteration (ms): 260.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.105654E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.43 | backward-params-all-reduce: 64.92 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.29 | optimizer-unscale-and-check-inf: 16.51 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 98.03 | batch-generator: 8.48
--------------------------------------------------------------------------------------------------
 validation loss at iteration 178000 | lm loss value: 3.323961E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 178000, match long value: -0.003385946073238589 | match short value: -0.007204749410762076 
----------------------------------------------------------------------------------------------------------
 iteration   178100/  200000 | consumed samples:      5699200 | elapsed time per iteration (ms): 272.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.093815E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.49 | backward-params-all-reduce: 63.64 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.22 | optimizer-unscale-and-check-inf: 16.00 | optimizer-clip-main-grad: 7.53 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 97.37 | batch-generator: 15.36
 iteration   178200/  200000 | consumed samples:      5702400 | elapsed time per iteration (ms): 269.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.081823E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.99 | backward-params-all-reduce: 63.57 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.38 | optimizer-unscale-and-check-inf: 18.50 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 100.18 | batch-generator: 13.90
 iteration   178300/  200000 | consumed samples:      5705600 | elapsed time per iteration (ms): 270.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.086848E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.10 | backward-params-all-reduce: 63.17 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.50 | optimizer-unscale-and-check-inf: 17.81 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 14.21 | optimizer: 101.34 | batch-generator: 8.64
 iteration   178400/  200000 | consumed samples:      5708800 | elapsed time per iteration (ms): 259.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.085423E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.43 | backward-params-all-reduce: 62.93 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.32 | optimizer-unscale-and-check-inf: 20.27 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 101.89 | batch-generator: 8.46
 iteration   178500/  200000 | consumed samples:      5712000 | elapsed time per iteration (ms): 261.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.072881E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.46 | backward-params-all-reduce: 64.69 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.39 | optimizer-unscale-and-check-inf: 18.53 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 100.21 | batch-generator: 8.58
--------------------------------------------------------------------------------------------------
 validation loss at iteration 178500 | lm loss value: 3.347911E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 178500, match long value: -0.006014358770920963 | match short value: -0.03409567187065026 
---------------------------------------------------------------------------------------------------------
 iteration   178600/  200000 | consumed samples:      5715200 | elapsed time per iteration (ms): 275.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.083525E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.46 | backward-params-all-reduce: 63.74 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.32 | optimizer-unscale-and-check-inf: 19.65 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 101.29 | batch-generator: 18.65
 iteration   178700/  200000 | consumed samples:      5718400 | elapsed time per iteration (ms): 259.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.068518E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.39 | backward-params-all-reduce: 63.99 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.43 | optimizer-unscale-and-check-inf: 18.80 | optimizer-clip-main-grad: 7.51 | optimizer-copy-main-to-model-params: 12.28 | optimizer: 99.22 | batch-generator: 8.68
 iteration   178800/  200000 | consumed samples:      5721600 | elapsed time per iteration (ms): 261.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.073026E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.52 | backward-params-all-reduce: 63.96 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.06 | optimizer-unscale-and-check-inf: 18.54 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 99.86 | batch-generator: 8.56
 iteration   178900/  200000 | consumed samples:      5724800 | elapsed time per iteration (ms): 262.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.074952E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.30 | backward-params-all-reduce: 63.70 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.75 | optimizer-unscale-and-check-inf: 18.07 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 12.58 | optimizer: 100.27 | batch-generator: 8.59
 iteration   179000/  200000 | consumed samples:      5728000 | elapsed time per iteration (ms): 273.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.086556E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.28 | backward-params-all-reduce: 63.38 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.18 | optimizer-unscale-and-check-inf: 17.84 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 14.16 | optimizer: 101.01 | batch-generator: 8.59
--------------------------------------------------------------------------------------------------
 validation loss at iteration 179000 | lm loss value: 3.359090E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 179000, match long value: 0.0013202833407513503 | match short value: -0.016532976556281623 
----------------------------------------------------------------------------------------------------------
 iteration   179100/  200000 | consumed samples:      5731200 | elapsed time per iteration (ms): 272.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.084452E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.40 | backward-params-all-reduce: 64.59 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.18 | optimizer-unscale-and-check-inf: 19.12 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 100.63 | batch-generator: 15.34
 iteration   179200/  200000 | consumed samples:      5734400 | elapsed time per iteration (ms): 261.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.074372E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.39 | backward-params-all-reduce: 64.02 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.45 | optimizer-unscale-and-check-inf: 18.23 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 100.01 | batch-generator: 8.62
 iteration   179300/  200000 | consumed samples:      5737600 | elapsed time per iteration (ms): 258.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.083526E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.11 | backward-params-all-reduce: 63.53 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.12 | optimizer-unscale-and-check-inf: 18.24 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.54 | optimizer: 99.61 | batch-generator: 8.54
 iteration   179400/  200000 | consumed samples:      5740800 | elapsed time per iteration (ms): 260.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.084358E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.38 | backward-params-all-reduce: 64.10 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.44 | optimizer-unscale-and-check-inf: 17.94 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 99.74 | batch-generator: 8.51
 iteration   179500/  200000 | consumed samples:      5744000 | elapsed time per iteration (ms): 263.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.082450E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.31 | backward-params-all-reduce: 65.03 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.53 | optimizer-unscale-and-check-inf: 18.40 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 100.33 | batch-generator: 8.73
--------------------------------------------------------------------------------------------------
 validation loss at iteration 179500 | lm loss value: 3.329794E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 179500, match long value: -0.001980289190769574 | match short value: -0.00754609959151512 
---------------------------------------------------------------------------------------------------------
 iteration   179600/  200000 | consumed samples:      5747200 | elapsed time per iteration (ms): 277.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.083133E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 54.75 | backward-params-all-reduce: 63.09 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.37 | optimizer-unscale-and-check-inf: 17.62 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 99.31 | batch-generator: 15.93
 iteration   179700/  200000 | consumed samples:      5750400 | elapsed time per iteration (ms): 271.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.079765E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.11 | backward-params-all-reduce: 64.23 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.76 | optimizer-unscale-and-check-inf: 15.45 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 14.17 | optimizer: 98.22 | batch-generator: 8.78
 iteration   179800/  200000 | consumed samples:      5753600 | elapsed time per iteration (ms): 264.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.070757E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.87 | backward-params-all-reduce: 65.02 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.32 | optimizer-unscale-and-check-inf: 19.39 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.41 | optimizer: 100.37 | batch-generator: 8.50
 iteration   179900/  200000 | consumed samples:      5756800 | elapsed time per iteration (ms): 258.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.076275E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.18 | backward-params-all-reduce: 62.58 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.24 | optimizer-unscale-and-check-inf: 18.11 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 99.70 | batch-generator: 8.52
 iteration   180000/  200000 | consumed samples:      5760000 | elapsed time per iteration (ms): 261.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.081343E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.40 | backward-params-all-reduce: 63.31 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 12.26 | optimizer-unscale-and-check-inf: 17.76 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.32 | batch-generator: 8.56
--------------------------------------------------------------------------------------------------
 validation loss at iteration 180000 | lm loss value: 3.312444E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 180000, match long value: -0.004088196350566515 | match short value: -0.026986846351202062 
----------------------------------------------------------------------------------------------------------
saving checkpoint at iteration  180000 to verify1
  successfully saved checkpoint at iteration  180000 to verify1
time (ms) | save-checkpoint: 24879.53
 iteration   180100/  200000 | consumed samples:      5763200 | elapsed time per iteration (ms): 522.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.086613E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.32 | backward-params-all-reduce: 62.62 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.60 | optimizer-unscale-and-check-inf: 16.62 | optimizer-clip-main-grad: 7.70 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 98.64 | batch-generator: 19.81
 iteration   180200/  200000 | consumed samples:      5766400 | elapsed time per iteration (ms): 258.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.080432E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.08 | backward-params-all-reduce: 63.11 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.10 | optimizer-unscale-and-check-inf: 17.23 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.54 | batch-generator: 8.38
 iteration   180300/  200000 | consumed samples:      5769600 | elapsed time per iteration (ms): 262.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.073811E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.51 | backward-params-all-reduce: 63.81 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.12 | optimizer-unscale-and-check-inf: 16.38 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 12.55 | optimizer: 97.90 | batch-generator: 8.53
 iteration   180400/  200000 | consumed samples:      5772800 | elapsed time per iteration (ms): 264.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.075109E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.35 | backward-params-all-reduce: 63.94 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.72 | optimizer-unscale-and-check-inf: 16.52 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 98.59 | batch-generator: 8.60
 iteration   180500/  200000 | consumed samples:      5776000 | elapsed time per iteration (ms): 271.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.088142E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.88 | backward-params-all-reduce: 64.68 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.50 | optimizer-unscale-and-check-inf: 16.54 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 14.40 | optimizer: 100.25 | batch-generator: 8.97
--------------------------------------------------------------------------------------------------
 validation loss at iteration 180500 | lm loss value: 3.282218E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 180500, match long value: -0.005929894445773419 | match short value: -0.02511589645416158 
---------------------------------------------------------------------------------------------------------
 iteration   180600/  200000 | consumed samples:      5779200 | elapsed time per iteration (ms): 271.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.080937E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.28 | backward-params-all-reduce: 64.69 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.23 | optimizer-unscale-and-check-inf: 15.74 | optimizer-clip-main-grad: 7.54 | optimizer-copy-main-to-model-params: 12.36 | optimizer: 96.55 | batch-generator: 15.88
 iteration   180700/  200000 | consumed samples:      5782400 | elapsed time per iteration (ms): 258.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.096082E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.29 | backward-params-all-reduce: 62.55 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.07 | optimizer-unscale-and-check-inf: 17.65 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.04 | batch-generator: 8.58
 iteration   180800/  200000 | consumed samples:      5785600 | elapsed time per iteration (ms): 260.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.091142E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.45 | backward-params-all-reduce: 62.41 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.18 | optimizer-unscale-and-check-inf: 18.02 | optimizer-clip-main-grad: 7.74 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.64 | batch-generator: 8.71
 iteration   180900/  200000 | consumed samples:      5788800 | elapsed time per iteration (ms): 258.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.083862E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.75 | backward-params-all-reduce: 61.73 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.19 | optimizer-unscale-and-check-inf: 18.59 | optimizer-clip-main-grad: 7.70 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 100.15 | batch-generator: 8.66
 iteration   181000/  200000 | consumed samples:      5792000 | elapsed time per iteration (ms): 260.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.084035E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.69 | backward-params-all-reduce: 62.76 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.10 | optimizer-unscale-and-check-inf: 17.40 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 98.85 | batch-generator: 8.73
--------------------------------------------------------------------------------------------------
 validation loss at iteration 181000 | lm loss value: 3.354590E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 181000, match long value: -0.0028240199110784723 | match short value: -0.01979850864149106 
----------------------------------------------------------------------------------------------------------
 iteration   181100/  200000 | consumed samples:      5795200 | elapsed time per iteration (ms): 282.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.082774E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.94 | backward-params-all-reduce: 63.62 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.67 | optimizer-unscale-and-check-inf: 18.37 | optimizer-clip-main-grad: 7.70 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 100.39 | batch-generator: 15.80
 iteration   181200/  200000 | consumed samples:      5798400 | elapsed time per iteration (ms): 273.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.071295E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.09 | backward-params-all-reduce: 64.04 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.14 | optimizer-unscale-and-check-inf: 18.05 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 14.24 | optimizer: 101.29 | batch-generator: 8.59
 iteration   181300/  200000 | consumed samples:      5801600 | elapsed time per iteration (ms): 266.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.076894E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.51 | backward-params-all-reduce: 64.16 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.18 | optimizer-unscale-and-check-inf: 18.97 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 100.55 | batch-generator: 14.98
 iteration   181400/  200000 | consumed samples:      5804800 | elapsed time per iteration (ms): 260.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.067901E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.62 | backward-params-all-reduce: 65.25 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.03 | optimizer-unscale-and-check-inf: 20.31 | optimizer-clip-main-grad: 7.71 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 101.71 | batch-generator: 8.67
 iteration   181500/  200000 | consumed samples:      5808000 | elapsed time per iteration (ms): 259.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.074643E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.54 | backward-params-all-reduce: 64.98 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.56 | optimizer-unscale-and-check-inf: 17.73 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.62 | batch-generator: 8.68
--------------------------------------------------------------------------------------------------
 validation loss at iteration 181500 | lm loss value: 3.340371E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 181500, match long value: -0.0019179101581766216 | match short value: 0.003966026773565898 
----------------------------------------------------------------------------------------------------------
 iteration   181600/  200000 | consumed samples:      5811200 | elapsed time per iteration (ms): 277.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.068492E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.98 | backward-params-all-reduce: 65.25 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.29 | optimizer-unscale-and-check-inf: 16.62 | optimizer-clip-main-grad: 7.75 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 98.40 | batch-generator: 20.67
 iteration   181700/  200000 | consumed samples:      5814400 | elapsed time per iteration (ms): 264.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.079343E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.19 | backward-params-all-reduce: 63.46 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.25 | optimizer-unscale-and-check-inf: 19.67 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 101.30 | batch-generator: 8.68
 iteration   181800/  200000 | consumed samples:      5817600 | elapsed time per iteration (ms): 261.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.074212E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.65 | backward-params-all-reduce: 62.58 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.53 | optimizer-unscale-and-check-inf: 19.08 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 100.95 | batch-generator: 8.65
 iteration   181900/  200000 | consumed samples:      5820800 | elapsed time per iteration (ms): 272.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.074725E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.64 | backward-params-all-reduce: 66.03 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.13 | optimizer-unscale-and-check-inf: 17.94 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 14.32 | optimizer: 101.24 | batch-generator: 8.69
 iteration   182000/  200000 | consumed samples:      5824000 | elapsed time per iteration (ms): 268.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.070697E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.69 | backward-params-all-reduce: 65.09 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.50 | optimizer-unscale-and-check-inf: 18.73 | optimizer-clip-main-grad: 7.70 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 100.60 | batch-generator: 8.65
--------------------------------------------------------------------------------------------------
 validation loss at iteration 182000 | lm loss value: 3.314962E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 182000, match long value: -0.0020422736295805525 | match short value: -0.038237914828483585 
-----------------------------------------------------------------------------------------------------------
 iteration   182100/  200000 | consumed samples:      5827200 | elapsed time per iteration (ms): 282.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.069264E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.87 | backward-params-all-reduce: 65.18 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.38 | optimizer-unscale-and-check-inf: 19.44 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 101.21 | batch-generator: 16.14
 iteration   182200/  200000 | consumed samples:      5830400 | elapsed time per iteration (ms): 259.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.068549E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.85 | backward-params-all-reduce: 62.78 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.26 | optimizer-unscale-and-check-inf: 18.62 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 12.44 | optimizer: 100.20 | batch-generator: 8.79
 iteration   182300/  200000 | consumed samples:      5833600 | elapsed time per iteration (ms): 257.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.065985E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.73 | backward-params-all-reduce: 61.67 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.45 | optimizer-unscale-and-check-inf: 18.23 | optimizer-clip-main-grad: 7.50 | optimizer-copy-main-to-model-params: 12.21 | optimizer: 98.59 | batch-generator: 8.77
 iteration   182400/  200000 | consumed samples:      5836800 | elapsed time per iteration (ms): 258.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.071703E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.78 | backward-params-all-reduce: 64.52 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 11.90 | optimizer-unscale-and-check-inf: 19.82 | optimizer-clip-main-grad: 7.70 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 101.09 | batch-generator: 8.72
 iteration   182500/  200000 | consumed samples:      5840000 | elapsed time per iteration (ms): 260.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.081193E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.35 | backward-params-all-reduce: 62.42 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.05 | optimizer-unscale-and-check-inf: 18.67 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 100.07 | batch-generator: 8.65
--------------------------------------------------------------------------------------------------
 validation loss at iteration 182500 | lm loss value: 3.313526E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 182500, match long value: -0.00018761768218470453 | match short value: -0.0013585324922756618 
-------------------------------------------------------------------------------------------------------------
 iteration   182600/  200000 | consumed samples:      5843200 | elapsed time per iteration (ms): 280.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.077068E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.64 | backward-params-all-reduce: 64.86 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.56 | optimizer-unscale-and-check-inf: 17.65 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 14.29 | optimizer: 101.37 | batch-generator: 17.22
 iteration   182700/  200000 | consumed samples:      5846400 | elapsed time per iteration (ms): 270.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.072343E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.88 | backward-params-all-reduce: 64.02 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.38 | optimizer-unscale-and-check-inf: 18.97 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 100.69 | batch-generator: 8.73
 iteration   182800/  200000 | consumed samples:      5849600 | elapsed time per iteration (ms): 260.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.071044E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.36 | backward-params-all-reduce: 64.22 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.17 | optimizer-unscale-and-check-inf: 19.88 | optimizer-clip-main-grad: 7.71 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 101.47 | batch-generator: 8.77
 iteration   182900/  200000 | consumed samples:      5852800 | elapsed time per iteration (ms): 260.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.066450E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.07 | backward-params-all-reduce: 65.69 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.02 | optimizer-unscale-and-check-inf: 18.54 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 99.92 | batch-generator: 8.64
 iteration   183000/  200000 | consumed samples:      5856000 | elapsed time per iteration (ms): 259.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.066483E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.84 | backward-params-all-reduce: 64.06 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.16 | optimizer-unscale-and-check-inf: 17.40 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 98.83 | batch-generator: 8.62
--------------------------------------------------------------------------------------------------
 validation loss at iteration 183000 | lm loss value: 3.333187E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 183000, match long value: -0.0026325079937083015 | match short value: -0.027918875908267814 
-----------------------------------------------------------------------------------------------------------
 iteration   183100/  200000 | consumed samples:      5859200 | elapsed time per iteration (ms): 276.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.072278E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.20 | backward-params-all-reduce: 63.16 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.54 | optimizer-unscale-and-check-inf: 16.76 | optimizer-clip-main-grad: 7.72 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 98.72 | batch-generator: 21.18
 iteration   183200/  200000 | consumed samples:      5862400 | elapsed time per iteration (ms): 259.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.069766E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.79 | backward-params-all-reduce: 62.73 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 11.99 | optimizer-unscale-and-check-inf: 18.09 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 99.27 | batch-generator: 8.62
 iteration   183300/  200000 | consumed samples:      5865600 | elapsed time per iteration (ms): 256.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.068078E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.53 | backward-params-all-reduce: 65.22 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 11.96 | optimizer-unscale-and-check-inf: 15.73 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.43 | optimizer: 96.89 | batch-generator: 8.66
 iteration   183400/  200000 | consumed samples:      5868800 | elapsed time per iteration (ms): 271.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.070702E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.85 | backward-params-all-reduce: 62.96 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.53 | optimizer-unscale-and-check-inf: 18.99 | optimizer-clip-main-grad: 7.71 | optimizer-copy-main-to-model-params: 14.40 | optimizer: 102.82 | batch-generator: 8.73
 iteration   183500/  200000 | consumed samples:      5872000 | elapsed time per iteration (ms): 260.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.075099E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.68 | backward-params-all-reduce: 62.63 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.68 | optimizer-unscale-and-check-inf: 18.96 | optimizer-clip-main-grad: 7.54 | optimizer-copy-main-to-model-params: 12.24 | optimizer: 99.64 | batch-generator: 8.70
--------------------------------------------------------------------------------------------------
 validation loss at iteration 183500 | lm loss value: 3.280719E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 183500, match long value: -0.0076348626297153875 | match short value: -0.023603969460693992 
-----------------------------------------------------------------------------------------------------------
 iteration   183600/  200000 | consumed samples:      5875200 | elapsed time per iteration (ms): 269.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.085013E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.10 | backward-params-all-reduce: 63.89 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.01 | optimizer-unscale-and-check-inf: 18.78 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 100.04 | batch-generator: 14.82
 iteration   183700/  200000 | consumed samples:      5878400 | elapsed time per iteration (ms): 260.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.068785E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.59 | backward-params-all-reduce: 62.94 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.29 | optimizer-unscale-and-check-inf: 18.14 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.70 | batch-generator: 8.57
 iteration   183800/  200000 | consumed samples:      5881600 | elapsed time per iteration (ms): 261.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.062045E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.05 | backward-params-all-reduce: 64.71 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.48 | optimizer-unscale-and-check-inf: 18.03 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 99.86 | batch-generator: 8.47
 iteration   183900/  200000 | consumed samples:      5884800 | elapsed time per iteration (ms): 358.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.068805E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.26 | backward-params-all-reduce: 63.55 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.39 | optimizer-unscale-and-check-inf: 17.41 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.16 | batch-generator: 106.33
 iteration   184000/  200000 | consumed samples:      5888000 | elapsed time per iteration (ms): 258.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.073772E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.73 | backward-params-all-reduce: 63.60 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.82 | optimizer-unscale-and-check-inf: 15.83 | optimizer-clip-main-grad: 7.74 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 98.09 | batch-generator: 8.62
--------------------------------------------------------------------------------------------------
 validation loss at iteration 184000 | lm loss value: 3.327537E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 184000, match long value: -0.002678545541693547 | match short value: -0.012520266291160561 
----------------------------------------------------------------------------------------------------------
 iteration   184100/  200000 | consumed samples:      5891200 | elapsed time per iteration (ms): 281.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.083381E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.74 | backward-params-all-reduce: 63.57 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.33 | optimizer-unscale-and-check-inf: 17.68 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 14.19 | optimizer: 100.96 | batch-generator: 15.86
 iteration   184200/  200000 | consumed samples:      5894400 | elapsed time per iteration (ms): 262.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.071946E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.64 | backward-params-all-reduce: 64.98 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.05 | optimizer-unscale-and-check-inf: 17.09 | optimizer-clip-main-grad: 7.78 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 99.64 | batch-generator: 8.59
 iteration   184300/  200000 | consumed samples:      5897600 | elapsed time per iteration (ms): 260.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.061211E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.75 | backward-params-all-reduce: 65.21 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.15 | optimizer-unscale-and-check-inf: 17.36 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.79 | batch-generator: 8.51
 iteration   184400/  200000 | consumed samples:      5900800 | elapsed time per iteration (ms): 265.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.074769E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.41 | backward-params-all-reduce: 64.06 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.69 | optimizer-unscale-and-check-inf: 16.93 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.89 | batch-generator: 14.80
 iteration   184500/  200000 | consumed samples:      5904000 | elapsed time per iteration (ms): 260.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.060253E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.94 | backward-params-all-reduce: 65.01 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.91 | optimizer-unscale-and-check-inf: 17.56 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.70 | batch-generator: 8.58
--------------------------------------------------------------------------------------------------
 validation loss at iteration 184500 | lm loss value: 3.392691E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 184500, match long value: -0.00229245786866282 | match short value: -0.038120396504592924 
---------------------------------------------------------------------------------------------------------
 iteration   184600/  200000 | consumed samples:      5907200 | elapsed time per iteration (ms): 275.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.077384E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.18 | backward-params-all-reduce: 64.18 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.03 | optimizer-unscale-and-check-inf: 17.91 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 100.16 | batch-generator: 20.64
 iteration   184700/  200000 | consumed samples:      5910400 | elapsed time per iteration (ms): 261.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.052858E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.22 | backward-params-all-reduce: 65.49 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.62 | optimizer-unscale-and-check-inf: 18.31 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 100.15 | batch-generator: 8.46
 iteration   184800/  200000 | consumed samples:      5913600 | elapsed time per iteration (ms): 265.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.086069E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.25 | backward-params-all-reduce: 66.01 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.63 | optimizer-unscale-and-check-inf: 18.82 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 14.31 | optimizer: 102.50 | batch-generator: 8.40
 iteration   184900/  200000 | consumed samples:      5916800 | elapsed time per iteration (ms): 265.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.062117E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.49 | backward-params-all-reduce: 65.09 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.70 | optimizer-unscale-and-check-inf: 18.12 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 100.00 | batch-generator: 8.42
 iteration   185000/  200000 | consumed samples:      5920000 | elapsed time per iteration (ms): 259.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.058440E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.34 | backward-params-all-reduce: 65.00 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.80 | optimizer-unscale-and-check-inf: 17.23 | optimizer-clip-main-grad: 7.54 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.19 | batch-generator: 8.36
--------------------------------------------------------------------------------------------------
 validation loss at iteration 185000 | lm loss value: 3.209630E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 185000, match long value: -0.005932018079438671 | match short value: -0.010680294456440491 
----------------------------------------------------------------------------------------------------------
 iteration   185100/  200000 | consumed samples:      5923200 | elapsed time per iteration (ms): 271.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.058521E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.42 | backward-params-all-reduce: 65.86 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.76 | optimizer-unscale-and-check-inf: 17.53 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.53 | batch-generator: 16.19
 iteration   185200/  200000 | consumed samples:      5926400 | elapsed time per iteration (ms): 259.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.067998E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.10 | backward-params-all-reduce: 65.10 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.11 | optimizer-unscale-and-check-inf: 17.13 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.47 | batch-generator: 8.52
 iteration   185300/  200000 | consumed samples:      5929600 | elapsed time per iteration (ms): 259.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.065311E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.04 | backward-params-all-reduce: 65.24 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.04 | optimizer-unscale-and-check-inf: 17.41 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.66 | batch-generator: 8.34
 iteration   185400/  200000 | consumed samples:      5932800 | elapsed time per iteration (ms): 262.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.055257E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.06 | backward-params-all-reduce: 65.09 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.77 | optimizer-unscale-and-check-inf: 18.03 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 100.09 | batch-generator: 8.41
 iteration   185500/  200000 | consumed samples:      5936000 | elapsed time per iteration (ms): 259.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.053355E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.80 | backward-params-all-reduce: 65.98 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.70 | optimizer-unscale-and-check-inf: 18.08 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 99.93 | batch-generator: 8.40
--------------------------------------------------------------------------------------------------
 validation loss at iteration 185500 | lm loss value: 3.249632E-05 | lm loss PPL: 1.000032E+00 | 
 at iteration 185500, match long value: -0.003413962476878728 | match short value: 0.002837537571508638 
---------------------------------------------------------------------------------------------------------
 iteration   185600/  200000 | consumed samples:      5939200 | elapsed time per iteration (ms): 281.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.056709E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.68 | backward-params-all-reduce: 64.70 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.46 | optimizer-unscale-and-check-inf: 18.06 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 14.25 | optimizer: 101.56 | batch-generator: 15.15
 iteration   185700/  200000 | consumed samples:      5942400 | elapsed time per iteration (ms): 257.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.058475E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.91 | backward-params-all-reduce: 63.15 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.64 | optimizer-unscale-and-check-inf: 18.57 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 100.45 | batch-generator: 8.41
 iteration   185800/  200000 | consumed samples:      5945600 | elapsed time per iteration (ms): 259.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.052745E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.06 | backward-params-all-reduce: 64.41 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.50 | optimizer-unscale-and-check-inf: 18.91 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 100.59 | batch-generator: 8.36
 iteration   185900/  200000 | consumed samples:      5948800 | elapsed time per iteration (ms): 259.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.072867E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.74 | backward-params-all-reduce: 63.77 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.27 | optimizer-unscale-and-check-inf: 17.02 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.45 | optimizer: 99.43 | batch-generator: 8.34
 iteration   186000/  200000 | consumed samples:      5952000 | elapsed time per iteration (ms): 259.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.070124E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.09 | backward-params-all-reduce: 65.21 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.04 | optimizer-unscale-and-check-inf: 17.44 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.45 | optimizer: 99.62 | batch-generator: 8.43
--------------------------------------------------------------------------------------------------
 validation loss at iteration 186000 | lm loss value: 3.533464E-05 | lm loss PPL: 1.000035E+00 | 
 at iteration 186000, match long value: -0.0055286541210296655 | match short value: -0.025991032773002555 
-----------------------------------------------------------------------------------------------------------
 iteration   186100/  200000 | consumed samples:      5955200 | elapsed time per iteration (ms): 271.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.066494E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.30 | backward-params-all-reduce: 64.21 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.07 | optimizer-unscale-and-check-inf: 16.93 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.26 | batch-generator: 16.05
 iteration   186200/  200000 | consumed samples:      5958400 | elapsed time per iteration (ms): 259.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.066510E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.09 | backward-params-all-reduce: 65.86 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.78 | optimizer-unscale-and-check-inf: 18.21 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 100.22 | batch-generator: 8.54
 iteration   186300/  200000 | consumed samples:      5961600 | elapsed time per iteration (ms): 270.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.061750E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.77 | backward-params-all-reduce: 65.28 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.77 | optimizer-unscale-and-check-inf: 16.85 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 14.34 | optimizer: 100.74 | batch-generator: 8.62
 iteration   186400/  200000 | consumed samples:      5964800 | elapsed time per iteration (ms): 258.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.055488E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.66 | backward-params-all-reduce: 63.10 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.70 | optimizer-unscale-and-check-inf: 17.81 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 99.81 | batch-generator: 8.60
 iteration   186500/  200000 | consumed samples:      5968000 | elapsed time per iteration (ms): 258.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.068889E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.49 | backward-params-all-reduce: 64.10 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.92 | optimizer-unscale-and-check-inf: 17.01 | optimizer-clip-main-grad: 7.46 | optimizer-copy-main-to-model-params: 12.27 | optimizer: 97.83 | batch-generator: 8.59
--------------------------------------------------------------------------------------------------
 validation loss at iteration 186500 | lm loss value: 3.389555E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 186500, match long value: -0.003688776340472891 | match short value: -0.007258427339571281 
----------------------------------------------------------------------------------------------------------
 iteration   186600/  200000 | consumed samples:      5971200 | elapsed time per iteration (ms): 275.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.066633E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.74 | backward-params-all-reduce: 62.34 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.48 | optimizer-unscale-and-check-inf: 15.45 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.17 | batch-generator: 21.32
 iteration   186700/  200000 | consumed samples:      5974400 | elapsed time per iteration (ms): 260.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.067252E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.29 | backward-params-all-reduce: 65.17 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.74 | optimizer-unscale-and-check-inf: 17.60 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 99.58 | batch-generator: 8.42
 iteration   186800/  200000 | consumed samples:      5977600 | elapsed time per iteration (ms): 260.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.068749E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.20 | backward-params-all-reduce: 63.42 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.40 | optimizer-unscale-and-check-inf: 16.29 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.01 | batch-generator: 8.46
 iteration   186900/  200000 | consumed samples:      5980800 | elapsed time per iteration (ms): 260.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.059654E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.21 | backward-params-all-reduce: 64.11 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.07 | optimizer-unscale-and-check-inf: 15.04 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 97.34 | batch-generator: 8.45
 iteration   187000/  200000 | consumed samples:      5984000 | elapsed time per iteration (ms): 263.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.055153E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.41 | backward-params-all-reduce: 63.67 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.77 | optimizer-unscale-and-check-inf: 15.47 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 14.28 | optimizer: 99.25 | batch-generator: 8.57
--------------------------------------------------------------------------------------------------
 validation loss at iteration 187000 | lm loss value: 3.322828E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 187000, match long value: -0.0034935212765305843 | match short value: -0.015204024013373986 
-----------------------------------------------------------------------------------------------------------
 iteration   187100/  200000 | consumed samples:      5987200 | elapsed time per iteration (ms): 279.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.065859E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.00 | backward-params-all-reduce: 63.52 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.49 | optimizer-unscale-and-check-inf: 15.35 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 97.11 | batch-generator: 16.30
 iteration   187200/  200000 | consumed samples:      5990400 | elapsed time per iteration (ms): 258.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.065315E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.17 | backward-params-all-reduce: 63.22 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.21 | optimizer-unscale-and-check-inf: 16.56 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 97.97 | batch-generator: 8.48
 iteration   187300/  200000 | consumed samples:      5993600 | elapsed time per iteration (ms): 258.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.057157E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.21 | backward-params-all-reduce: 62.80 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.74 | optimizer-unscale-and-check-inf: 15.68 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 97.59 | batch-generator: 8.44
 iteration   187400/  200000 | consumed samples:      5996800 | elapsed time per iteration (ms): 257.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.065723E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.34 | backward-params-all-reduce: 63.16 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.55 | optimizer-unscale-and-check-inf: 14.94 | optimizer-clip-main-grad: 7.54 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 96.67 | batch-generator: 8.40
 iteration   187500/  200000 | consumed samples:      6000000 | elapsed time per iteration (ms): 257.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.078610E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.25 | backward-params-all-reduce: 63.08 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.47 | optimizer-unscale-and-check-inf: 15.51 | optimizer-clip-main-grad: 7.45 | optimizer-copy-main-to-model-params: 12.25 | optimizer: 95.88 | batch-generator: 8.52
--------------------------------------------------------------------------------------------------
 validation loss at iteration 187500 | lm loss value: 3.297598E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 187500, match long value: -0.004943999885038019 | match short value: -0.009094760961449106 
----------------------------------------------------------------------------------------------------------
 iteration   187600/  200000 | consumed samples:      6003200 | elapsed time per iteration (ms): 279.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.039907E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.50 | backward-params-all-reduce: 63.79 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.81 | optimizer-unscale-and-check-inf: 17.74 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 99.82 | batch-generator: 21.51
 iteration   187700/  200000 | consumed samples:      6006400 | elapsed time per iteration (ms): 259.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.077934E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.12 | backward-params-all-reduce: 64.44 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.72 | optimizer-unscale-and-check-inf: 16.49 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 98.39 | batch-generator: 8.55
 iteration   187800/  200000 | consumed samples:      6009600 | elapsed time per iteration (ms): 271.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.069839E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.84 | backward-params-all-reduce: 64.02 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.69 | optimizer-unscale-and-check-inf: 16.89 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 14.13 | optimizer: 100.49 | batch-generator: 8.64
 iteration   187900/  200000 | consumed samples:      6012800 | elapsed time per iteration (ms): 259.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.054250E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.02 | backward-params-all-reduce: 64.05 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.18 | optimizer-unscale-and-check-inf: 18.32 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.73 | batch-generator: 8.60
 iteration   188000/  200000 | consumed samples:      6016000 | elapsed time per iteration (ms): 259.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.054959E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.12 | backward-params-all-reduce: 63.79 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.75 | optimizer-unscale-and-check-inf: 15.99 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.02 | batch-generator: 8.55
--------------------------------------------------------------------------------------------------
 validation loss at iteration 188000 | lm loss value: 3.278518E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 188000, match long value: -0.0030918924225100098 | match short value: -0.009903290749398275 
-----------------------------------------------------------------------------------------------------------
 iteration   188100/  200000 | consumed samples:      6019200 | elapsed time per iteration (ms): 279.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.051376E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.69 | backward-params-all-reduce: 66.15 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.52 | optimizer-unscale-and-check-inf: 16.97 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 98.69 | batch-generator: 21.29
 iteration   188200/  200000 | consumed samples:      6022400 | elapsed time per iteration (ms): 259.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.050897E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.19 | backward-params-all-reduce: 63.41 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.06 | optimizer-unscale-and-check-inf: 17.63 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 100.01 | batch-generator: 8.54
 iteration   188300/  200000 | consumed samples:      6025600 | elapsed time per iteration (ms): 261.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.053377E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.05 | backward-params-all-reduce: 63.41 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.08 | optimizer-unscale-and-check-inf: 16.48 | optimizer-clip-main-grad: 7.72 | optimizer-copy-main-to-model-params: 12.55 | optimizer: 98.99 | batch-generator: 8.71
 iteration   188400/  200000 | consumed samples:      6028800 | elapsed time per iteration (ms): 259.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.059241E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.11 | backward-params-all-reduce: 63.53 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.12 | optimizer-unscale-and-check-inf: 16.83 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.21 | batch-generator: 8.53
 iteration   188500/  200000 | consumed samples:      6032000 | elapsed time per iteration (ms): 270.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.052619E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.42 | backward-params-all-reduce: 64.10 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.44 | optimizer-unscale-and-check-inf: 15.89 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 14.06 | optimizer: 98.16 | batch-generator: 8.98
--------------------------------------------------------------------------------------------------
 validation loss at iteration 188500 | lm loss value: 3.259366E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 188500, match long value: -0.0012439518239543716 | match short value: -0.01766684917633013 
----------------------------------------------------------------------------------------------------------
 iteration   188600/  200000 | consumed samples:      6035200 | elapsed time per iteration (ms): 273.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.065623E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.15 | backward-params-all-reduce: 64.87 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.32 | optimizer-unscale-and-check-inf: 16.87 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 98.45 | batch-generator: 16.49
 iteration   188700/  200000 | consumed samples:      6038400 | elapsed time per iteration (ms): 258.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.059663E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.08 | backward-params-all-reduce: 64.09 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.25 | optimizer-unscale-and-check-inf: 16.47 | optimizer-clip-main-grad: 7.46 | optimizer-copy-main-to-model-params: 12.32 | optimizer: 96.68 | batch-generator: 8.72
 iteration   188800/  200000 | consumed samples:      6041600 | elapsed time per iteration (ms): 264.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.060753E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.76 | backward-params-all-reduce: 64.15 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.24 | optimizer-unscale-and-check-inf: 18.46 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.99 | batch-generator: 8.63
 iteration   188900/  200000 | consumed samples:      6044800 | elapsed time per iteration (ms): 257.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.055550E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.09 | backward-params-all-reduce: 63.48 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.37 | optimizer-unscale-and-check-inf: 16.58 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 98.16 | batch-generator: 8.60
 iteration   189000/  200000 | consumed samples:      6048000 | elapsed time per iteration (ms): 259.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.060761E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.18 | backward-params-all-reduce: 63.71 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.45 | optimizer-unscale-and-check-inf: 16.91 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.65 | batch-generator: 8.61
--------------------------------------------------------------------------------------------------
 validation loss at iteration 189000 | lm loss value: 3.456146E-05 | lm loss PPL: 1.000035E+00 | 
 at iteration 189000, match long value: -0.0006566386403297299 | match short value: -0.03232664114219684 
----------------------------------------------------------------------------------------------------------
 iteration   189100/  200000 | consumed samples:      6051200 | elapsed time per iteration (ms): 274.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.071333E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.47 | backward-params-all-reduce: 63.34 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.38 | optimizer-unscale-and-check-inf: 18.01 | optimizer-clip-main-grad: 7.72 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 99.84 | batch-generator: 17.23
 iteration   189200/  200000 | consumed samples:      6054400 | elapsed time per iteration (ms): 266.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.059049E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.69 | backward-params-all-reduce: 63.57 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.08 | optimizer-unscale-and-check-inf: 18.22 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 14.25 | optimizer: 101.41 | batch-generator: 8.83
 iteration   189300/  200000 | consumed samples:      6057600 | elapsed time per iteration (ms): 268.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.050897E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.63 | backward-params-all-reduce: 64.31 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.66 | optimizer-unscale-and-check-inf: 18.34 | optimizer-clip-main-grad: 7.72 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 100.38 | batch-generator: 8.70
 iteration   189400/  200000 | consumed samples:      6060800 | elapsed time per iteration (ms): 261.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.045807E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.46 | backward-params-all-reduce: 62.51 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.72 | optimizer-unscale-and-check-inf: 18.12 | optimizer-clip-main-grad: 7.87 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 101.41 | batch-generator: 8.95
 iteration   189500/  200000 | consumed samples:      6064000 | elapsed time per iteration (ms): 260.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.063733E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.68 | backward-params-all-reduce: 63.90 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.16 | optimizer-unscale-and-check-inf: 20.13 | optimizer-clip-main-grad: 7.76 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 101.70 | batch-generator: 8.73
--------------------------------------------------------------------------------------------------
 validation loss at iteration 189500 | lm loss value: 3.490132E-05 | lm loss PPL: 1.000035E+00 | 
 at iteration 189500, match long value: -0.0017753473326243148 | match short value: -0.018440404501689375 
-----------------------------------------------------------------------------------------------------------
 iteration   189600/  200000 | consumed samples:      6067200 | elapsed time per iteration (ms): 280.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.059211E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.98 | backward-params-all-reduce: 63.79 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.72 | optimizer-unscale-and-check-inf: 18.45 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 100.51 | batch-generator: 22.52
 iteration   189700/  200000 | consumed samples:      6070400 | elapsed time per iteration (ms): 260.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.060878E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.11 | backward-params-all-reduce: 63.82 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.39 | optimizer-unscale-and-check-inf: 17.70 | optimizer-clip-main-grad: 7.75 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.52 | batch-generator: 8.82
 iteration   189800/  200000 | consumed samples:      6073600 | elapsed time per iteration (ms): 261.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.067352E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.81 | backward-params-all-reduce: 64.35 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 11.98 | optimizer-unscale-and-check-inf: 17.69 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.00 | batch-generator: 8.75
 iteration   189900/  200000 | consumed samples:      6076800 | elapsed time per iteration (ms): 258.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.066724E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.01 | backward-params-all-reduce: 62.74 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.18 | optimizer-unscale-and-check-inf: 17.79 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.22 | batch-generator: 8.69
 iteration   190000/  200000 | consumed samples:      6080000 | elapsed time per iteration (ms): 273.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.074373E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.70 | backward-params-all-reduce: 64.60 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.30 | optimizer-unscale-and-check-inf: 18.40 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 14.22 | optimizer: 101.81 | batch-generator: 8.79
--------------------------------------------------------------------------------------------------
 validation loss at iteration 190000 | lm loss value: 3.491938E-05 | lm loss PPL: 1.000035E+00 | 
 at iteration 190000, match long value: -0.0035685876095797476 | match short value: -0.016648525063411745 
-----------------------------------------------------------------------------------------------------------
 iteration   190100/  200000 | consumed samples:      6083200 | elapsed time per iteration (ms): 273.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.080996E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.54 | backward-params-all-reduce: 64.30 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 11.93 | optimizer-unscale-and-check-inf: 18.05 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.34 | batch-generator: 16.71
 iteration   190200/  200000 | consumed samples:      6086400 | elapsed time per iteration (ms): 259.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.064425E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.54 | backward-params-all-reduce: 62.03 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.20 | optimizer-unscale-and-check-inf: 18.47 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 12.45 | optimizer: 99.96 | batch-generator: 8.82
 iteration   190300/  200000 | consumed samples:      6089600 | elapsed time per iteration (ms): 258.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.064381E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.60 | backward-params-all-reduce: 61.99 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.15 | optimizer-unscale-and-check-inf: 19.01 | optimizer-clip-main-grad: 7.70 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 100.58 | batch-generator: 8.74
 iteration   190400/  200000 | consumed samples:      6092800 | elapsed time per iteration (ms): 257.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.068555E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.44 | backward-params-all-reduce: 62.80 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 11.96 | optimizer-unscale-and-check-inf: 16.65 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 97.90 | batch-generator: 8.61
 iteration   190500/  200000 | consumed samples:      6096000 | elapsed time per iteration (ms): 259.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.060953E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.54 | backward-params-all-reduce: 61.93 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.29 | optimizer-unscale-and-check-inf: 18.37 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 100.00 | batch-generator: 8.68
--------------------------------------------------------------------------------------------------
 validation loss at iteration 190500 | lm loss value: 3.127966E-05 | lm loss PPL: 1.000031E+00 | 
 at iteration 190500, match long value: -0.001798155661242276 | match short value: -0.048446996363562644 
----------------------------------------------------------------------------------------------------------
 iteration   190600/  200000 | consumed samples:      6099200 | elapsed time per iteration (ms): 274.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.057030E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.86 | backward-params-all-reduce: 64.57 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.32 | optimizer-unscale-and-check-inf: 16.04 | optimizer-clip-main-grad: 7.70 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 97.74 | batch-generator: 17.53
 iteration   190700/  200000 | consumed samples:      6102400 | elapsed time per iteration (ms): 277.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.074546E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.83 | backward-params-all-reduce: 63.55 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.39 | optimizer-unscale-and-check-inf: 16.16 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 14.13 | optimizer: 99.49 | batch-generator: 14.25
 iteration   190800/  200000 | consumed samples:      6105600 | elapsed time per iteration (ms): 261.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.054657E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.27 | backward-params-all-reduce: 65.24 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.74 | optimizer-unscale-and-check-inf: 17.75 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.76 | batch-generator: 8.64
 iteration   190900/  200000 | consumed samples:      6108800 | elapsed time per iteration (ms): 262.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.052057E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.85 | backward-params-all-reduce: 64.96 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.57 | optimizer-unscale-and-check-inf: 18.55 | optimizer-clip-main-grad: 7.70 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 100.48 | batch-generator: 8.74
 iteration   191000/  200000 | consumed samples:      6112000 | elapsed time per iteration (ms): 261.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.050854E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.08 | backward-params-all-reduce: 65.13 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.44 | optimizer-unscale-and-check-inf: 18.78 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 100.60 | batch-generator: 8.77
--------------------------------------------------------------------------------------------------
 validation loss at iteration 191000 | lm loss value: 3.353207E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 191000, match long value: -0.006647650848428599 | match short value: -0.009563520146526136 
----------------------------------------------------------------------------------------------------------
 iteration   191100/  200000 | consumed samples:      6115200 | elapsed time per iteration (ms): 281.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.069816E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.82 | backward-params-all-reduce: 65.54 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.77 | optimizer-unscale-and-check-inf: 18.49 | optimizer-clip-main-grad: 7.70 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 100.66 | batch-generator: 22.15
 iteration   191200/  200000 | consumed samples:      6118400 | elapsed time per iteration (ms): 261.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.064359E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.65 | backward-params-all-reduce: 64.94 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.58 | optimizer-unscale-and-check-inf: 20.09 | optimizer-clip-main-grad: 7.73 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 102.03 | batch-generator: 8.77
 iteration   191300/  200000 | consumed samples:      6121600 | elapsed time per iteration (ms): 263.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.060788E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.46 | backward-params-all-reduce: 65.55 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.45 | optimizer-unscale-and-check-inf: 19.63 | optimizer-clip-main-grad: 7.71 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 101.47 | batch-generator: 8.67
 iteration   191400/  200000 | consumed samples:      6124800 | elapsed time per iteration (ms): 267.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.074601E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 54.20 | backward-params-all-reduce: 63.49 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.91 | optimizer-unscale-and-check-inf: 18.88 | optimizer-clip-main-grad: 7.74 | optimizer-copy-main-to-model-params: 14.37 | optimizer: 103.11 | batch-generator: 8.82
 iteration   191500/  200000 | consumed samples:      6128000 | elapsed time per iteration (ms): 269.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.073551E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.17 | backward-params-all-reduce: 64.63 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.65 | optimizer-unscale-and-check-inf: 19.65 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 101.65 | batch-generator: 8.84
--------------------------------------------------------------------------------------------------
 validation loss at iteration 191500 | lm loss value: 3.518333E-05 | lm loss PPL: 1.000035E+00 | 
 at iteration 191500, match long value: -0.003970454926248754 | match short value: -0.017076623104906086 
----------------------------------------------------------------------------------------------------------
 iteration   191600/  200000 | consumed samples:      6131200 | elapsed time per iteration (ms): 274.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.060859E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.82 | backward-params-all-reduce: 64.35 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.13 | optimizer-unscale-and-check-inf: 19.53 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.32 | optimizer: 100.31 | batch-generator: 18.63
 iteration   191700/  200000 | consumed samples:      6134400 | elapsed time per iteration (ms): 262.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.059956E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.26 | backward-params-all-reduce: 66.34 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.58 | optimizer-unscale-and-check-inf: 19.92 | optimizer-clip-main-grad: 7.72 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 101.92 | batch-generator: 9.20
 iteration   191800/  200000 | consumed samples:      6137600 | elapsed time per iteration (ms): 261.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.055339E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.21 | backward-params-all-reduce: 64.60 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.36 | optimizer-unscale-and-check-inf: 18.05 | optimizer-clip-main-grad: 7.70 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.79 | batch-generator: 9.12
 iteration   191900/  200000 | consumed samples:      6140800 | elapsed time per iteration (ms): 262.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.066042E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.97 | backward-params-all-reduce: 63.67 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.97 | optimizer-unscale-and-check-inf: 19.36 | optimizer-clip-main-grad: 7.79 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 101.78 | batch-generator: 9.23
 iteration   192000/  200000 | consumed samples:      6144000 | elapsed time per iteration (ms): 266.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.065390E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 54.19 | backward-params-all-reduce: 63.79 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.49 | optimizer-unscale-and-check-inf: 18.71 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 100.54 | batch-generator: 9.16
--------------------------------------------------------------------------------------------------
 validation loss at iteration 192000 | lm loss value: 3.101424E-05 | lm loss PPL: 1.000031E+00 | 
 at iteration 192000, match long value: -0.0012804453717100105 | match short value: -0.006734352958695616 
-----------------------------------------------------------------------------------------------------------
 iteration   192100/  200000 | consumed samples:      6147200 | elapsed time per iteration (ms): 276.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.057146E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.18 | backward-params-all-reduce: 65.50 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.42 | optimizer-unscale-and-check-inf: 18.50 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 100.16 | batch-generator: 19.48
 iteration   192200/  200000 | consumed samples:      6150400 | elapsed time per iteration (ms): 273.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.061220E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.31 | backward-params-all-reduce: 64.71 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.08 | optimizer-unscale-and-check-inf: 17.92 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 14.34 | optimizer: 101.08 | batch-generator: 8.86
 iteration   192300/  200000 | consumed samples:      6153600 | elapsed time per iteration (ms): 263.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.059694E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.46 | backward-params-all-reduce: 64.73 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.66 | optimizer-unscale-and-check-inf: 19.19 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 101.21 | batch-generator: 8.87
 iteration   192400/  200000 | consumed samples:      6156800 | elapsed time per iteration (ms): 261.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.064545E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.26 | backward-params-all-reduce: 63.62 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.97 | optimizer-unscale-and-check-inf: 17.55 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.45 | optimizer: 99.78 | batch-generator: 8.89
 iteration   192500/  200000 | consumed samples:      6160000 | elapsed time per iteration (ms): 261.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.066202E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.30 | backward-params-all-reduce: 64.50 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.21 | optimizer-unscale-and-check-inf: 20.23 | optimizer-clip-main-grad: 7.73 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 101.80 | batch-generator: 8.80
--------------------------------------------------------------------------------------------------
 validation loss at iteration 192500 | lm loss value: 3.300237E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 192500, match long value: -0.0065486402304899775 | match short value: 0.00781824507633128 
---------------------------------------------------------------------------------------------------------
 iteration   192600/  200000 | consumed samples:      6163200 | elapsed time per iteration (ms): 277.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.073767E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.70 | backward-params-all-reduce: 63.35 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.14 | optimizer-unscale-and-check-inf: 19.68 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.22 | optimizer: 99.82 | batch-generator: 21.79
 iteration   192700/  200000 | consumed samples:      6166400 | elapsed time per iteration (ms): 264.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.075907E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.99 | backward-params-all-reduce: 63.06 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.41 | optimizer-unscale-and-check-inf: 19.67 | optimizer-clip-main-grad: 7.73 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 101.51 | batch-generator: 8.94
 iteration   192800/  200000 | consumed samples:      6169600 | elapsed time per iteration (ms): 258.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.069244E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.75 | backward-params-all-reduce: 62.81 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.22 | optimizer-unscale-and-check-inf: 17.84 | optimizer-clip-main-grad: 7.72 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 99.50 | batch-generator: 8.90
 iteration   192900/  200000 | consumed samples:      6172800 | elapsed time per iteration (ms): 276.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.066824E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.07 | backward-params-all-reduce: 65.13 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.22 | optimizer-unscale-and-check-inf: 17.64 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 14.15 | optimizer: 100.27 | batch-generator: 8.78
 iteration   193000/  200000 | consumed samples:      6176000 | elapsed time per iteration (ms): 264.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.066125E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.41 | backward-params-all-reduce: 66.83 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.29 | optimizer-unscale-and-check-inf: 18.64 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 100.29 | batch-generator: 8.82
--------------------------------------------------------------------------------------------------
 validation loss at iteration 193000 | lm loss value: 3.345964E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 193000, match long value: -0.003700788840231632 | match short value: -0.014833886634863215 
----------------------------------------------------------------------------------------------------------
 iteration   193100/  200000 | consumed samples:      6179200 | elapsed time per iteration (ms): 275.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.085331E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.52 | backward-params-all-reduce: 62.62 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.18 | optimizer-unscale-and-check-inf: 18.42 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 99.94 | batch-generator: 17.59
 iteration   193200/  200000 | consumed samples:      6182400 | elapsed time per iteration (ms): 260.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.064040E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.48 | backward-params-all-reduce: 63.55 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.13 | optimizer-unscale-and-check-inf: 17.95 | optimizer-clip-main-grad: 7.70 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.44 | batch-generator: 8.91
 iteration   193300/  200000 | consumed samples:      6185600 | elapsed time per iteration (ms): 258.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.064624E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.30 | backward-params-all-reduce: 63.73 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 11.97 | optimizer-unscale-and-check-inf: 18.10 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 99.38 | batch-generator: 8.82
 iteration   193400/  200000 | consumed samples:      6188800 | elapsed time per iteration (ms): 260.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.063372E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.25 | backward-params-all-reduce: 63.85 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.10 | optimizer-unscale-and-check-inf: 19.24 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 100.59 | batch-generator: 8.68
 iteration   193500/  200000 | consumed samples:      6192000 | elapsed time per iteration (ms): 262.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.074514E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.40 | backward-params-all-reduce: 64.48 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.08 | optimizer-unscale-and-check-inf: 18.18 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.45 | optimizer: 99.50 | batch-generator: 8.65
--------------------------------------------------------------------------------------------------
 validation loss at iteration 193500 | lm loss value: 3.312493E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 193500, match long value: -0.0020810027916040147 | match short value: -0.03751400026896875 
----------------------------------------------------------------------------------------------------------
 iteration   193600/  200000 | consumed samples:      6195200 | elapsed time per iteration (ms): 276.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.077507E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.53 | backward-params-all-reduce: 63.36 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.20 | optimizer-unscale-and-check-inf: 17.84 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 14.26 | optimizer: 101.08 | batch-generator: 16.44
 iteration   193700/  200000 | consumed samples:      6198400 | elapsed time per iteration (ms): 370.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.076173E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.66 | backward-params-all-reduce: 63.37 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.28 | optimizer-unscale-and-check-inf: 17.60 | optimizer-clip-main-grad: 7.76 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 99.34 | batch-generator: 111.27
 iteration   193800/  200000 | consumed samples:      6201600 | elapsed time per iteration (ms): 265.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.072342E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.13 | backward-params-all-reduce: 64.97 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.27 | optimizer-unscale-and-check-inf: 18.31 | optimizer-clip-main-grad: 7.54 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.76 | batch-generator: 14.51
 iteration   193900/  200000 | consumed samples:      6204800 | elapsed time per iteration (ms): 260.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.058725E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.74 | backward-params-all-reduce: 66.38 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.69 | optimizer-unscale-and-check-inf: 17.35 | optimizer-clip-main-grad: 7.45 | optimizer-copy-main-to-model-params: 12.26 | optimizer: 97.92 | batch-generator: 8.67
 iteration   194000/  200000 | consumed samples:      6208000 | elapsed time per iteration (ms): 261.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.061878E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.05 | backward-params-all-reduce: 67.00 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.69 | optimizer-unscale-and-check-inf: 17.73 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.61 | batch-generator: 8.67
--------------------------------------------------------------------------------------------------
 validation loss at iteration 194000 | lm loss value: 3.408537E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 194000, match long value: -0.004372724245101536 | match short value: -0.0172338989819996 
--------------------------------------------------------------------------------------------------------
 iteration   194100/  200000 | consumed samples:      6211200 | elapsed time per iteration (ms): 278.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.088166E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 54.12 | backward-params-all-reduce: 65.34 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.88 | optimizer-unscale-and-check-inf: 15.75 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 97.94 | batch-generator: 21.12
 iteration   194200/  200000 | consumed samples:      6214400 | elapsed time per iteration (ms): 262.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.071853E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.33 | backward-params-all-reduce: 66.03 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.07 | optimizer-unscale-and-check-inf: 17.15 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.40 | batch-generator: 8.54
 iteration   194300/  200000 | consumed samples:      6217600 | elapsed time per iteration (ms): 260.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.083038E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.59 | backward-params-all-reduce: 65.59 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.14 | optimizer-unscale-and-check-inf: 16.66 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.01 | batch-generator: 8.54
 iteration   194400/  200000 | consumed samples:      6220800 | elapsed time per iteration (ms): 270.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.077301E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.10 | backward-params-all-reduce: 63.64 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.94 | optimizer-unscale-and-check-inf: 17.34 | optimizer-clip-main-grad: 7.54 | optimizer-copy-main-to-model-params: 14.27 | optimizer: 101.23 | batch-generator: 8.64
 iteration   194500/  200000 | consumed samples:      6224000 | elapsed time per iteration (ms): 257.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.066459E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.79 | backward-params-all-reduce: 63.88 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.96 | optimizer-unscale-and-check-inf: 18.82 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 101.07 | batch-generator: 8.66
--------------------------------------------------------------------------------------------------
 validation loss at iteration 194500 | lm loss value: 3.357639E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 194500, match long value: -0.006674786585048716 | match short value: -0.04332924406131531 
---------------------------------------------------------------------------------------------------------
 iteration   194600/  200000 | consumed samples:      6227200 | elapsed time per iteration (ms): 271.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.092149E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.01 | backward-params-all-reduce: 64.68 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.85 | optimizer-unscale-and-check-inf: 17.36 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 99.45 | batch-generator: 13.92
 iteration   194700/  200000 | consumed samples:      6230400 | elapsed time per iteration (ms): 260.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.073001E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.69 | backward-params-all-reduce: 65.51 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.97 | optimizer-unscale-and-check-inf: 17.49 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.67 | batch-generator: 8.63
 iteration   194800/  200000 | consumed samples:      6233600 | elapsed time per iteration (ms): 259.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.091671E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.20 | backward-params-all-reduce: 65.63 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.86 | optimizer-unscale-and-check-inf: 16.76 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.96 | batch-generator: 8.78
 iteration   194900/  200000 | consumed samples:      6236800 | elapsed time per iteration (ms): 259.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.066152E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.10 | backward-params-all-reduce: 65.05 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.09 | optimizer-unscale-and-check-inf: 16.99 | optimizer-clip-main-grad: 7.50 | optimizer-copy-main-to-model-params: 12.24 | optimizer: 98.01 | batch-generator: 8.66
 iteration   195000/  200000 | consumed samples:      6240000 | elapsed time per iteration (ms): 264.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.096007E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.64 | backward-params-all-reduce: 65.95 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.73 | optimizer-unscale-and-check-inf: 17.55 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 99.53 | batch-generator: 8.69
--------------------------------------------------------------------------------------------------
 validation loss at iteration 195000 | lm loss value: 3.293788E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 195000, match long value: -0.0007592644204464749 | match short value: 0.000677060163185682 
----------------------------------------------------------------------------------------------------------
 iteration   195100/  200000 | consumed samples:      6243200 | elapsed time per iteration (ms): 284.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.099280E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.43 | backward-params-all-reduce: 64.33 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.68 | optimizer-unscale-and-check-inf: 16.81 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 14.29 | optimizer: 100.56 | batch-generator: 16.02
 iteration   195200/  200000 | consumed samples:      6246400 | elapsed time per iteration (ms): 258.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.107294E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.05 | backward-params-all-reduce: 63.48 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.41 | optimizer-unscale-and-check-inf: 17.70 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 100.37 | batch-generator: 8.74
 iteration   195300/  200000 | consumed samples:      6249600 | elapsed time per iteration (ms): 258.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.146382E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.08 | backward-params-all-reduce: 63.58 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.39 | optimizer-unscale-and-check-inf: 17.89 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 100.50 | batch-generator: 8.55
 iteration   195400/  200000 | consumed samples:      6252800 | elapsed time per iteration (ms): 260.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.116841E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.55 | backward-params-all-reduce: 63.68 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.95 | optimizer-unscale-and-check-inf: 17.18 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 99.43 | batch-generator: 8.68
 iteration   195500/  200000 | consumed samples:      6256000 | elapsed time per iteration (ms): 259.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.104158E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.11 | backward-params-all-reduce: 64.97 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.24 | optimizer-unscale-and-check-inf: 17.55 | optimizer-clip-main-grad: 7.53 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.93 | batch-generator: 8.61
--------------------------------------------------------------------------------------------------
 validation loss at iteration 195500 | lm loss value: 3.373112E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 195500, match long value: -0.0014952199577302771 | match short value: -0.009378011453196037 
-----------------------------------------------------------------------------------------------------------
 iteration   195600/  200000 | consumed samples:      6259200 | elapsed time per iteration (ms): 274.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.120915E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.33 | backward-params-all-reduce: 64.40 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.14 | optimizer-unscale-and-check-inf: 17.21 | optimizer-clip-main-grad: 7.58 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 99.56 | batch-generator: 19.14
 iteration   195700/  200000 | consumed samples:      6262400 | elapsed time per iteration (ms): 263.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.112662E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.06 | backward-params-all-reduce: 63.95 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.43 | optimizer-unscale-and-check-inf: 17.40 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 99.09 | batch-generator: 8.66
 iteration   195800/  200000 | consumed samples:      6265600 | elapsed time per iteration (ms): 266.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.114666E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.57 | backward-params-all-reduce: 63.83 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.06 | optimizer-unscale-and-check-inf: 18.18 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 14.25 | optimizer: 101.29 | batch-generator: 8.71
 iteration   195900/  200000 | consumed samples:      6268800 | elapsed time per iteration (ms): 265.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.115173E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.33 | backward-params-all-reduce: 62.97 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.13 | optimizer-unscale-and-check-inf: 17.13 | optimizer-clip-main-grad: 7.52 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 98.39 | batch-generator: 8.73
 iteration   196000/  200000 | consumed samples:      6272000 | elapsed time per iteration (ms): 258.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.102067E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.33 | backward-params-all-reduce: 63.28 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.18 | optimizer-unscale-and-check-inf: 17.03 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 98.38 | batch-generator: 8.74
--------------------------------------------------------------------------------------------------
 validation loss at iteration 196000 | lm loss value: 3.261126E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 196000, match long value: -0.0006330191455293867 | match short value: -0.020978688227095487 
-----------------------------------------------------------------------------------------------------------
 iteration   196100/  200000 | consumed samples:      6275200 | elapsed time per iteration (ms): 270.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.108669E-05 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.98 | backward-params-all-reduce: 63.45 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.20 | optimizer-unscale-and-check-inf: 17.23 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 98.76 | batch-generator: 15.75
 iteration   196200/  200000 | consumed samples:      6278400 | elapsed time per iteration (ms): 255.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.118064E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   4 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.86 | backward-params-all-reduce: 62.98 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.11 | optimizer-unscale-and-check-inf: 18.19 | optimizer-clip-main-grad: 7.27 | optimizer-copy-main-to-model-params: 11.99 | optimizer: 96.73 | batch-generator: 8.75
 iteration   196300/  200000 | consumed samples:      6281600 | elapsed time per iteration (ms): 260.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.106856E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.65 | backward-params-all-reduce: 64.89 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.14 | optimizer-unscale-and-check-inf: 17.21 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 98.66 | batch-generator: 8.95
 iteration   196400/  200000 | consumed samples:      6284800 | elapsed time per iteration (ms): 262.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.122904E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.58 | backward-params-all-reduce: 64.25 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.29 | optimizer-unscale-and-check-inf: 16.86 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 98.55 | batch-generator: 8.65
 iteration   196500/  200000 | consumed samples:      6288000 | elapsed time per iteration (ms): 260.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.138755E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.09 | backward-params-all-reduce: 63.74 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.16 | optimizer-unscale-and-check-inf: 17.17 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.54 | optimizer: 98.57 | batch-generator: 8.80
--------------------------------------------------------------------------------------------------
 validation loss at iteration 196500 | lm loss value: 3.274858E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 196500, match long value: -0.0011063367528537644 | match short value: -0.007862161102203204 
-----------------------------------------------------------------------------------------------------------
 iteration   196600/  200000 | consumed samples:      6291200 | elapsed time per iteration (ms): 284.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.132904E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.75 | backward-params-all-reduce: 63.73 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.04 | optimizer-unscale-and-check-inf: 16.28 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 14.22 | optimizer: 99.29 | batch-generator: 15.91
 iteration   196700/  200000 | consumed samples:      6294400 | elapsed time per iteration (ms): 260.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.126571E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.09 | backward-params-all-reduce: 64.49 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.02 | optimizer-unscale-and-check-inf: 18.32 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.61 | batch-generator: 8.77
 iteration   196800/  200000 | consumed samples:      6297600 | elapsed time per iteration (ms): 259.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.135384E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.35 | backward-params-all-reduce: 63.26 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.09 | optimizer-unscale-and-check-inf: 17.29 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 98.68 | batch-generator: 8.73
 iteration   196900/  200000 | consumed samples:      6300800 | elapsed time per iteration (ms): 264.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.122628E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.20 | backward-params-all-reduce: 64.03 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.05 | optimizer-unscale-and-check-inf: 17.42 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 98.73 | batch-generator: 14.34
 iteration   197000/  200000 | consumed samples:      6304000 | elapsed time per iteration (ms): 260.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.093673E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.29 | backward-params-all-reduce: 64.40 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.45 | optimizer-unscale-and-check-inf: 18.54 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 12.54 | optimizer: 100.38 | batch-generator: 8.70
--------------------------------------------------------------------------------------------------
 validation loss at iteration 197000 | lm loss value: 3.306331E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 197000, match long value: -0.0042119618914712945 | match short value: -0.022587281575797143 
-----------------------------------------------------------------------------------------------------------
 iteration   197100/  200000 | consumed samples:      6307200 | elapsed time per iteration (ms): 279.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.099152E-05 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.87 | backward-params-all-reduce: 65.22 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.71 | optimizer-unscale-and-check-inf: 17.33 | optimizer-clip-main-grad: 7.68 | optimizer-copy-main-to-model-params: 12.54 | optimizer: 99.54 | batch-generator: 20.47
 iteration   197200/  200000 | consumed samples:      6310400 | elapsed time per iteration (ms): 259.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.110427E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.37 | backward-params-all-reduce: 63.69 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.98 | optimizer-unscale-and-check-inf: 18.00 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.55 | optimizer: 100.38 | batch-generator: 8.79
 iteration   197300/  200000 | consumed samples:      6313600 | elapsed time per iteration (ms): 273.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.110932E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.31 | backward-params-all-reduce: 64.13 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.85 | optimizer-unscale-and-check-inf: 15.91 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 14.22 | optimizer: 99.73 | batch-generator: 8.75
 iteration   197400/  200000 | consumed samples:      6316800 | elapsed time per iteration (ms): 261.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.108469E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.33 | backward-params-all-reduce: 63.62 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.37 | optimizer-unscale-and-check-inf: 17.87 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 12.52 | optimizer: 99.59 | batch-generator: 8.89
 iteration   197500/  200000 | consumed samples:      6320000 | elapsed time per iteration (ms): 261.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.098453E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.31 | backward-params-all-reduce: 64.95 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.27 | optimizer-unscale-and-check-inf: 17.76 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.33 | batch-generator: 8.68
--------------------------------------------------------------------------------------------------
 validation loss at iteration 197500 | lm loss value: 3.406813E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 197500, match long value: -0.002736735518960767 | match short value: -0.02659263005858731 
---------------------------------------------------------------------------------------------------------
 iteration   197600/  200000 | consumed samples:      6323200 | elapsed time per iteration (ms): 272.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.103719E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.43 | backward-params-all-reduce: 64.16 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 13.00 | optimizer-unscale-and-check-inf: 17.10 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.53 | optimizer: 99.39 | batch-generator: 15.96
 iteration   197700/  200000 | consumed samples:      6326400 | elapsed time per iteration (ms): 261.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.112857E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.50 | backward-params-all-reduce: 63.72 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.39 | optimizer-unscale-and-check-inf: 19.08 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 100.72 | batch-generator: 8.73
 iteration   197800/  200000 | consumed samples:      6329600 | elapsed time per iteration (ms): 261.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.125136E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.65 | backward-params-all-reduce: 63.36 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.32 | optimizer-unscale-and-check-inf: 18.72 | optimizer-clip-main-grad: 7.62 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 100.31 | batch-generator: 8.64
 iteration   197900/  200000 | consumed samples:      6332800 | elapsed time per iteration (ms): 264.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.120184E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.37 | backward-params-all-reduce: 63.92 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.11 | optimizer-unscale-and-check-inf: 18.56 | optimizer-clip-main-grad: 7.56 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 99.81 | batch-generator: 8.64
 iteration   198000/  200000 | consumed samples:      6336000 | elapsed time per iteration (ms): 267.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.109672E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.70 | backward-params-all-reduce: 63.72 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.25 | optimizer-unscale-and-check-inf: 18.70 | optimizer-clip-main-grad: 7.64 | optimizer-copy-main-to-model-params: 14.31 | optimizer: 102.06 | batch-generator: 8.67
--------------------------------------------------------------------------------------------------
 validation loss at iteration 198000 | lm loss value: 3.297870E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 198000, match long value: -0.00218852126626987 | match short value: -0.00748276849777969 
--------------------------------------------------------------------------------------------------------
 iteration   198100/  200000 | consumed samples:      6339200 | elapsed time per iteration (ms): 277.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.127102E-05 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.06 | backward-params-all-reduce: 64.45 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.21 | optimizer-unscale-and-check-inf: 18.00 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.46 | optimizer: 99.40 | batch-generator: 16.46
 iteration   198200/  200000 | consumed samples:      6342400 | elapsed time per iteration (ms): 259.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.128623E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.77 | backward-params-all-reduce: 63.41 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.14 | optimizer-unscale-and-check-inf: 19.33 | optimizer-clip-main-grad: 7.63 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 100.80 | batch-generator: 8.69
 iteration   198300/  200000 | consumed samples:      6345600 | elapsed time per iteration (ms): 258.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.114149E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.62 | backward-params-all-reduce: 64.26 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.04 | optimizer-unscale-and-check-inf: 18.38 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.37 | optimizer: 99.00 | batch-generator: 8.73
 iteration   198400/  200000 | consumed samples:      6348800 | elapsed time per iteration (ms): 258.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.124620E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.89 | backward-params-all-reduce: 62.46 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.25 | optimizer-unscale-and-check-inf: 17.99 | optimizer-clip-main-grad: 7.66 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.57 | batch-generator: 8.89
 iteration   198500/  200000 | consumed samples:      6352000 | elapsed time per iteration (ms): 259.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.120695E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.47 | backward-params-all-reduce: 62.40 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.05 | optimizer-unscale-and-check-inf: 20.00 | optimizer-clip-main-grad: 7.77 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 101.49 | batch-generator: 8.96
--------------------------------------------------------------------------------------------------
 validation loss at iteration 198500 | lm loss value: 3.360076E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 198500, match long value: -0.004381689433819595 | match short value: -0.030634260505784543 
----------------------------------------------------------------------------------------------------------
 iteration   198600/  200000 | consumed samples:      6355200 | elapsed time per iteration (ms): 281.4 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.121334E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.26 | backward-params-all-reduce: 64.90 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.33 | optimizer-unscale-and-check-inf: 18.88 | optimizer-clip-main-grad: 7.71 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 100.58 | batch-generator: 21.17
 iteration   198700/  200000 | consumed samples:      6358400 | elapsed time per iteration (ms): 259.9 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.118880E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.26 | backward-params-all-reduce: 62.93 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.41 | optimizer-unscale-and-check-inf: 18.95 | optimizer-clip-main-grad: 7.70 | optimizer-copy-main-to-model-params: 12.49 | optimizer: 100.74 | batch-generator: 8.77
 iteration   198800/  200000 | consumed samples:      6361600 | elapsed time per iteration (ms): 272.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.112002E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.26 | backward-params-all-reduce: 62.72 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.00 | optimizer-unscale-and-check-inf: 19.30 | optimizer-clip-main-grad: 7.73 | optimizer-copy-main-to-model-params: 14.23 | optimizer: 102.43 | batch-generator: 8.86
 iteration   198900/  200000 | consumed samples:      6364800 | elapsed time per iteration (ms): 262.0 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.116865E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.53 | backward-params-all-reduce: 63.36 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 14.17 | optimizer-unscale-and-check-inf: 16.95 | optimizer-clip-main-grad: 7.91 | optimizer-copy-main-to-model-params: 12.54 | optimizer: 100.76 | batch-generator: 9.00
 iteration   199000/  200000 | consumed samples:      6368000 | elapsed time per iteration (ms): 259.8 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.126639E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 54.01 | backward-params-all-reduce: 62.65 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.22 | optimizer-unscale-and-check-inf: 17.85 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 99.39 | batch-generator: 8.81
--------------------------------------------------------------------------------------------------
 validation loss at iteration 199000 | lm loss value: 3.312993E-05 | lm loss PPL: 1.000033E+00 | 
 at iteration 199000, match long value: -0.0018374760680082546 | match short value: -0.008452097162245315 
-----------------------------------------------------------------------------------------------------------
 iteration   199100/  200000 | consumed samples:      6371200 | elapsed time per iteration (ms): 274.6 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.130989E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.49 | backward-params-all-reduce: 64.84 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.45 | optimizer-unscale-and-check-inf: 18.20 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 99.95 | batch-generator: 16.91
 iteration   199200/  200000 | consumed samples:      6374400 | elapsed time per iteration (ms): 261.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.117202E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 54.16 | backward-params-all-reduce: 62.68 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.49 | optimizer-unscale-and-check-inf: 17.48 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.47 | optimizer: 99.26 | batch-generator: 9.05
 iteration   199300/  200000 | consumed samples:      6377600 | elapsed time per iteration (ms): 262.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.137324E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.21 | backward-params-all-reduce: 64.50 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.12 | optimizer-unscale-and-check-inf: 19.67 | optimizer-clip-main-grad: 7.57 | optimizer-copy-main-to-model-params: 12.21 | optimizer: 99.77 | batch-generator: 9.10
 iteration   199400/  200000 | consumed samples:      6380800 | elapsed time per iteration (ms): 256.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.119210E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.30 | backward-params-all-reduce: 62.73 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 11.99 | optimizer-unscale-and-check-inf: 17.24 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 12.44 | optimizer: 98.47 | batch-generator: 8.96
 iteration   199500/  200000 | consumed samples:      6384000 | elapsed time per iteration (ms): 275.3 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.133383E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.15 | backward-params-all-reduce: 65.26 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.14 | optimizer-unscale-and-check-inf: 18.21 | optimizer-clip-main-grad: 7.67 | optimizer-copy-main-to-model-params: 14.22 | optimizer: 101.41 | batch-generator: 8.93
--------------------------------------------------------------------------------------------------
 validation loss at iteration 199500 | lm loss value: 3.437575E-05 | lm loss PPL: 1.000034E+00 | 
 at iteration 199500, match long value: -0.003669915023508841 | match short value: -0.018229454050292738 
----------------------------------------------------------------------------------------------------------
 iteration   199600/  200000 | consumed samples:      6387200 | elapsed time per iteration (ms): 273.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.122442E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.79 | backward-params-all-reduce: 64.17 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 11.96 | optimizer-unscale-and-check-inf: 17.85 | optimizer-clip-main-grad: 7.70 | optimizer-copy-main-to-model-params: 12.45 | optimizer: 99.14 | batch-generator: 15.75
 iteration   199700/  200000 | consumed samples:      6390400 | elapsed time per iteration (ms): 262.1 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.117481E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.12 | backward-params-all-reduce: 65.53 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.23 | optimizer-unscale-and-check-inf: 19.56 | optimizer-clip-main-grad: 7.72 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 101.19 | batch-generator: 9.11
 iteration   199800/  200000 | consumed samples:      6393600 | elapsed time per iteration (ms): 260.2 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.136209E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.24 | backward-params-all-reduce: 63.31 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.04 | optimizer-unscale-and-check-inf: 18.40 | optimizer-clip-main-grad: 7.61 | optimizer-copy-main-to-model-params: 12.50 | optimizer: 99.70 | batch-generator: 8.98
 iteration   199900/  200000 | consumed samples:      6396800 | elapsed time per iteration (ms): 259.5 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.131991E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.29 | backward-params-all-reduce: 63.55 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.09 | optimizer-unscale-and-check-inf: 16.64 | optimizer-clip-main-grad: 7.59 | optimizer-copy-main-to-model-params: 12.45 | optimizer: 97.88 | batch-generator: 8.74
 iteration   200000/  200000 | consumed samples:      6400000 | elapsed time per iteration (ms): 258.7 | learning rate: 1.000E-05 | global batch size:    32 | lm loss: 1.126943E-05 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.32 | backward-params-all-reduce: 62.54 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 12.36 | optimizer-unscale-and-check-inf: 16.47 | optimizer-clip-main-grad: 7.60 | optimizer-copy-main-to-model-params: 12.48 | optimizer: 98.05 | batch-generator: 8.70
--------------------------------------------------------------------------------------------------
 validation loss at iteration 200000 | lm loss value: 3.131166E-05 | lm loss PPL: 1.000031E+00 | 
 at iteration 200000, match long value: -0.002548350195800607 | match short value: -0.012320390867300832 
----------------------------------------------------------------------------------------------------------
saving checkpoint at iteration  200000 to verify1
  successfully saved checkpoint at iteration  200000 to verify1
time (ms) | save-checkpoint: 24039.63
[after training is done] datetime: 2021-12-24 08:51:37 
saving checkpoint at iteration  200000 to verify1
------------------------------------------------------------------------------------------------------------------
 validation loss at the end of training for val data | lm loss value: 3.238952E-05 | lm loss PPL: 1.000032E+00 | 
  successfully saved checkpoint at iteration  200000 to verify1
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:177: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torch.distributed.run.
Note that --use_env is set by default in torch.distributed.run.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
