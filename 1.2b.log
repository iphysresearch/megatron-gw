using world size: 8, data-parallel-size: 8, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.999
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ False
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  data_impl ....................................... mmap
  data_parallel_size .............................. 8
  data_path ....................................... ['../bigdata/']
  dataloader_type ................................. cyclic
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  embedding_path .................................. None
  encoder_seq_length .............................. 127
  eod_mask_loss ................................... False
  eval_interval ................................... 500
  eval_iters ...................................... 10
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  ffn_hidden_size ................................. 8192
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 32
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 2048
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  init_method_std ................................. 0.02
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  kv_channels ..................................... 64
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ verify1
  local_rank ...................................... 0
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 100
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0001
  lr_decay_iters .................................. 99000
  lr_decay_samples ................................ None
  lr_decay_style .................................. linear
  lr_warmup_fraction .............................. 0.002
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 127
  merge_file ...................................... None
  micro_batch_size ................................ 4
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mmap_warmup ..................................... False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 32
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_layers ...................................... 24
  num_layers_per_virtual_pipeline_stage ........... None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  sample_rate ..................................... 1.0
  save ............................................ verify1
  save_interval ................................... 50000
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  segment_length .................................. 2048
  seq_length ...................................... 127
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 949,50,1
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. None
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  titles_data_path ................................ None
  tokenizer_type .................................. BertWordPieceLowerCase
  train_iters ..................................... 200000
  train_samples ................................... None
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... ./bert_vocab_files/bert-base-uncased-vocab.txt
  weight_decay .................................... 0.01
  world_size ...................................... 8
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 1
> building BertWordPieceLowerCase tokenizer ...
 > padded vocab (size: 30524) with 68 dummy tokens (new size: 30592)
> initializing torch distributed ...
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /workspace/zhouy/megatron-gw/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /workspace/zhouy/megatron-gw/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /workspace/zhouy/megatron-gw/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_mix_prec_layer_norm_cuda...
>>> done with compiling and loading fused kernels. Compilation time: 7.453 seconds
time to initialize megatron (seconds): 27.328
[after megatron is initialized] datetime: 2021-12-23 13:03:23 
building BERT model ...
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1216997376
> learning rate decay style: linear
WARNING: could not find the metadata file verify1/latest_checkpointed_iteration.txt 
    will not load any checkpoints and will start from random
time (ms) | load-checkpoint: 0.53
[after model, optimizer, and learning rate scheduler are built] datetime: 2021-12-23 13:03:24 
> building train, validation, and test datasets ...
> building train, validation, and test datasets for BERT ...
> finished creating BERT datasets ...
[after dataloaders are built] datetime: 2021-12-23 13:03:30 time (ms) | model-and-optimizer-setup: 837.92 | train/valid/test-data-iterators-setup: 6100.09

done with setup ...
training ...
[before the start of training step] datetime: 2021-12-23 13:03:30 
 iteration      100/  200000 | consumed samples:         3200 | elapsed time per iteration (ms): 303.6 | learning rate: 4.444E-05 | global batch size:    32 | lm loss: 9.706122E-02 | loss scale: 2097152.0 | grad norm: 0.024 | number of skipped iterations:  12 | number of nan iterations:   0 |
time (ms) | backward-compute: 56.97 | backward-params-all-reduce: 66.16 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 14.00 | optimizer-unscale-and-check-inf: 19.24 | optimizer-clip-main-grad: 8.19 | optimizer-copy-main-to-model-params: 10.71 | optimizer: 99.68 | batch-generator: 13.67
[Rank 0] (after 100 iterations) memory (MB) | allocated: 23212.96435546875 | max allocated: 23214.6025390625 | reserved: 26164.0 | max reserved: 26164.0
 iteration      200/  200000 | consumed samples:         6400 | elapsed time per iteration (ms): 264.3 | learning rate: 9.495E-05 | global batch size:    32 | lm loss: 2.122366E-03 | loss scale: 2097152.0 | grad norm: 0.021 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.77 | backward-params-all-reduce: 65.66 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.94 | optimizer-unscale-and-check-inf: 14.99 | optimizer-clip-main-grad: 7.26 | optimizer-copy-main-to-model-params: 13.71 | optimizer: 95.87 | batch-generator: 7.89
 iteration      300/  200000 | consumed samples:         9600 | elapsed time per iteration (ms): 261.4 | learning rate: 9.992E-05 | global batch size:    32 | lm loss: 1.275565E-03 | loss scale: 2097152.0 | grad norm: 0.012 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.58 | backward-params-all-reduce: 67.07 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.02 | optimizer-unscale-and-check-inf: 19.65 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 99.14 | batch-generator: 7.96
 iteration      400/  200000 | consumed samples:        12800 | elapsed time per iteration (ms): 262.2 | learning rate: 9.983E-05 | global batch size:    32 | lm loss: 1.008312E-03 | loss scale: 2097152.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.86 | backward-params-all-reduce: 65.04 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.91 | optimizer-unscale-and-check-inf: 16.60 | optimizer-clip-main-grad: 7.27 | optimizer-copy-main-to-model-params: 12.14 | optimizer: 95.88 | batch-generator: 7.89
 iteration      500/  200000 | consumed samples:        16000 | elapsed time per iteration (ms): 259.3 | learning rate: 9.974E-05 | global batch size:    32 | lm loss: 9.232771E-04 | loss scale: 2097152.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.70 | backward-params-all-reduce: 64.83 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.88 | optimizer-unscale-and-check-inf: 21.64 | optimizer-clip-main-grad: 7.40 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 101.03 | batch-generator: 8.13
-----------------------------------------------------------------------------------------------
 validation loss at iteration 500 | lm loss value: 6.948888E-04 | lm loss PPL: 1.000695E+00 | 
 at iteration 500, match long value: 0.0011605112495240327 | match short value: 0.019676115992471748 
------------------------------------------------------------------------------------------------------
 iteration      600/  200000 | consumed samples:        19200 | elapsed time per iteration (ms): 266.6 | learning rate: 9.964E-05 | global batch size:    32 | lm loss: 6.531002E-04 | loss scale: 2097152.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.77 | backward-params-all-reduce: 61.88 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.96 | optimizer-unscale-and-check-inf: 17.76 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.16 | batch-generator: 17.53
 iteration      700/  200000 | consumed samples:        22400 | elapsed time per iteration (ms): 253.1 | learning rate: 9.955E-05 | global batch size:    32 | lm loss: 6.531756E-04 | loss scale: 2097152.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.34 | backward-params-all-reduce: 62.79 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.21 | optimizer-unscale-and-check-inf: 13.81 | optimizer-clip-main-grad: 7.22 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 93.37 | batch-generator: 7.78
 iteration      800/  200000 | consumed samples:        25600 | elapsed time per iteration (ms): 253.3 | learning rate: 9.946E-05 | global batch size:    32 | lm loss: 6.483161E-04 | loss scale: 2097152.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.71 | backward-params-all-reduce: 61.63 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.00 | optimizer-unscale-and-check-inf: 18.63 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.13 | batch-generator: 8.03
 iteration      900/  200000 | consumed samples:        28800 | elapsed time per iteration (ms): 256.7 | learning rate: 9.937E-05 | global batch size:    32 | lm loss: 6.528109E-04 | loss scale: 2097152.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.90 | backward-params-all-reduce: 63.08 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.93 | optimizer-unscale-and-check-inf: 16.92 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 13.72 | optimizer: 97.85 | batch-generator: 8.08
 iteration     1000/  200000 | consumed samples:        32000 | elapsed time per iteration (ms): 254.2 | learning rate: 9.928E-05 | global batch size:    32 | lm loss: 6.433538E-04 | loss scale: 2097152.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.77 | backward-params-all-reduce: 63.25 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.15 | optimizer-unscale-and-check-inf: 16.92 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 96.58 | batch-generator: 8.10
------------------------------------------------------------------------------------------------
 validation loss at iteration 1000 | lm loss value: 6.471841E-04 | lm loss PPL: 1.000647E+00 | 
 at iteration 1000, match long value: 0.001305984535178022 | match short value: 0.02493577873348988 
-----------------------------------------------------------------------------------------------------
 iteration     1100/  200000 | consumed samples:        35200 | elapsed time per iteration (ms): 266.7 | learning rate: 9.919E-05 | global batch size:    32 | lm loss: 6.440675E-04 | loss scale: 4194304.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.75 | backward-params-all-reduce: 63.53 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.99 | optimizer-unscale-and-check-inf: 19.13 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.62 | batch-generator: 14.58
 iteration     1200/  200000 | consumed samples:        38400 | elapsed time per iteration (ms): 254.1 | learning rate: 9.910E-05 | global batch size:    32 | lm loss: 6.573156E-04 | loss scale: 4194304.0 | grad norm: 0.012 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.71 | backward-params-all-reduce: 64.09 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 16.83 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 96.15 | batch-generator: 8.01
 iteration     1300/  200000 | consumed samples:        41600 | elapsed time per iteration (ms): 254.4 | learning rate: 9.901E-05 | global batch size:    32 | lm loss: 6.447437E-04 | loss scale: 4194304.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.81 | backward-params-all-reduce: 63.03 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.92 | optimizer-unscale-and-check-inf: 18.98 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 98.36 | batch-generator: 8.00
 iteration     1400/  200000 | consumed samples:        44800 | elapsed time per iteration (ms): 253.7 | learning rate: 9.892E-05 | global batch size:    32 | lm loss: 6.527977E-04 | loss scale: 4194304.0 | grad norm: 0.019 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.72 | backward-params-all-reduce: 62.44 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 18.53 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 97.83 | batch-generator: 8.06
 iteration     1500/  200000 | consumed samples:        48000 | elapsed time per iteration (ms): 254.7 | learning rate: 9.882E-05 | global batch size:    32 | lm loss: 6.486205E-04 | loss scale: 4194304.0 | grad norm: 0.011 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.81 | backward-params-all-reduce: 62.66 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 13.61 | optimizer-unscale-and-check-inf: 16.28 | optimizer-clip-main-grad: 7.52 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.58 | batch-generator: 7.95
------------------------------------------------------------------------------------------------
 validation loss at iteration 1500 | lm loss value: 6.502272E-04 | lm loss PPL: 1.000650E+00 | 
 at iteration 1500, match long value: 0.0013199325367931013 | match short value: 0.017527291528781255 
-------------------------------------------------------------------------------------------------------
 iteration     1600/  200000 | consumed samples:        51200 | elapsed time per iteration (ms): 264.6 | learning rate: 9.873E-05 | global batch size:    32 | lm loss: 6.633323E-04 | loss scale: 4194304.0 | grad norm: 0.012 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.08 | backward-params-all-reduce: 62.47 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.01 | optimizer-unscale-and-check-inf: 18.82 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 98.25 | batch-generator: 14.37
 iteration     1700/  200000 | consumed samples:        54400 | elapsed time per iteration (ms): 258.4 | learning rate: 9.864E-05 | global batch size:    32 | lm loss: 6.533711E-04 | loss scale: 4194304.0 | grad norm: 0.011 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.05 | backward-params-all-reduce: 64.38 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.11 | optimizer-unscale-and-check-inf: 16.87 | optimizer-clip-main-grad: 7.26 | optimizer-copy-main-to-model-params: 13.71 | optimizer: 97.95 | batch-generator: 8.08
 iteration     1800/  200000 | consumed samples:        57600 | elapsed time per iteration (ms): 253.8 | learning rate: 9.855E-05 | global batch size:    32 | lm loss: 6.604908E-04 | loss scale: 4194304.0 | grad norm: 0.015 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.87 | backward-params-all-reduce: 62.72 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.97 | optimizer-unscale-and-check-inf: 18.23 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.14 | optimizer: 97.64 | batch-generator: 8.01
 iteration     1900/  200000 | consumed samples:        60800 | elapsed time per iteration (ms): 259.0 | learning rate: 9.846E-05 | global batch size:    32 | lm loss: 6.657952E-04 | loss scale: 4194304.0 | grad norm: 0.012 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.96 | backward-params-all-reduce: 64.23 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.38 | optimizer-unscale-and-check-inf: 18.84 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 98.67 | batch-generator: 7.99
 iteration     2000/  200000 | consumed samples:        64000 | elapsed time per iteration (ms): 260.7 | learning rate: 9.837E-05 | global batch size:    32 | lm loss: 6.632921E-04 | loss scale: 4194304.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.06 | backward-params-all-reduce: 64.88 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.92 | optimizer-unscale-and-check-inf: 20.00 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 99.41 | batch-generator: 8.01
------------------------------------------------------------------------------------------------
 validation loss at iteration 2000 | lm loss value: 6.489762E-04 | lm loss PPL: 1.000649E+00 | 
 at iteration 2000, match long value: 4.677141232891406e-05 | match short value: -0.0022298297418027204 
---------------------------------------------------------------------------------------------------------
 iteration     2100/  200000 | consumed samples:        67200 | elapsed time per iteration (ms): 274.5 | learning rate: 9.828E-05 | global batch size:    32 | lm loss: 6.739845E-04 | loss scale: 8388608.0 | grad norm: 0.016 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.98 | backward-params-all-reduce: 63.89 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.01 | optimizer-unscale-and-check-inf: 20.13 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.14 | optimizer: 99.62 | batch-generator: 17.76
 iteration     2200/  200000 | consumed samples:        70400 | elapsed time per iteration (ms): 258.3 | learning rate: 9.819E-05 | global batch size:    32 | lm loss: 6.662659E-04 | loss scale: 8388608.0 | grad norm: 0.014 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.84 | backward-params-all-reduce: 65.42 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.90 | optimizer-unscale-and-check-inf: 19.05 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.43 | batch-generator: 8.03
 iteration     2300/  200000 | consumed samples:        73600 | elapsed time per iteration (ms): 257.1 | learning rate: 9.810E-05 | global batch size:    32 | lm loss: 6.718781E-04 | loss scale: 8388608.0 | grad norm: 0.011 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.74 | backward-params-all-reduce: 64.06 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.91 | optimizer-unscale-and-check-inf: 19.22 | optimizer-clip-main-grad: 7.27 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 98.54 | batch-generator: 8.08
 iteration     2400/  200000 | consumed samples:        76800 | elapsed time per iteration (ms): 259.0 | learning rate: 9.801E-05 | global batch size:    32 | lm loss: 6.627337E-04 | loss scale: 8388608.0 | grad norm: 0.022 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.26 | backward-params-all-reduce: 63.07 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.92 | optimizer-unscale-and-check-inf: 18.53 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 13.72 | optimizer: 99.47 | batch-generator: 8.07
 iteration     2500/  200000 | consumed samples:        80000 | elapsed time per iteration (ms): 256.1 | learning rate: 9.791E-05 | global batch size:    32 | lm loss: 6.646355E-04 | loss scale: 8388608.0 | grad norm: 0.012 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.12 | backward-params-all-reduce: 63.93 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.03 | optimizer-unscale-and-check-inf: 18.70 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 98.22 | batch-generator: 8.12
------------------------------------------------------------------------------------------------
 validation loss at iteration 2500 | lm loss value: 6.756119E-04 | lm loss PPL: 1.000676E+00 | 
 at iteration 2500, match long value: 0.0009392717878881689 | match short value: 0.009333466422590043 
-------------------------------------------------------------------------------------------------------
 iteration     2600/  200000 | consumed samples:        83200 | elapsed time per iteration (ms): 269.5 | learning rate: 9.782E-05 | global batch size:    32 | lm loss: 6.516803E-04 | loss scale: 8388608.0 | grad norm: 0.020 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.03 | backward-params-all-reduce: 63.52 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.57 | optimizer-unscale-and-check-inf: 18.51 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.14 | optimizer: 98.52 | batch-generator: 12.17
 iteration     2700/  200000 | consumed samples:        86400 | elapsed time per iteration (ms): 261.2 | learning rate: 9.773E-05 | global batch size:    32 | lm loss: 6.710421E-04 | loss scale: 8388608.0 | grad norm: 0.012 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.20 | backward-params-all-reduce: 63.29 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.14 | optimizer-unscale-and-check-inf: 20.33 | optimizer-clip-main-grad: 7.38 | optimizer-copy-main-to-model-params: 12.14 | optimizer: 99.98 | batch-generator: 7.94
 iteration     2800/  200000 | consumed samples:        89600 | elapsed time per iteration (ms): 255.9 | learning rate: 9.764E-05 | global batch size:    32 | lm loss: 6.740583E-04 | loss scale: 8388608.0 | grad norm: 0.011 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.96 | backward-params-all-reduce: 63.70 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.13 | optimizer-unscale-and-check-inf: 18.37 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.06 | batch-generator: 8.16
 iteration     2900/  200000 | consumed samples:        92800 | elapsed time per iteration (ms): 257.4 | learning rate: 9.755E-05 | global batch size:    32 | lm loss: 6.590704E-04 | loss scale: 8388608.0 | grad norm: 0.012 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.95 | backward-params-all-reduce: 64.23 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.01 | optimizer-unscale-and-check-inf: 19.16 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.14 | optimizer: 98.62 | batch-generator: 7.98
 iteration     3000/  200000 | consumed samples:        96000 | elapsed time per iteration (ms): 257.4 | learning rate: 9.746E-05 | global batch size:    32 | lm loss: 6.700991E-04 | loss scale: 8388608.0 | grad norm: 0.020 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.04 | backward-params-all-reduce: 63.29 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.95 | optimizer-unscale-and-check-inf: 19.26 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.70 | batch-generator: 7.92
------------------------------------------------------------------------------------------------
 validation loss at iteration 3000 | lm loss value: 6.773787E-04 | lm loss PPL: 1.000678E+00 | 
 at iteration 3000, match long value: 0.001046301355478694 | match short value: -0.007681360992047134 
-------------------------------------------------------------------------------------------------------
 iteration     3100/  200000 | consumed samples:        99200 | elapsed time per iteration (ms): 274.1 | learning rate: 9.737E-05 | global batch size:    32 | lm loss: 6.570277E-04 | loss scale: 16777216.0 | grad norm: 0.023 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.56 | backward-params-all-reduce: 63.90 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.02 | optimizer-unscale-and-check-inf: 18.99 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 13.69 | optimizer: 100.01 | batch-generator: 14.73
 iteration     3200/  200000 | consumed samples:       102400 | elapsed time per iteration (ms): 257.9 | learning rate: 9.728E-05 | global batch size:    32 | lm loss: 6.794741E-04 | loss scale: 16777216.0 | grad norm: 0.013 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.35 | backward-params-all-reduce: 64.64 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.92 | optimizer-unscale-and-check-inf: 18.76 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.14 | optimizer: 98.14 | batch-generator: 7.93
 iteration     3300/  200000 | consumed samples:       105600 | elapsed time per iteration (ms): 259.1 | learning rate: 9.719E-05 | global batch size:    32 | lm loss: 6.458659E-04 | loss scale: 16777216.0 | grad norm: 0.012 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.01 | backward-params-all-reduce: 63.59 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.88 | optimizer-unscale-and-check-inf: 19.21 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.14 | optimizer: 98.55 | batch-generator: 7.95
 iteration     3400/  200000 | consumed samples:       108800 | elapsed time per iteration (ms): 257.4 | learning rate: 9.709E-05 | global batch size:    32 | lm loss: 6.754433E-04 | loss scale: 16777216.0 | grad norm: 0.014 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.97 | backward-params-all-reduce: 64.84 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.11 | optimizer-unscale-and-check-inf: 19.78 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.14 | optimizer: 99.37 | batch-generator: 7.95
 iteration     3500/  200000 | consumed samples:       112000 | elapsed time per iteration (ms): 257.8 | learning rate: 9.700E-05 | global batch size:    32 | lm loss: 6.659480E-04 | loss scale: 16777216.0 | grad norm: 0.011 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.09 | backward-params-all-reduce: 64.45 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.07 | optimizer-unscale-and-check-inf: 18.15 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 97.74 | batch-generator: 8.03
------------------------------------------------------------------------------------------------
 validation loss at iteration 3500 | lm loss value: 6.357734E-04 | lm loss PPL: 1.000636E+00 | 
 at iteration 3500, match long value: 0.003147490109555821 | match short value: 0.00855956883722807 
-----------------------------------------------------------------------------------------------------
 iteration     3600/  200000 | consumed samples:       115200 | elapsed time per iteration (ms): 273.4 | learning rate: 9.691E-05 | global batch size:    32 | lm loss: 6.666736E-04 | loss scale: 16777216.0 | grad norm: 0.010 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.21 | backward-params-all-reduce: 62.77 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.88 | optimizer-unscale-and-check-inf: 19.86 | optimizer-clip-main-grad: 7.42 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 99.34 | batch-generator: 21.34
 iteration     3700/  200000 | consumed samples:       118400 | elapsed time per iteration (ms): 256.9 | learning rate: 9.682E-05 | global batch size:    32 | lm loss: 6.585941E-04 | loss scale: 16777216.0 | grad norm: 0.017 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.98 | backward-params-all-reduce: 61.83 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.97 | optimizer-unscale-and-check-inf: 19.68 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 99.19 | batch-generator: 8.17
 iteration     3800/  200000 | consumed samples:       121600 | elapsed time per iteration (ms): 263.0 | learning rate: 9.673E-05 | global batch size:    32 | lm loss: 6.816215E-04 | loss scale: 16777216.0 | grad norm: 0.015 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.22 | backward-params-all-reduce: 63.79 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.28 | optimizer-unscale-and-check-inf: 18.75 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 98.61 | batch-generator: 8.20
 iteration     3900/  200000 | consumed samples:       124800 | elapsed time per iteration (ms): 257.3 | learning rate: 9.664E-05 | global batch size:    32 | lm loss: 6.839024E-04 | loss scale: 16777216.0 | grad norm: 0.010 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.22 | backward-params-all-reduce: 61.66 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.44 | optimizer-unscale-and-check-inf: 17.98 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 13.73 | optimizer: 99.47 | batch-generator: 8.17
 iteration     4000/  200000 | consumed samples:       128000 | elapsed time per iteration (ms): 252.9 | learning rate: 9.655E-05 | global batch size:    32 | lm loss: 6.417631E-04 | loss scale: 16777216.0 | grad norm: 0.022 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.08 | backward-params-all-reduce: 61.69 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.87 | optimizer-unscale-and-check-inf: 15.37 | optimizer-clip-main-grad: 7.27 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 94.71 | batch-generator: 7.92
------------------------------------------------------------------------------------------------
 validation loss at iteration 4000 | lm loss value: 6.799516E-04 | lm loss PPL: 1.000680E+00 | 
 at iteration 4000, match long value: 0.005326727714209906 | match short value: 0.01639648839480421 
-----------------------------------------------------------------------------------------------------
 iteration     4100/  200000 | consumed samples:       131200 | elapsed time per iteration (ms): 270.4 | learning rate: 9.646E-05 | global batch size:    32 | lm loss: 6.542465E-04 | loss scale: 33554432.0 | grad norm: 0.012 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.09 | backward-params-all-reduce: 64.09 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.94 | optimizer-unscale-and-check-inf: 19.59 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 99.07 | batch-generator: 13.38
 iteration     4200/  200000 | consumed samples:       134400 | elapsed time per iteration (ms): 254.7 | learning rate: 9.637E-05 | global batch size:    32 | lm loss: 6.580620E-04 | loss scale: 33554432.0 | grad norm: 0.023 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.33 | backward-params-all-reduce: 62.41 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.58 | optimizer-unscale-and-check-inf: 18.23 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 98.34 | batch-generator: 8.09
 iteration     4300/  200000 | consumed samples:       137600 | elapsed time per iteration (ms): 252.8 | learning rate: 9.627E-05 | global batch size:    32 | lm loss: 6.576505E-04 | loss scale: 33554432.0 | grad norm: 0.015 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.03 | backward-params-all-reduce: 61.83 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.97 | optimizer-unscale-and-check-inf: 18.29 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.78 | batch-generator: 8.13
 iteration     4400/  200000 | consumed samples:       140800 | elapsed time per iteration (ms): 255.6 | learning rate: 9.618E-05 | global batch size:    32 | lm loss: 6.230443E-04 | loss scale: 33554432.0 | grad norm: 0.015 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.99 | backward-params-all-reduce: 62.79 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.96 | optimizer-unscale-and-check-inf: 19.37 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.89 | batch-generator: 7.99
 iteration     4500/  200000 | consumed samples:       144000 | elapsed time per iteration (ms): 262.4 | learning rate: 9.609E-05 | global batch size:    32 | lm loss: 9.362479E-04 | loss scale: 33554432.0 | grad norm: 0.012 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.87 | backward-params-all-reduce: 62.65 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.99 | optimizer-unscale-and-check-inf: 19.32 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.86 | batch-generator: 8.09
------------------------------------------------------------------------------------------------
 validation loss at iteration 4500 | lm loss value: 7.425734E-04 | lm loss PPL: 1.000743E+00 | 
 at iteration 4500, match long value: 0.0021327255922030937 | match short value: 0.012301835104456019 
-------------------------------------------------------------------------------------------------------
 iteration     4600/  200000 | consumed samples:       147200 | elapsed time per iteration (ms): 270.3 | learning rate: 9.600E-05 | global batch size:    32 | lm loss: 6.850941E-04 | loss scale: 33554432.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.11 | backward-params-all-reduce: 62.94 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 17.64 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 13.67 | optimizer: 98.45 | batch-generator: 14.90
 iteration     4700/  200000 | consumed samples:       150400 | elapsed time per iteration (ms): 255.9 | learning rate: 9.591E-05 | global batch size:    32 | lm loss: 6.668676E-04 | loss scale: 33554432.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.88 | backward-params-all-reduce: 63.12 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.94 | optimizer-unscale-and-check-inf: 18.18 | optimizer-clip-main-grad: 7.28 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.59 | batch-generator: 8.06
 iteration     4800/  200000 | consumed samples:       153600 | elapsed time per iteration (ms): 255.6 | learning rate: 9.582E-05 | global batch size:    32 | lm loss: 6.672812E-04 | loss scale: 33554432.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.96 | backward-params-all-reduce: 62.79 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 18.89 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.23 | batch-generator: 8.05
 iteration     4900/  200000 | consumed samples:       156800 | elapsed time per iteration (ms): 256.3 | learning rate: 9.573E-05 | global batch size:    32 | lm loss: 6.636014E-04 | loss scale: 33554432.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.93 | backward-params-all-reduce: 63.65 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 18.24 | optimizer-clip-main-grad: 7.28 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.56 | batch-generator: 8.02
 iteration     5000/  200000 | consumed samples:       160000 | elapsed time per iteration (ms): 254.9 | learning rate: 9.564E-05 | global batch size:    32 | lm loss: 6.549595E-04 | loss scale: 33554432.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.88 | backward-params-all-reduce: 62.95 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 17.34 | optimizer-clip-main-grad: 7.28 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 96.66 | batch-generator: 8.11
------------------------------------------------------------------------------------------------
 validation loss at iteration 5000 | lm loss value: 5.945347E-04 | lm loss PPL: 1.000595E+00 | 
 at iteration 5000, match long value: 0.00627795903462642 | match short value: -0.018503755467173234 
------------------------------------------------------------------------------------------------------
 iteration     5100/  200000 | consumed samples:       163200 | elapsed time per iteration (ms): 267.2 | learning rate: 9.555E-05 | global batch size:    32 | lm loss: 6.576794E-04 | loss scale: 67108864.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.21 | backward-params-all-reduce: 62.96 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 14.86 | optimizer-clip-main-grad: 7.26 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 94.15 | batch-generator: 19.20
 iteration     5200/  200000 | consumed samples:       166400 | elapsed time per iteration (ms): 249.3 | learning rate: 9.545E-05 | global batch size:    32 | lm loss: 6.704904E-04 | loss scale: 67108864.0 | grad norm: 0.014 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.00 | backward-params-all-reduce: 61.53 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 14.22 | optimizer-clip-main-grad: 7.24 | optimizer-copy-main-to-model-params: 12.22 | optimizer: 93.54 | batch-generator: 8.30
 iteration     5300/  200000 | consumed samples:       169600 | elapsed time per iteration (ms): 258.4 | learning rate: 9.536E-05 | global batch size:    32 | lm loss: 6.583305E-04 | loss scale: 67108864.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.17 | backward-params-all-reduce: 62.69 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.60 | optimizer-unscale-and-check-inf: 15.16 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 13.78 | optimizer: 97.11 | batch-generator: 8.37
 iteration     5400/  200000 | consumed samples:       172800 | elapsed time per iteration (ms): 253.7 | learning rate: 9.527E-05 | global batch size:    32 | lm loss: 6.717139E-04 | loss scale: 67108864.0 | grad norm: 0.015 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.97 | backward-params-all-reduce: 61.94 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.55 | optimizer-unscale-and-check-inf: 15.66 | optimizer-clip-main-grad: 7.72 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 96.16 | batch-generator: 8.43
 iteration     5500/  200000 | consumed samples:       176000 | elapsed time per iteration (ms): 254.6 | learning rate: 9.518E-05 | global batch size:    32 | lm loss: 7.205898E-04 | loss scale: 67108864.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.01 | backward-params-all-reduce: 62.42 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.80 | optimizer-unscale-and-check-inf: 15.31 | optimizer-clip-main-grad: 7.69 | optimizer-copy-main-to-model-params: 12.19 | optimizer: 96.02 | batch-generator: 8.43
------------------------------------------------------------------------------------------------
 validation loss at iteration 5500 | lm loss value: 6.776529E-04 | lm loss PPL: 1.000678E+00 | 
 at iteration 5500, match long value: 0.006297584225754647 | match short value: 0.03624938877968978 
-----------------------------------------------------------------------------------------------------
 iteration     5600/  200000 | consumed samples:       179200 | elapsed time per iteration (ms): 265.7 | learning rate: 9.509E-05 | global batch size:    32 | lm loss: 6.694280E-04 | loss scale: 67108864.0 | grad norm: 0.015 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.04 | backward-params-all-reduce: 63.00 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 17.13 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 96.47 | batch-generator: 14.44
 iteration     5700/  200000 | consumed samples:       182400 | elapsed time per iteration (ms): 257.8 | learning rate: 9.500E-05 | global batch size:    32 | lm loss: 6.564434E-04 | loss scale: 67108864.0 | grad norm: 0.010 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.24 | backward-params-all-reduce: 67.44 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.87 | optimizer-unscale-and-check-inf: 15.56 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 94.91 | batch-generator: 8.19
 iteration     5800/  200000 | consumed samples:       185600 | elapsed time per iteration (ms): 255.1 | learning rate: 9.491E-05 | global batch size:    32 | lm loss: 6.675154E-04 | loss scale: 67108864.0 | grad norm: 0.011 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.08 | backward-params-all-reduce: 63.82 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.01 | optimizer-unscale-and-check-inf: 15.96 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 95.44 | batch-generator: 8.12
 iteration     5900/  200000 | consumed samples:       188800 | elapsed time per iteration (ms): 253.7 | learning rate: 9.482E-05 | global batch size:    32 | lm loss: 6.914335E-04 | loss scale: 67108864.0 | grad norm: 0.011 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.04 | backward-params-all-reduce: 63.42 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.92 | optimizer-unscale-and-check-inf: 15.64 | optimizer-clip-main-grad: 7.25 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 94.98 | batch-generator: 8.05
 iteration     6000/  200000 | consumed samples:       192000 | elapsed time per iteration (ms): 251.8 | learning rate: 9.473E-05 | global batch size:    32 | lm loss: 6.628920E-04 | loss scale: 67108864.0 | grad norm: 0.015 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.18 | backward-params-all-reduce: 63.25 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.00 | optimizer-unscale-and-check-inf: 14.76 | optimizer-clip-main-grad: 7.24 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 94.19 | batch-generator: 8.27
------------------------------------------------------------------------------------------------
 validation loss at iteration 6000 | lm loss value: 6.850993E-04 | lm loss PPL: 1.000685E+00 | 
 at iteration 6000, match long value: 0.0024178760996495475 | match short value: 0.010794997810666873 
-------------------------------------------------------------------------------------------------------
 iteration     6100/  200000 | consumed samples:       195200 | elapsed time per iteration (ms): 271.0 | learning rate: 9.463E-05 | global batch size:    32 | lm loss: 6.649253E-04 | loss scale: 134217728.0 | grad norm: 0.015 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.19 | backward-params-all-reduce: 65.04 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.03 | optimizer-unscale-and-check-inf: 15.14 | optimizer-clip-main-grad: 7.26 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 95.69 | batch-generator: 13.72
 iteration     6200/  200000 | consumed samples:       198400 | elapsed time per iteration (ms): 256.0 | learning rate: 9.454E-05 | global batch size:    32 | lm loss: 6.666468E-04 | loss scale: 134217728.0 | grad norm: 0.021 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.26 | backward-params-all-reduce: 64.55 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.96 | optimizer-unscale-and-check-inf: 16.42 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 95.86 | batch-generator: 8.15
 iteration     6300/  200000 | consumed samples:       201600 | elapsed time per iteration (ms): 259.7 | learning rate: 9.445E-05 | global batch size:    32 | lm loss: 6.685780E-04 | loss scale: 134217728.0 | grad norm: 0.013 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.21 | backward-params-all-reduce: 66.04 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.30 | optimizer-unscale-and-check-inf: 16.35 | optimizer-clip-main-grad: 7.27 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 96.10 | batch-generator: 8.07
 iteration     6400/  200000 | consumed samples:       204800 | elapsed time per iteration (ms): 254.1 | learning rate: 9.436E-05 | global batch size:    32 | lm loss: 6.554129E-04 | loss scale: 134217728.0 | grad norm: 0.017 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.11 | backward-params-all-reduce: 64.08 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.96 | optimizer-unscale-and-check-inf: 16.30 | optimizer-clip-main-grad: 7.28 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 95.72 | batch-generator: 8.05
 iteration     6500/  200000 | consumed samples:       208000 | elapsed time per iteration (ms): 249.3 | learning rate: 9.427E-05 | global batch size:    32 | lm loss: 6.379052E-04 | loss scale: 67108864.0 | grad norm: 0.021 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.06 | backward-params-all-reduce: 63.29 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.92 | optimizer-unscale-and-check-inf: 15.49 | optimizer-clip-main-grad: 7.11 | optimizer-copy-main-to-model-params: 11.92 | optimizer: 93.49 | batch-generator: 8.23
------------------------------------------------------------------------------------------------
 validation loss at iteration 6500 | lm loss value: 6.743786E-04 | lm loss PPL: 1.000675E+00 | 
 at iteration 6500, match long value: 0.005602140517465603 | match short value: 0.03013577808505266 
-----------------------------------------------------------------------------------------------------
 iteration     6600/  200000 | consumed samples:       211200 | elapsed time per iteration (ms): 266.5 | learning rate: 9.418E-05 | global batch size:    32 | lm loss: 6.199125E-04 | loss scale: 67108864.0 | grad norm: 0.015 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.33 | backward-params-all-reduce: 63.06 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.04 | optimizer-unscale-and-check-inf: 15.28 | optimizer-clip-main-grad: 7.25 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 94.77 | batch-generator: 17.83
 iteration     6700/  200000 | consumed samples:       214400 | elapsed time per iteration (ms): 253.1 | learning rate: 9.409E-05 | global batch size:    32 | lm loss: 6.038041E-04 | loss scale: 67108864.0 | grad norm: 0.016 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.03 | backward-params-all-reduce: 63.22 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.87 | optimizer-unscale-and-check-inf: 15.49 | optimizer-clip-main-grad: 7.27 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 94.83 | batch-generator: 8.27
 iteration     6800/  200000 | consumed samples:       217600 | elapsed time per iteration (ms): 264.5 | learning rate: 9.400E-05 | global batch size:    32 | lm loss: 6.062541E-04 | loss scale: 67108864.0 | grad norm: 0.015 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.67 | backward-params-all-reduce: 62.78 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 16.70 | optimizer-clip-main-grad: 7.27 | optimizer-copy-main-to-model-params: 13.57 | optimizer: 97.41 | batch-generator: 8.17
 iteration     6900/  200000 | consumed samples:       220800 | elapsed time per iteration (ms): 251.5 | learning rate: 9.391E-05 | global batch size:    32 | lm loss: 6.083208E-04 | loss scale: 67108864.0 | grad norm: 0.012 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.08 | backward-params-all-reduce: 62.59 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.92 | optimizer-unscale-and-check-inf: 15.20 | optimizer-clip-main-grad: 7.27 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 94.58 | batch-generator: 8.35
 iteration     7000/  200000 | consumed samples:       224000 | elapsed time per iteration (ms): 255.6 | learning rate: 9.382E-05 | global batch size:    32 | lm loss: 5.684185E-04 | loss scale: 67108864.0 | grad norm: 0.020 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.08 | backward-params-all-reduce: 63.64 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.92 | optimizer-unscale-and-check-inf: 15.99 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 95.39 | batch-generator: 8.20
------------------------------------------------------------------------------------------------
 validation loss at iteration 7000 | lm loss value: 5.827330E-04 | lm loss PPL: 1.000583E+00 | 
 at iteration 7000, match long value: 0.0018467114605622524 | match short value: -0.04635529469033603 
-------------------------------------------------------------------------------------------------------
 iteration     7100/  200000 | consumed samples:       227200 | elapsed time per iteration (ms): 270.0 | learning rate: 9.373E-05 | global batch size:    32 | lm loss: 5.564332E-04 | loss scale: 67108864.0 | grad norm: 0.020 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.15 | backward-params-all-reduce: 62.31 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.54 | optimizer-unscale-and-check-inf: 18.72 | optimizer-clip-main-grad: 7.39 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.85 | batch-generator: 18.64
 iteration     7200/  200000 | consumed samples:       230400 | elapsed time per iteration (ms): 253.8 | learning rate: 9.363E-05 | global batch size:    32 | lm loss: 5.029616E-04 | loss scale: 67108864.0 | grad norm: 0.015 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.18 | backward-params-all-reduce: 62.33 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.99 | optimizer-unscale-and-check-inf: 18.86 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 98.36 | batch-generator: 8.23
 iteration     7300/  200000 | consumed samples:       233600 | elapsed time per iteration (ms): 253.9 | learning rate: 9.354E-05 | global batch size:    32 | lm loss: 4.911084E-04 | loss scale: 67108864.0 | grad norm: 0.018 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.15 | backward-params-all-reduce: 62.89 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.58 | optimizer-unscale-and-check-inf: 18.15 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.24 | batch-generator: 8.15
 iteration     7400/  200000 | consumed samples:       236800 | elapsed time per iteration (ms): 253.3 | learning rate: 9.345E-05 | global batch size:    32 | lm loss: 3.637996E-04 | loss scale: 67108864.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.06 | backward-params-all-reduce: 62.64 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.48 | optimizer-unscale-and-check-inf: 17.80 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.76 | batch-generator: 8.07
 iteration     7500/  200000 | consumed samples:       240000 | elapsed time per iteration (ms): 263.7 | learning rate: 9.336E-05 | global batch size:    32 | lm loss: 3.247115E-04 | loss scale: 134217728.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.40 | backward-params-all-reduce: 64.37 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.04 | optimizer-unscale-and-check-inf: 19.29 | optimizer-clip-main-grad: 7.39 | optimizer-copy-main-to-model-params: 13.58 | optimizer: 100.32 | batch-generator: 8.00
------------------------------------------------------------------------------------------------
 validation loss at iteration 7500 | lm loss value: 2.992494E-04 | lm loss PPL: 1.000299E+00 | 
 at iteration 7500, match long value: 0.013486362379114086 | match short value: -0.015062045082788867 
-------------------------------------------------------------------------------------------------------
 iteration     7600/  200000 | consumed samples:       243200 | elapsed time per iteration (ms): 271.8 | learning rate: 9.327E-05 | global batch size:    32 | lm loss: 2.859428E-04 | loss scale: 134217728.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.24 | backward-params-all-reduce: 64.48 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.25 | optimizer-unscale-and-check-inf: 19.28 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 99.01 | batch-generator: 13.57
 iteration     7700/  200000 | consumed samples:       246400 | elapsed time per iteration (ms): 259.5 | learning rate: 9.318E-05 | global batch size:    32 | lm loss: 2.435664E-04 | loss scale: 134217728.0 | grad norm: 0.018 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.16 | backward-params-all-reduce: 63.64 | backward-embedding-all-reduce: 0.04 | optimizer-copy-to-main-grad: 14.83 | optimizer-unscale-and-check-inf: 13.84 | optimizer-clip-main-grad: 7.21 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 96.08 | batch-generator: 7.84
 iteration     7800/  200000 | consumed samples:       249600 | elapsed time per iteration (ms): 254.3 | learning rate: 9.309E-05 | global batch size:    32 | lm loss: 2.313434E-04 | loss scale: 134217728.0 | grad norm: 0.010 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.15 | backward-params-all-reduce: 63.06 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 13.40 | optimizer-unscale-and-check-inf: 16.78 | optimizer-clip-main-grad: 7.26 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.61 | batch-generator: 7.98
 iteration     7900/  200000 | consumed samples:       252800 | elapsed time per iteration (ms): 255.8 | learning rate: 9.300E-05 | global batch size:    32 | lm loss: 2.083290E-04 | loss scale: 134217728.0 | grad norm: 0.010 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.22 | backward-params-all-reduce: 63.84 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.89 | optimizer-unscale-and-check-inf: 19.09 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.47 | batch-generator: 8.01
 iteration     8000/  200000 | consumed samples:       256000 | elapsed time per iteration (ms): 255.0 | learning rate: 9.291E-05 | global batch size:    32 | lm loss: 1.941747E-04 | loss scale: 134217728.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.11 | backward-params-all-reduce: 64.12 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 18.83 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.15 | batch-generator: 7.99
------------------------------------------------------------------------------------------------
 validation loss at iteration 8000 | lm loss value: 1.952711E-04 | lm loss PPL: 1.000195E+00 | 
 at iteration 8000, match long value: 0.006769473088153232 | match short value: -0.02730216666806583 
------------------------------------------------------------------------------------------------------
 iteration     8100/  200000 | consumed samples:       259200 | elapsed time per iteration (ms): 269.8 | learning rate: 9.281E-05 | global batch size:    32 | lm loss: 1.974218E-04 | loss scale: 134217728.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.23 | backward-params-all-reduce: 63.02 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.00 | optimizer-unscale-and-check-inf: 17.95 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.47 | batch-generator: 18.72
 iteration     8200/  200000 | consumed samples:       262400 | elapsed time per iteration (ms): 257.0 | learning rate: 9.272E-05 | global batch size:    32 | lm loss: 1.809771E-04 | loss scale: 134217728.0 | grad norm: 0.009 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.99 | backward-params-all-reduce: 63.08 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.03 | optimizer-unscale-and-check-inf: 16.92 | optimizer-clip-main-grad: 7.21 | optimizer-copy-main-to-model-params: 13.53 | optimizer: 97.22 | batch-generator: 8.02
 iteration     8300/  200000 | consumed samples:       265600 | elapsed time per iteration (ms): 256.4 | learning rate: 9.263E-05 | global batch size:    32 | lm loss: 1.720501E-04 | loss scale: 67108864.0 | grad norm: 0.007 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.23 | backward-params-all-reduce: 62.62 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.96 | optimizer-unscale-and-check-inf: 18.07 | optimizer-clip-main-grad: 7.24 | optimizer-copy-main-to-model-params: 12.04 | optimizer: 96.83 | batch-generator: 8.05
 iteration     8400/  200000 | consumed samples:       268800 | elapsed time per iteration (ms): 253.6 | learning rate: 9.254E-05 | global batch size:    32 | lm loss: 1.522172E-04 | loss scale: 67108864.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.31 | backward-params-all-reduce: 62.61 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 17.68 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 97.02 | batch-generator: 8.09
 iteration     8500/  200000 | consumed samples:       272000 | elapsed time per iteration (ms): 254.7 | learning rate: 9.245E-05 | global batch size:    32 | lm loss: 1.474098E-04 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.30 | backward-params-all-reduce: 62.53 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 17.80 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.14 | batch-generator: 8.05
------------------------------------------------------------------------------------------------
 validation loss at iteration 8500 | lm loss value: 1.309695E-04 | lm loss PPL: 1.000131E+00 | 
 at iteration 8500, match long value: 0.0058164422793951995 | match short value: -0.04718268702171878 
-------------------------------------------------------------------------------------------------------
 iteration     8600/  200000 | consumed samples:       275200 | elapsed time per iteration (ms): 264.1 | learning rate: 9.236E-05 | global batch size:    32 | lm loss: 1.429363E-04 | loss scale: 67108864.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.31 | backward-params-all-reduce: 63.50 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.00 | optimizer-unscale-and-check-inf: 17.04 | optimizer-clip-main-grad: 7.28 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 96.48 | batch-generator: 14.38
 iteration     8700/  200000 | consumed samples:       278400 | elapsed time per iteration (ms): 258.0 | learning rate: 9.227E-05 | global batch size:    32 | lm loss: 1.324781E-04 | loss scale: 67108864.0 | grad norm: 0.010 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 54.32 | backward-params-all-reduce: 61.63 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.61 | optimizer-unscale-and-check-inf: 17.25 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.34 | batch-generator: 7.92
 iteration     8800/  200000 | consumed samples:       281600 | elapsed time per iteration (ms): 264.2 | learning rate: 9.218E-05 | global batch size:    32 | lm loss: 1.364638E-04 | loss scale: 67108864.0 | grad norm: 0.012 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 55.53 | backward-params-all-reduce: 60.85 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 13.04 | optimizer-unscale-and-check-inf: 16.73 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 97.24 | batch-generator: 7.92
 iteration     8900/  200000 | consumed samples:       284800 | elapsed time per iteration (ms): 255.7 | learning rate: 9.209E-05 | global batch size:    32 | lm loss: 1.339422E-04 | loss scale: 67108864.0 | grad norm: 0.016 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 55.12 | backward-params-all-reduce: 59.70 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.35 | optimizer-unscale-and-check-inf: 16.71 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 96.56 | batch-generator: 7.99
 iteration     9000/  200000 | consumed samples:       288000 | elapsed time per iteration (ms): 260.8 | learning rate: 9.200E-05 | global batch size:    32 | lm loss: 1.307620E-04 | loss scale: 67108864.0 | grad norm: 0.011 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.53 | backward-params-all-reduce: 62.89 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.92 | optimizer-unscale-and-check-inf: 16.13 | optimizer-clip-main-grad: 7.25 | optimizer-copy-main-to-model-params: 13.60 | optimizer: 96.88 | batch-generator: 8.03
------------------------------------------------------------------------------------------------
 validation loss at iteration 9000 | lm loss value: 1.227051E-04 | lm loss PPL: 1.000123E+00 | 
 at iteration 9000, match long value: 0.0020662965621656656 | match short value: -0.03364817865173323 
-------------------------------------------------------------------------------------------------------
 iteration     9100/  200000 | consumed samples:       291200 | elapsed time per iteration (ms): 265.4 | learning rate: 9.191E-05 | global batch size:    32 | lm loss: 1.301183E-04 | loss scale: 67108864.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.04 | backward-params-all-reduce: 63.35 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.87 | optimizer-unscale-and-check-inf: 16.55 | optimizer-clip-main-grad: 7.26 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 95.83 | batch-generator: 14.27
 iteration     9200/  200000 | consumed samples:       294400 | elapsed time per iteration (ms): 256.1 | learning rate: 9.181E-05 | global batch size:    32 | lm loss: 1.142497E-04 | loss scale: 67108864.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.08 | backward-params-all-reduce: 63.52 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 17.38 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 96.68 | batch-generator: 7.87
 iteration     9300/  200000 | consumed samples:       297600 | elapsed time per iteration (ms): 253.7 | learning rate: 9.172E-05 | global batch size:    32 | lm loss: 1.120337E-04 | loss scale: 134217728.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.12 | backward-params-all-reduce: 62.61 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.18 | optimizer-unscale-and-check-inf: 15.93 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 95.58 | batch-generator: 7.95
 iteration     9400/  200000 | consumed samples:       300800 | elapsed time per iteration (ms): 254.2 | learning rate: 9.163E-05 | global batch size:    32 | lm loss: 1.204555E-04 | loss scale: 134217728.0 | grad norm: 0.012 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.37 | backward-params-all-reduce: 63.24 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.87 | optimizer-unscale-and-check-inf: 16.01 | optimizer-clip-main-grad: 7.24 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 95.28 | batch-generator: 7.94
 iteration     9500/  200000 | consumed samples:       304000 | elapsed time per iteration (ms): 254.0 | learning rate: 9.154E-05 | global batch size:    32 | lm loss: 1.187519E-04 | loss scale: 134217728.0 | grad norm: 0.010 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.35 | backward-params-all-reduce: 64.14 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.87 | optimizer-unscale-and-check-inf: 15.34 | optimizer-clip-main-grad: 7.26 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 94.65 | batch-generator: 8.03
------------------------------------------------------------------------------------------------
 validation loss at iteration 9500 | lm loss value: 1.094406E-04 | lm loss PPL: 1.000109E+00 | 
 at iteration 9500, match long value: 0.007256680471798105 | match short value: -0.008399359040954783 
-------------------------------------------------------------------------------------------------------
 iteration     9600/  200000 | consumed samples:       307200 | elapsed time per iteration (ms): 266.7 | learning rate: 9.145E-05 | global batch size:    32 | lm loss: 1.095530E-04 | loss scale: 134217728.0 | grad norm: 0.010 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.36 | backward-params-all-reduce: 62.39 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.97 | optimizer-unscale-and-check-inf: 15.70 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 95.16 | batch-generator: 18.04
 iteration     9700/  200000 | consumed samples:       310400 | elapsed time per iteration (ms): 253.7 | learning rate: 9.136E-05 | global batch size:    32 | lm loss: 1.011459E-04 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.54 | backward-params-all-reduce: 60.37 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.92 | optimizer-unscale-and-check-inf: 14.72 | optimizer-clip-main-grad: 7.27 | optimizer-copy-main-to-model-params: 13.69 | optimizer: 95.60 | batch-generator: 8.32
 iteration     9800/  200000 | consumed samples:       313600 | elapsed time per iteration (ms): 250.4 | learning rate: 9.127E-05 | global batch size:    32 | lm loss: 1.049916E-04 | loss scale: 134217728.0 | grad norm: 0.009 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.12 | backward-params-all-reduce: 62.09 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 14.10 | optimizer-clip-main-grad: 7.16 | optimizer-copy-main-to-model-params: 12.04 | optimizer: 92.68 | batch-generator: 8.02
 iteration     9900/  200000 | consumed samples:       316800 | elapsed time per iteration (ms): 252.2 | learning rate: 9.118E-05 | global batch size:    32 | lm loss: 1.144618E-04 | loss scale: 134217728.0 | grad norm: 0.013 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.71 | backward-params-all-reduce: 63.07 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 15.04 | optimizer-clip-main-grad: 7.24 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 94.30 | batch-generator: 8.08
 iteration    10000/  200000 | consumed samples:       320000 | elapsed time per iteration (ms): 252.4 | learning rate: 9.109E-05 | global batch size:    32 | lm loss: 1.202325E-04 | loss scale: 134217728.0 | grad norm: 0.011 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.37 | backward-params-all-reduce: 63.66 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.88 | optimizer-unscale-and-check-inf: 15.18 | optimizer-clip-main-grad: 7.25 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 94.48 | batch-generator: 8.02
-------------------------------------------------------------------------------------------------
 validation loss at iteration 10000 | lm loss value: 1.018302E-04 | lm loss PPL: 1.000102E+00 | 
 at iteration 10000, match long value: 0.005996477185757027 | match short value: 0.015246487524455154 
-------------------------------------------------------------------------------------------------------
 iteration    10100/  200000 | consumed samples:       323200 | elapsed time per iteration (ms): 265.2 | learning rate: 9.100E-05 | global batch size:    32 | lm loss: 9.871851E-05 | loss scale: 134217728.0 | grad norm: 0.014 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.15 | backward-params-all-reduce: 63.52 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.93 | optimizer-unscale-and-check-inf: 16.03 | optimizer-clip-main-grad: 7.26 | optimizer-copy-main-to-model-params: 12.21 | optimizer: 95.43 | batch-generator: 15.78
 iteration    10200/  200000 | consumed samples:       326400 | elapsed time per iteration (ms): 259.5 | learning rate: 9.090E-05 | global batch size:    32 | lm loss: 1.041510E-04 | loss scale: 134217728.0 | grad norm: 0.011 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.09 | backward-params-all-reduce: 63.77 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.05 | optimizer-unscale-and-check-inf: 17.22 | optimizer-clip-main-grad: 7.27 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 96.71 | batch-generator: 8.01
 iteration    10300/  200000 | consumed samples:       329600 | elapsed time per iteration (ms): 255.1 | learning rate: 9.081E-05 | global batch size:    32 | lm loss: 1.134543E-04 | loss scale: 134217728.0 | grad norm: 0.012 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.99 | backward-params-all-reduce: 62.72 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 17.22 | optimizer-clip-main-grad: 7.28 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 96.49 | batch-generator: 8.01
 iteration    10400/  200000 | consumed samples:       332800 | elapsed time per iteration (ms): 260.4 | learning rate: 9.072E-05 | global batch size:    32 | lm loss: 9.927821E-05 | loss scale: 134217728.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.78 | backward-params-all-reduce: 62.52 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.91 | optimizer-unscale-and-check-inf: 16.36 | optimizer-clip-main-grad: 7.26 | optimizer-copy-main-to-model-params: 13.71 | optimizer: 97.23 | batch-generator: 8.03
 iteration    10500/  200000 | consumed samples:       336000 | elapsed time per iteration (ms): 251.9 | learning rate: 9.063E-05 | global batch size:    32 | lm loss: 9.240475E-05 | loss scale: 134217728.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.12 | backward-params-all-reduce: 63.08 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.07 | optimizer-unscale-and-check-inf: 15.57 | optimizer-clip-main-grad: 7.26 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 95.06 | batch-generator: 8.16
-------------------------------------------------------------------------------------------------
 validation loss at iteration 10500 | lm loss value: 8.892057E-05 | lm loss PPL: 1.000089E+00 | 
 at iteration 10500, match long value: 0.012628706263515144 | match short value: 0.022391853773364192 
-------------------------------------------------------------------------------------------------------
 iteration    10600/  200000 | consumed samples:       339200 | elapsed time per iteration (ms): 274.2 | learning rate: 9.054E-05 | global batch size:    32 | lm loss: 9.571443E-05 | loss scale: 134217728.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.37 | backward-params-all-reduce: 62.74 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.91 | optimizer-unscale-and-check-inf: 20.12 | optimizer-clip-main-grad: 7.37 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 99.57 | batch-generator: 18.91
 iteration    10700/  200000 | consumed samples:       342400 | elapsed time per iteration (ms): 257.5 | learning rate: 9.045E-05 | global batch size:    32 | lm loss: 9.286555E-05 | loss scale: 134217728.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.29 | backward-params-all-reduce: 61.99 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.17 | optimizer-unscale-and-check-inf: 19.21 | optimizer-clip-main-grad: 7.37 | optimizer-copy-main-to-model-params: 12.21 | optimizer: 98.96 | batch-generator: 8.29
 iteration    10800/  200000 | consumed samples:       345600 | elapsed time per iteration (ms): 252.9 | learning rate: 9.036E-05 | global batch size:    32 | lm loss: 1.056184E-04 | loss scale: 67108864.0 | grad norm: 0.007 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.24 | backward-params-all-reduce: 61.18 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.04 | optimizer-unscale-and-check-inf: 19.78 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.08 | optimizer: 98.75 | batch-generator: 8.42
 iteration    10900/  200000 | consumed samples:       348800 | elapsed time per iteration (ms): 253.2 | learning rate: 9.027E-05 | global batch size:    32 | lm loss: 8.993842E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.18 | backward-params-all-reduce: 61.59 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.05 | optimizer-unscale-and-check-inf: 19.03 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 98.64 | batch-generator: 8.33
 iteration    11000/  200000 | consumed samples:       352000 | elapsed time per iteration (ms): 255.1 | learning rate: 9.018E-05 | global batch size:    32 | lm loss: 9.151815E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.10 | backward-params-all-reduce: 62.00 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.90 | optimizer-unscale-and-check-inf: 19.78 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 99.19 | batch-generator: 8.11
-------------------------------------------------------------------------------------------------
 validation loss at iteration 11000 | lm loss value: 9.594142E-05 | lm loss PPL: 1.000096E+00 | 
 at iteration 11000, match long value: -0.0023703957996087342 | match short value: -0.04734827961062023 
---------------------------------------------------------------------------------------------------------
 iteration    11100/  200000 | consumed samples:       355200 | elapsed time per iteration (ms): 270.2 | learning rate: 9.009E-05 | global batch size:    32 | lm loss: 8.987840E-05 | loss scale: 67108864.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.28 | backward-params-all-reduce: 63.25 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.95 | optimizer-unscale-and-check-inf: 19.14 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.58 | batch-generator: 17.07
 iteration    11200/  200000 | consumed samples:       358400 | elapsed time per iteration (ms): 261.9 | learning rate: 8.999E-05 | global batch size:    32 | lm loss: 1.034726E-04 | loss scale: 67108864.0 | grad norm: 0.011 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.57 | backward-params-all-reduce: 62.61 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.88 | optimizer-unscale-and-check-inf: 17.96 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 13.63 | optimizer: 98.80 | batch-generator: 8.05
 iteration    11300/  200000 | consumed samples:       361600 | elapsed time per iteration (ms): 254.9 | learning rate: 8.990E-05 | global batch size:    32 | lm loss: 9.026983E-05 | loss scale: 67108864.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.06 | backward-params-all-reduce: 63.33 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 17.40 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 96.78 | batch-generator: 8.09
 iteration    11400/  200000 | consumed samples:       364800 | elapsed time per iteration (ms): 254.1 | learning rate: 8.981E-05 | global batch size:    32 | lm loss: 8.876670E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.03 | backward-params-all-reduce: 62.14 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 18.17 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 97.53 | batch-generator: 8.04
 iteration    11500/  200000 | consumed samples:       368000 | elapsed time per iteration (ms): 253.7 | learning rate: 8.972E-05 | global batch size:    32 | lm loss: 8.447748E-05 | loss scale: 67108864.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.18 | backward-params-all-reduce: 62.82 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 18.42 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.20 | optimizer: 97.83 | batch-generator: 8.19
-------------------------------------------------------------------------------------------------
 validation loss at iteration 11500 | lm loss value: 8.748881E-05 | lm loss PPL: 1.000087E+00 | 
 at iteration 11500, match long value: 0.006615929266951882 | match short value: -0.0032349974539730435 
---------------------------------------------------------------------------------------------------------
 iteration    11600/  200000 | consumed samples:       371200 | elapsed time per iteration (ms): 265.0 | learning rate: 8.963E-05 | global batch size:    32 | lm loss: 8.218158E-05 | loss scale: 67108864.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.48 | backward-params-all-reduce: 63.31 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.96 | optimizer-unscale-and-check-inf: 18.18 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.64 | batch-generator: 14.55
 iteration    11700/  200000 | consumed samples:       374400 | elapsed time per iteration (ms): 251.9 | learning rate: 8.954E-05 | global batch size:    32 | lm loss: 8.542181E-05 | loss scale: 33554432.0 | grad norm: 0.009 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.05 | backward-params-all-reduce: 61.62 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.96 | optimizer-unscale-and-check-inf: 17.89 | optimizer-clip-main-grad: 7.23 | optimizer-copy-main-to-model-params: 12.06 | optimizer: 96.67 | batch-generator: 8.11
 iteration    11800/  200000 | consumed samples:       377600 | elapsed time per iteration (ms): 252.5 | learning rate: 8.945E-05 | global batch size:    32 | lm loss: 8.541317E-05 | loss scale: 33554432.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.32 | backward-params-all-reduce: 62.83 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 16.80 | optimizer-clip-main-grad: 7.28 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 96.12 | batch-generator: 8.07
 iteration    11900/  200000 | consumed samples:       380800 | elapsed time per iteration (ms): 263.2 | learning rate: 8.936E-05 | global batch size:    32 | lm loss: 8.118355E-05 | loss scale: 33554432.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.31 | backward-params-all-reduce: 63.34 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.16 | optimizer-unscale-and-check-inf: 18.12 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 13.65 | optimizer: 99.26 | batch-generator: 8.04
 iteration    12000/  200000 | consumed samples:       384000 | elapsed time per iteration (ms): 253.5 | learning rate: 8.927E-05 | global batch size:    32 | lm loss: 8.654658E-05 | loss scale: 33554432.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.01 | backward-params-all-reduce: 62.42 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.92 | optimizer-unscale-and-check-inf: 17.24 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 96.65 | batch-generator: 8.22
-------------------------------------------------------------------------------------------------
 validation loss at iteration 12000 | lm loss value: 8.559191E-05 | lm loss PPL: 1.000086E+00 | 
 at iteration 12000, match long value: 0.009072494128957515 | match short value: 0.02152848250422629 
------------------------------------------------------------------------------------------------------
 iteration    12100/  200000 | consumed samples:       387200 | elapsed time per iteration (ms): 266.7 | learning rate: 8.918E-05 | global batch size:    32 | lm loss: 8.903409E-05 | loss scale: 33554432.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.18 | backward-params-all-reduce: 63.55 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.96 | optimizer-unscale-and-check-inf: 18.29 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.71 | batch-generator: 13.68
 iteration    12200/  200000 | consumed samples:       390400 | elapsed time per iteration (ms): 253.7 | learning rate: 8.908E-05 | global batch size:    32 | lm loss: 8.642395E-05 | loss scale: 33554432.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.35 | backward-params-all-reduce: 62.12 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 17.79 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.13 | batch-generator: 7.92
 iteration    12300/  200000 | consumed samples:       393600 | elapsed time per iteration (ms): 254.8 | learning rate: 8.899E-05 | global batch size:    32 | lm loss: 8.255916E-05 | loss scale: 33554432.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.44 | backward-params-all-reduce: 62.70 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 16.95 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 96.28 | batch-generator: 8.11
 iteration    12400/  200000 | consumed samples:       396800 | elapsed time per iteration (ms): 254.9 | learning rate: 8.890E-05 | global batch size:    32 | lm loss: 7.565868E-05 | loss scale: 33554432.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.30 | backward-params-all-reduce: 62.70 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.96 | optimizer-unscale-and-check-inf: 17.33 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 96.80 | batch-generator: 8.03
 iteration    12500/  200000 | consumed samples:       400000 | elapsed time per iteration (ms): 254.1 | learning rate: 8.881E-05 | global batch size:    32 | lm loss: 8.689513E-05 | loss scale: 33554432.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.13 | backward-params-all-reduce: 62.23 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 17.88 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.20 | batch-generator: 7.93
-------------------------------------------------------------------------------------------------
 validation loss at iteration 12500 | lm loss value: 8.127779E-05 | lm loss PPL: 1.000081E+00 | 
 at iteration 12500, match long value: 0.004657965890719699 | match short value: -0.008475144116243323 
--------------------------------------------------------------------------------------------------------
 iteration    12600/  200000 | consumed samples:       403200 | elapsed time per iteration (ms): 275.9 | learning rate: 8.872E-05 | global batch size:    32 | lm loss: 8.464648E-05 | loss scale: 33554432.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.37 | backward-params-all-reduce: 63.65 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 17.20 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 13.64 | optimizer: 98.03 | batch-generator: 17.03
 iteration    12700/  200000 | consumed samples:       406400 | elapsed time per iteration (ms): 254.1 | learning rate: 8.863E-05 | global batch size:    32 | lm loss: 7.828865E-05 | loss scale: 67108864.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.07 | backward-params-all-reduce: 63.05 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.00 | optimizer-unscale-and-check-inf: 17.11 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 96.62 | batch-generator: 8.05
 iteration    12800/  200000 | consumed samples:       409600 | elapsed time per iteration (ms): 261.7 | learning rate: 8.854E-05 | global batch size:    32 | lm loss: 8.274710E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.10 | backward-params-all-reduce: 61.92 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 19.49 | optimizer-clip-main-grad: 7.37 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.88 | batch-generator: 8.03
 iteration    12900/  200000 | consumed samples:       412800 | elapsed time per iteration (ms): 257.5 | learning rate: 8.845E-05 | global batch size:    32 | lm loss: 7.419335E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.99 | backward-params-all-reduce: 62.66 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.90 | optimizer-unscale-and-check-inf: 17.82 | optimizer-clip-main-grad: 7.28 | optimizer-copy-main-to-model-params: 12.21 | optimizer: 97.21 | batch-generator: 8.15
 iteration    13000/  200000 | consumed samples:       416000 | elapsed time per iteration (ms): 252.3 | learning rate: 8.836E-05 | global batch size:    32 | lm loss: 7.658334E-05 | loss scale: 67108864.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.05 | backward-params-all-reduce: 62.09 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.91 | optimizer-unscale-and-check-inf: 16.53 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.21 | optimizer: 95.98 | batch-generator: 7.97
-------------------------------------------------------------------------------------------------
 validation loss at iteration 13000 | lm loss value: 7.524450E-05 | lm loss PPL: 1.000075E+00 | 
 at iteration 13000, match long value: 0.004617617474645407 | match short value: -0.03287058280601329 
-------------------------------------------------------------------------------------------------------
 iteration    13100/  200000 | consumed samples:       419200 | elapsed time per iteration (ms): 266.5 | learning rate: 8.826E-05 | global batch size:    32 | lm loss: 7.449465E-05 | loss scale: 67108864.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.08 | backward-params-all-reduce: 63.83 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.10 | optimizer-unscale-and-check-inf: 18.32 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 97.92 | batch-generator: 14.19
 iteration    13200/  200000 | consumed samples:       422400 | elapsed time per iteration (ms): 251.7 | learning rate: 8.817E-05 | global batch size:    32 | lm loss: 8.570069E-05 | loss scale: 67108864.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.93 | backward-params-all-reduce: 62.72 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.01 | optimizer-unscale-and-check-inf: 15.76 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 95.27 | batch-generator: 8.17
 iteration    13300/  200000 | consumed samples:       425600 | elapsed time per iteration (ms): 254.6 | learning rate: 8.808E-05 | global batch size:    32 | lm loss: 7.562949E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.10 | backward-params-all-reduce: 63.70 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 16.68 | optimizer-clip-main-grad: 7.28 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 96.00 | batch-generator: 8.02
 iteration    13400/  200000 | consumed samples:       428800 | elapsed time per iteration (ms): 261.7 | learning rate: 8.799E-05 | global batch size:    32 | lm loss: 8.011111E-05 | loss scale: 67108864.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.50 | backward-params-all-reduce: 63.50 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.91 | optimizer-unscale-and-check-inf: 16.55 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 13.64 | optimizer: 97.42 | batch-generator: 8.04
 iteration    13500/  200000 | consumed samples:       432000 | elapsed time per iteration (ms): 253.6 | learning rate: 8.790E-05 | global batch size:    32 | lm loss: 7.830413E-05 | loss scale: 67108864.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.41 | backward-params-all-reduce: 62.14 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 17.73 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 97.13 | batch-generator: 8.04
-------------------------------------------------------------------------------------------------
 validation loss at iteration 13500 | lm loss value: 7.904798E-05 | lm loss PPL: 1.000079E+00 | 
 at iteration 13500, match long value: 0.006476417360652864 | match short value: 0.00860225413580336 
------------------------------------------------------------------------------------------------------
 iteration    13600/  200000 | consumed samples:       435200 | elapsed time per iteration (ms): 263.9 | learning rate: 8.781E-05 | global batch size:    32 | lm loss: 8.892001E-05 | loss scale: 67108864.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.07 | backward-params-all-reduce: 62.47 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 16.91 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 96.24 | batch-generator: 14.28
 iteration    13700/  200000 | consumed samples:       438400 | elapsed time per iteration (ms): 254.7 | learning rate: 8.772E-05 | global batch size:    32 | lm loss: 7.455702E-05 | loss scale: 134217728.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.52 | backward-params-all-reduce: 63.27 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.87 | optimizer-unscale-and-check-inf: 17.25 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 96.64 | batch-generator: 7.92
 iteration    13800/  200000 | consumed samples:       441600 | elapsed time per iteration (ms): 253.6 | learning rate: 8.763E-05 | global batch size:    32 | lm loss: 7.497571E-05 | loss scale: 134217728.0 | grad norm: 0.007 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.44 | backward-params-all-reduce: 64.27 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.87 | optimizer-unscale-and-check-inf: 16.47 | optimizer-clip-main-grad: 7.20 | optimizer-copy-main-to-model-params: 12.07 | optimizer: 95.13 | batch-generator: 7.93
 iteration    13900/  200000 | consumed samples:       444800 | elapsed time per iteration (ms): 254.4 | learning rate: 8.754E-05 | global batch size:    32 | lm loss: 7.445254E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.19 | backward-params-all-reduce: 62.85 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.91 | optimizer-unscale-and-check-inf: 17.69 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.19 | optimizer: 97.07 | batch-generator: 8.05
 iteration    14000/  200000 | consumed samples:       448000 | elapsed time per iteration (ms): 254.4 | learning rate: 8.745E-05 | global batch size:    32 | lm loss: 6.997975E-05 | loss scale: 134217728.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.30 | backward-params-all-reduce: 61.86 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 17.07 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 96.40 | batch-generator: 8.16
-------------------------------------------------------------------------------------------------
 validation loss at iteration 14000 | lm loss value: 8.673936E-05 | lm loss PPL: 1.000087E+00 | 
 at iteration 14000, match long value: 0.005632387314937408 | match short value: -0.009006322436388612 
--------------------------------------------------------------------------------------------------------
 iteration    14100/  200000 | consumed samples:       451200 | elapsed time per iteration (ms): 285.5 | learning rate: 8.736E-05 | global batch size:    32 | lm loss: 8.220960E-05 | loss scale: 67108864.0 | grad norm: 0.010 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.02 | backward-params-all-reduce: 63.52 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.12 | optimizer-unscale-and-check-inf: 20.15 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 13.40 | optimizer: 100.49 | batch-generator: 21.61
 iteration    14200/  200000 | consumed samples:       454400 | elapsed time per iteration (ms): 258.4 | learning rate: 8.726E-05 | global batch size:    32 | lm loss: 7.312521E-05 | loss scale: 67108864.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.13 | backward-params-all-reduce: 63.56 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.19 | optimizer-unscale-and-check-inf: 20.04 | optimizer-clip-main-grad: 7.37 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 99.76 | batch-generator: 7.94
 iteration    14300/  200000 | consumed samples:       457600 | elapsed time per iteration (ms): 259.3 | learning rate: 8.717E-05 | global batch size:    32 | lm loss: 7.279892E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.08 | backward-params-all-reduce: 66.09 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.12 | optimizer-unscale-and-check-inf: 20.77 | optimizer-clip-main-grad: 7.39 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 100.46 | batch-generator: 8.05
 iteration    14400/  200000 | consumed samples:       460800 | elapsed time per iteration (ms): 264.3 | learning rate: 8.708E-05 | global batch size:    32 | lm loss: 6.783302E-05 | loss scale: 67108864.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.10 | backward-params-all-reduce: 63.52 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.10 | optimizer-unscale-and-check-inf: 19.03 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.65 | batch-generator: 8.06
 iteration    14500/  200000 | consumed samples:       464000 | elapsed time per iteration (ms): 259.8 | learning rate: 8.699E-05 | global batch size:    32 | lm loss: 7.111533E-05 | loss scale: 67108864.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.04 | backward-params-all-reduce: 66.60 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.95 | optimizer-unscale-and-check-inf: 21.29 | optimizer-clip-main-grad: 7.40 | optimizer-copy-main-to-model-params: 12.19 | optimizer: 100.82 | batch-generator: 8.02
-------------------------------------------------------------------------------------------------
 validation loss at iteration 14500 | lm loss value: 7.918688E-05 | lm loss PPL: 1.000079E+00 | 
 at iteration 14500, match long value: 0.007190713510384619 | match short value: -0.019527978435993285 
--------------------------------------------------------------------------------------------------------
 iteration    14600/  200000 | consumed samples:       467200 | elapsed time per iteration (ms): 269.4 | learning rate: 8.690E-05 | global batch size:    32 | lm loss: 7.380549E-05 | loss scale: 67108864.0 | grad norm: 0.015 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.21 | backward-params-all-reduce: 65.58 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.89 | optimizer-unscale-and-check-inf: 20.53 | optimizer-clip-main-grad: 7.38 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 99.96 | batch-generator: 14.59
 iteration    14700/  200000 | consumed samples:       470400 | elapsed time per iteration (ms): 260.6 | learning rate: 8.681E-05 | global batch size:    32 | lm loss: 6.998019E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.73 | backward-params-all-reduce: 63.35 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.99 | optimizer-unscale-and-check-inf: 20.96 | optimizer-clip-main-grad: 7.39 | optimizer-copy-main-to-model-params: 12.22 | optimizer: 100.57 | batch-generator: 8.29
 iteration    14800/  200000 | consumed samples:       473600 | elapsed time per iteration (ms): 267.0 | learning rate: 8.672E-05 | global batch size:    32 | lm loss: 6.076205E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.67 | backward-params-all-reduce: 63.94 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.05 | optimizer-unscale-and-check-inf: 19.75 | optimizer-clip-main-grad: 7.41 | optimizer-copy-main-to-model-params: 13.59 | optimizer: 100.80 | batch-generator: 8.27
 iteration    14900/  200000 | consumed samples:       476800 | elapsed time per iteration (ms): 256.4 | learning rate: 8.663E-05 | global batch size:    32 | lm loss: 6.932076E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.73 | backward-params-all-reduce: 64.53 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.11 | optimizer-unscale-and-check-inf: 19.41 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 99.01 | batch-generator: 8.14
 iteration    15000/  200000 | consumed samples:       480000 | elapsed time per iteration (ms): 257.4 | learning rate: 8.654E-05 | global batch size:    32 | lm loss: 6.495927E-05 | loss scale: 67108864.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.91 | backward-params-all-reduce: 63.60 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.30 | optimizer-unscale-and-check-inf: 19.21 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 99.00 | batch-generator: 8.08
-------------------------------------------------------------------------------------------------
 validation loss at iteration 15000 | lm loss value: 7.438125E-05 | lm loss PPL: 1.000074E+00 | 
 at iteration 15000, match long value: 0.006128545502984964 | match short value: 0.01190838978560674 
------------------------------------------------------------------------------------------------------
 iteration    15100/  200000 | consumed samples:       483200 | elapsed time per iteration (ms): 267.8 | learning rate: 8.644E-05 | global batch size:    32 | lm loss: 6.733441E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.14 | backward-params-all-reduce: 64.02 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.00 | optimizer-unscale-and-check-inf: 19.60 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 99.11 | batch-generator: 14.55
 iteration    15200/  200000 | consumed samples:       486400 | elapsed time per iteration (ms): 258.9 | learning rate: 8.635E-05 | global batch size:    32 | lm loss: 7.175343E-05 | loss scale: 134217728.0 | grad norm: 0.014 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.19 | backward-params-all-reduce: 63.83 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.24 | optimizer-unscale-and-check-inf: 19.04 | optimizer-clip-main-grad: 7.38 | optimizer-copy-main-to-model-params: 12.19 | optimizer: 98.86 | batch-generator: 8.20
 iteration    15300/  200000 | consumed samples:       489600 | elapsed time per iteration (ms): 257.9 | learning rate: 8.626E-05 | global batch size:    32 | lm loss: 7.123641E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.16 | backward-params-all-reduce: 63.96 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.30 | optimizer-unscale-and-check-inf: 20.17 | optimizer-clip-main-grad: 7.41 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 100.06 | batch-generator: 8.01
 iteration    15400/  200000 | consumed samples:       492800 | elapsed time per iteration (ms): 254.0 | learning rate: 8.617E-05 | global batch size:    32 | lm loss: 6.294495E-05 | loss scale: 134217728.0 | grad norm: 0.006 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.24 | backward-params-all-reduce: 62.45 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 19.97 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.05 | optimizer: 98.73 | batch-generator: 8.19
 iteration    15500/  200000 | consumed samples:       496000 | elapsed time per iteration (ms): 257.9 | learning rate: 8.608E-05 | global batch size:    32 | lm loss: 6.720989E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.24 | backward-params-all-reduce: 62.87 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.90 | optimizer-unscale-and-check-inf: 20.02 | optimizer-clip-main-grad: 7.38 | optimizer-copy-main-to-model-params: 13.73 | optimizer: 101.04 | batch-generator: 8.12
-------------------------------------------------------------------------------------------------
 validation loss at iteration 15500 | lm loss value: 7.327267E-05 | lm loss PPL: 1.000073E+00 | 
 at iteration 15500, match long value: 0.006790344749226475 | match short value: -0.009922865024047663 
--------------------------------------------------------------------------------------------------------
 iteration    15600/  200000 | consumed samples:       499200 | elapsed time per iteration (ms): 274.7 | learning rate: 8.599E-05 | global batch size:    32 | lm loss: 6.536708E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.97 | backward-params-all-reduce: 63.65 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.97 | optimizer-unscale-and-check-inf: 18.75 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.23 | batch-generator: 15.33
 iteration    15700/  200000 | consumed samples:       502400 | elapsed time per iteration (ms): 256.2 | learning rate: 8.590E-05 | global batch size:    32 | lm loss: 7.285119E-05 | loss scale: 134217728.0 | grad norm: 0.010 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.08 | backward-params-all-reduce: 64.04 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 20.41 | optimizer-clip-main-grad: 7.39 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 99.82 | batch-generator: 8.12
 iteration    15800/  200000 | consumed samples:       505600 | elapsed time per iteration (ms): 258.1 | learning rate: 8.581E-05 | global batch size:    32 | lm loss: 8.534914E-05 | loss scale: 67108864.0 | grad norm: 0.008 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.11 | backward-params-all-reduce: 64.54 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.99 | optimizer-unscale-and-check-inf: 21.32 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.04 | optimizer: 100.22 | batch-generator: 8.10
 iteration    15900/  200000 | consumed samples:       508800 | elapsed time per iteration (ms): 255.5 | learning rate: 8.572E-05 | global batch size:    32 | lm loss: 6.654515E-05 | loss scale: 67108864.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.19 | backward-params-all-reduce: 62.59 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.07 | optimizer-unscale-and-check-inf: 19.11 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.69 | batch-generator: 8.07
 iteration    16000/  200000 | consumed samples:       512000 | elapsed time per iteration (ms): 257.7 | learning rate: 8.563E-05 | global batch size:    32 | lm loss: 7.090880E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.40 | backward-params-all-reduce: 62.68 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.96 | optimizer-unscale-and-check-inf: 19.75 | optimizer-clip-main-grad: 7.42 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 99.34 | batch-generator: 8.05
-------------------------------------------------------------------------------------------------
 validation loss at iteration 16000 | lm loss value: 6.694133E-05 | lm loss PPL: 1.000067E+00 | 
 at iteration 16000, match long value: 0.004656365347008725 | match short value: -0.006938893326582231 
--------------------------------------------------------------------------------------------------------
 iteration    16100/  200000 | consumed samples:       515200 | elapsed time per iteration (ms): 275.4 | learning rate: 8.554E-05 | global batch size:    32 | lm loss: 6.398832E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.27 | backward-params-all-reduce: 63.89 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.91 | optimizer-unscale-and-check-inf: 18.87 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.29 | batch-generator: 16.58
 iteration    16200/  200000 | consumed samples:       518400 | elapsed time per iteration (ms): 257.9 | learning rate: 8.544E-05 | global batch size:    32 | lm loss: 6.320606E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.07 | backward-params-all-reduce: 63.66 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 19.49 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.83 | batch-generator: 7.99
 iteration    16300/  200000 | consumed samples:       521600 | elapsed time per iteration (ms): 267.4 | learning rate: 8.535E-05 | global batch size:    32 | lm loss: 6.068806E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.74 | backward-params-all-reduce: 62.53 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.83 | optimizer-unscale-and-check-inf: 19.12 | optimizer-clip-main-grad: 7.37 | optimizer-copy-main-to-model-params: 13.58 | optimizer: 99.91 | batch-generator: 7.98
 iteration    16400/  200000 | consumed samples:       524800 | elapsed time per iteration (ms): 254.9 | learning rate: 8.526E-05 | global batch size:    32 | lm loss: 6.262591E-05 | loss scale: 67108864.0 | grad norm: 0.010 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.11 | backward-params-all-reduce: 62.52 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.95 | optimizer-unscale-and-check-inf: 19.14 | optimizer-clip-main-grad: 7.39 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.65 | batch-generator: 8.02
 iteration    16500/  200000 | consumed samples:       528000 | elapsed time per iteration (ms): 253.2 | learning rate: 8.517E-05 | global batch size:    32 | lm loss: 6.421274E-05 | loss scale: 67108864.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.11 | backward-params-all-reduce: 62.41 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 18.31 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.64 | batch-generator: 8.00
-------------------------------------------------------------------------------------------------
 validation loss at iteration 16500 | lm loss value: 6.642663E-05 | lm loss PPL: 1.000066E+00 | 
 at iteration 16500, match long value: 0.005813606466764835 | match short value: -0.011864865835936052 
--------------------------------------------------------------------------------------------------------
 iteration    16600/  200000 | consumed samples:       531200 | elapsed time per iteration (ms): 266.9 | learning rate: 8.508E-05 | global batch size:    32 | lm loss: 6.641977E-05 | loss scale: 67108864.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.09 | backward-params-all-reduce: 64.14 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.99 | optimizer-unscale-and-check-inf: 19.37 | optimizer-clip-main-grad: 7.37 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.90 | batch-generator: 13.53
 iteration    16700/  200000 | consumed samples:       534400 | elapsed time per iteration (ms): 253.9 | learning rate: 8.499E-05 | global batch size:    32 | lm loss: 6.077047E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.04 | backward-params-all-reduce: 62.46 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.92 | optimizer-unscale-and-check-inf: 18.06 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.50 | batch-generator: 8.12
 iteration    16800/  200000 | consumed samples:       537600 | elapsed time per iteration (ms): 254.4 | learning rate: 8.490E-05 | global batch size:    32 | lm loss: 6.534823E-05 | loss scale: 134217728.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.04 | backward-params-all-reduce: 63.83 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 18.67 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.02 | batch-generator: 8.09
 iteration    16900/  200000 | consumed samples:       540800 | elapsed time per iteration (ms): 257.8 | learning rate: 8.481E-05 | global batch size:    32 | lm loss: 6.468730E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.80 | backward-params-all-reduce: 63.64 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.26 | optimizer-unscale-and-check-inf: 18.65 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.43 | batch-generator: 8.00
 iteration    17000/  200000 | consumed samples:       544000 | elapsed time per iteration (ms): 266.3 | learning rate: 8.472E-05 | global batch size:    32 | lm loss: 6.042966E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.76 | backward-params-all-reduce: 62.50 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.09 | optimizer-unscale-and-check-inf: 19.25 | optimizer-clip-main-grad: 7.39 | optimizer-copy-main-to-model-params: 13.54 | optimizer: 100.29 | batch-generator: 8.13
-------------------------------------------------------------------------------------------------
 validation loss at iteration 17000 | lm loss value: 6.686539E-05 | lm loss PPL: 1.000067E+00 | 
 at iteration 17000, match long value: 0.005092244054290499 | match short value: -0.027977671673066185 
--------------------------------------------------------------------------------------------------------
 iteration    17100/  200000 | consumed samples:       547200 | elapsed time per iteration (ms): 275.4 | learning rate: 8.462E-05 | global batch size:    32 | lm loss: 6.240158E-05 | loss scale: 134217728.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.96 | backward-params-all-reduce: 65.93 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.02 | optimizer-unscale-and-check-inf: 19.23 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.77 | batch-generator: 13.82
 iteration    17200/  200000 | consumed samples:       550400 | elapsed time per iteration (ms): 258.0 | learning rate: 8.453E-05 | global batch size:    32 | lm loss: 6.069713E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.63 | backward-params-all-reduce: 64.92 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.18 | optimizer-unscale-and-check-inf: 17.35 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.02 | batch-generator: 7.96
 iteration    17300/  200000 | consumed samples:       553600 | elapsed time per iteration (ms): 255.6 | learning rate: 8.444E-05 | global batch size:    32 | lm loss: 6.962299E-05 | loss scale: 134217728.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.08 | backward-params-all-reduce: 63.04 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.91 | optimizer-unscale-and-check-inf: 19.04 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.46 | batch-generator: 8.06
 iteration    17400/  200000 | consumed samples:       556800 | elapsed time per iteration (ms): 260.0 | learning rate: 8.435E-05 | global batch size:    32 | lm loss: 6.353446E-05 | loss scale: 67108864.0 | grad norm: 0.007 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.42 | backward-params-all-reduce: 62.50 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.12 | optimizer-unscale-and-check-inf: 19.53 | optimizer-clip-main-grad: 7.23 | optimizer-copy-main-to-model-params: 11.93 | optimizer: 97.90 | batch-generator: 8.06
 iteration    17500/  200000 | consumed samples:       560000 | elapsed time per iteration (ms): 264.7 | learning rate: 8.426E-05 | global batch size:    32 | lm loss: 6.868062E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.12 | backward-params-all-reduce: 62.44 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.90 | optimizer-unscale-and-check-inf: 21.21 | optimizer-clip-main-grad: 7.37 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 100.64 | batch-generator: 8.19
-------------------------------------------------------------------------------------------------
 validation loss at iteration 17500 | lm loss value: 6.056869E-05 | lm loss PPL: 1.000061E+00 | 
 at iteration 17500, match long value: 0.004985581100985622 | match short value: 0.000988988158197718 
-------------------------------------------------------------------------------------------------------
 iteration    17600/  200000 | consumed samples:       563200 | elapsed time per iteration (ms): 282.2 | learning rate: 8.417E-05 | global batch size:    32 | lm loss: 5.739539E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.08 | backward-params-all-reduce: 64.43 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.40 | optimizer-unscale-and-check-inf: 18.18 | optimizer-clip-main-grad: 7.45 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.20 | batch-generator: 25.12
 iteration    17700/  200000 | consumed samples:       566400 | elapsed time per iteration (ms): 263.4 | learning rate: 8.408E-05 | global batch size:    32 | lm loss: 6.001322E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 56.62 | backward-params-all-reduce: 61.76 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.00 | optimizer-unscale-and-check-inf: 15.06 | optimizer-clip-main-grad: 7.25 | optimizer-copy-main-to-model-params: 13.72 | optimizer: 96.01 | batch-generator: 7.81
 iteration    17800/  200000 | consumed samples:       569600 | elapsed time per iteration (ms): 269.9 | learning rate: 8.399E-05 | global batch size:    32 | lm loss: 6.305661E-05 | loss scale: 67108864.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 55.08 | backward-params-all-reduce: 62.39 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.97 | optimizer-unscale-and-check-inf: 17.66 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 97.07 | batch-generator: 7.89
 iteration    17900/  200000 | consumed samples:       572800 | elapsed time per iteration (ms): 263.9 | learning rate: 8.390E-05 | global batch size:    32 | lm loss: 5.858616E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.05 | backward-params-all-reduce: 64.76 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.82 | optimizer-unscale-and-check-inf: 22.01 | optimizer-clip-main-grad: 7.41 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 101.39 | batch-generator: 8.02
 iteration    18000/  200000 | consumed samples:       576000 | elapsed time per iteration (ms): 259.7 | learning rate: 8.381E-05 | global batch size:    32 | lm loss: 5.964084E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.37 | backward-params-all-reduce: 64.40 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 19.63 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.99 | batch-generator: 8.01
-------------------------------------------------------------------------------------------------
 validation loss at iteration 18000 | lm loss value: 5.953495E-05 | lm loss PPL: 1.000060E+00 | 
 at iteration 18000, match long value: 0.0034548900365235853 | match short value: 0.00816466083579158 
-------------------------------------------------------------------------------------------------------
 iteration    18100/  200000 | consumed samples:       579200 | elapsed time per iteration (ms): 274.1 | learning rate: 8.372E-05 | global batch size:    32 | lm loss: 5.555817E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.10 | backward-params-all-reduce: 65.39 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.57 | optimizer-unscale-and-check-inf: 16.06 | optimizer-clip-main-grad: 7.55 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 96.34 | batch-generator: 14.10
 iteration    18200/  200000 | consumed samples:       582400 | elapsed time per iteration (ms): 259.7 | learning rate: 8.362E-05 | global batch size:    32 | lm loss: 5.595219E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.18 | backward-params-all-reduce: 65.24 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.01 | optimizer-unscale-and-check-inf: 21.01 | optimizer-clip-main-grad: 7.41 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 100.59 | batch-generator: 8.08
 iteration    18300/  200000 | consumed samples:       585600 | elapsed time per iteration (ms): 256.0 | learning rate: 8.353E-05 | global batch size:    32 | lm loss: 5.792126E-05 | loss scale: 67108864.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.99 | backward-params-all-reduce: 62.95 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 19.87 | optimizer-clip-main-grad: 7.38 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 99.24 | batch-generator: 8.14
 iteration    18400/  200000 | consumed samples:       588800 | elapsed time per iteration (ms): 261.2 | learning rate: 8.344E-05 | global batch size:    32 | lm loss: 5.598560E-05 | loss scale: 134217728.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.09 | backward-params-all-reduce: 65.33 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.93 | optimizer-unscale-and-check-inf: 21.50 | optimizer-clip-main-grad: 7.40 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 100.99 | batch-generator: 8.05
 iteration    18500/  200000 | consumed samples:       592000 | elapsed time per iteration (ms): 266.3 | learning rate: 8.335E-05 | global batch size:    32 | lm loss: 6.265325E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.40 | backward-params-all-reduce: 63.69 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.83 | optimizer-unscale-and-check-inf: 19.23 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 13.59 | optimizer: 99.95 | batch-generator: 8.00
-------------------------------------------------------------------------------------------------
 validation loss at iteration 18500 | lm loss value: 6.354386E-05 | lm loss PPL: 1.000064E+00 | 
 at iteration 18500, match long value: 0.0008223274098051151 | match short value: -0.012875532069876874 
---------------------------------------------------------------------------------------------------------
 iteration    18600/  200000 | consumed samples:       595200 | elapsed time per iteration (ms): 269.5 | learning rate: 8.326E-05 | global batch size:    32 | lm loss: 7.057164E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.04 | backward-params-all-reduce: 64.75 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.93 | optimizer-unscale-and-check-inf: 19.46 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 98.86 | batch-generator: 14.75
 iteration    18700/  200000 | consumed samples:       598400 | elapsed time per iteration (ms): 257.1 | learning rate: 8.317E-05 | global batch size:    32 | lm loss: 5.647944E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.93 | backward-params-all-reduce: 64.21 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 19.28 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.62 | batch-generator: 8.06
 iteration    18800/  200000 | consumed samples:       601600 | elapsed time per iteration (ms): 253.1 | learning rate: 8.308E-05 | global batch size:    32 | lm loss: 6.182975E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.06 | backward-params-all-reduce: 62.68 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.83 | optimizer-unscale-and-check-inf: 20.05 | optimizer-clip-main-grad: 7.18 | optimizer-copy-main-to-model-params: 11.91 | optimizer: 98.01 | batch-generator: 7.91
 iteration    18900/  200000 | consumed samples:       604800 | elapsed time per iteration (ms): 255.6 | learning rate: 8.299E-05 | global batch size:    32 | lm loss: 6.514828E-05 | loss scale: 67108864.0 | grad norm: 0.016 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.10 | backward-params-all-reduce: 62.88 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 20.70 | optimizer-clip-main-grad: 7.39 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 100.07 | batch-generator: 8.00
 iteration    19000/  200000 | consumed samples:       608000 | elapsed time per iteration (ms): 255.2 | learning rate: 8.290E-05 | global batch size:    32 | lm loss: 6.185324E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.01 | backward-params-all-reduce: 62.45 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 20.26 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 99.62 | batch-generator: 8.04
-------------------------------------------------------------------------------------------------
 validation loss at iteration 19000 | lm loss value: 6.091855E-05 | lm loss PPL: 1.000061E+00 | 
 at iteration 19000, match long value: 0.006888342485436751 | match short value: 0.024775426589066032 
-------------------------------------------------------------------------------------------------------
 iteration    19100/  200000 | consumed samples:       611200 | elapsed time per iteration (ms): 272.3 | learning rate: 8.281E-05 | global batch size:    32 | lm loss: 5.628190E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.11 | backward-params-all-reduce: 63.45 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.94 | optimizer-unscale-and-check-inf: 19.83 | optimizer-clip-main-grad: 7.40 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 99.33 | batch-generator: 16.91
 iteration    19200/  200000 | consumed samples:       614400 | elapsed time per iteration (ms): 267.8 | learning rate: 8.272E-05 | global batch size:    32 | lm loss: 5.515790E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.73 | backward-params-all-reduce: 62.99 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 13.16 | optimizer-unscale-and-check-inf: 13.80 | optimizer-clip-main-grad: 7.65 | optimizer-copy-main-to-model-params: 13.66 | optimizer: 96.27 | batch-generator: 8.03
 iteration    19300/  200000 | consumed samples:       617600 | elapsed time per iteration (ms): 255.4 | learning rate: 8.262E-05 | global batch size:    32 | lm loss: 5.745310E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.10 | backward-params-all-reduce: 63.29 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.16 | optimizer-unscale-and-check-inf: 18.21 | optimizer-clip-main-grad: 7.41 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.97 | batch-generator: 7.92
 iteration    19400/  200000 | consumed samples:       620800 | elapsed time per iteration (ms): 256.8 | learning rate: 8.253E-05 | global batch size:    32 | lm loss: 6.439643E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.02 | backward-params-all-reduce: 62.35 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 13.26 | optimizer-unscale-and-check-inf: 17.71 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 98.40 | batch-generator: 7.81
 iteration    19500/  200000 | consumed samples:       624000 | elapsed time per iteration (ms): 256.4 | learning rate: 8.244E-05 | global batch size:    32 | lm loss: 5.937604E-05 | loss scale: 67108864.0 | grad norm: 0.010 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.01 | backward-params-all-reduce: 62.65 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.05 | optimizer-unscale-and-check-inf: 19.97 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 99.48 | batch-generator: 7.82
-------------------------------------------------------------------------------------------------
 validation loss at iteration 19500 | lm loss value: 7.898835E-05 | lm loss PPL: 1.000079E+00 | 
 at iteration 19500, match long value: 0.007015894929559641 | match short value: -0.00424118368984006 
-------------------------------------------------------------------------------------------------------
 iteration    19600/  200000 | consumed samples:       627200 | elapsed time per iteration (ms): 267.9 | learning rate: 8.235E-05 | global batch size:    32 | lm loss: 5.497758E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.15 | backward-params-all-reduce: 63.12 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 20.25 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 99.62 | batch-generator: 14.35
 iteration    19700/  200000 | consumed samples:       630400 | elapsed time per iteration (ms): 255.0 | learning rate: 8.226E-05 | global batch size:    32 | lm loss: 5.162123E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.08 | backward-params-all-reduce: 62.28 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.12 | optimizer-unscale-and-check-inf: 19.08 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.69 | batch-generator: 7.84
 iteration    19800/  200000 | consumed samples:       633600 | elapsed time per iteration (ms): 255.6 | learning rate: 8.217E-05 | global batch size:    32 | lm loss: 5.308630E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.07 | backward-params-all-reduce: 62.70 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.98 | optimizer-unscale-and-check-inf: 18.84 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.27 | batch-generator: 7.94
 iteration    19900/  200000 | consumed samples:       636800 | elapsed time per iteration (ms): 263.3 | learning rate: 8.208E-05 | global batch size:    32 | lm loss: 5.507422E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.45 | backward-params-all-reduce: 62.44 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.19 | optimizer-unscale-and-check-inf: 19.66 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 13.60 | optimizer: 100.80 | batch-generator: 7.98
 iteration    20000/  200000 | consumed samples:       640000 | elapsed time per iteration (ms): 265.9 | learning rate: 8.199E-05 | global batch size:    32 | lm loss: 5.129917E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.55 | backward-params-all-reduce: 63.71 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.95 | optimizer-unscale-and-check-inf: 19.65 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 99.11 | batch-generator: 8.02
-------------------------------------------------------------------------------------------------
 validation loss at iteration 20000 | lm loss value: 5.384025E-05 | lm loss PPL: 1.000054E+00 | 
 at iteration 20000, match long value: 0.005614450384431502 | match short value: 0.03731200168115949 
------------------------------------------------------------------------------------------------------
 iteration    20100/  200000 | consumed samples:       643200 | elapsed time per iteration (ms): 267.5 | learning rate: 8.190E-05 | global batch size:    32 | lm loss: 5.598148E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.21 | backward-params-all-reduce: 63.50 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.96 | optimizer-unscale-and-check-inf: 19.13 | optimizer-clip-main-grad: 7.23 | optimizer-copy-main-to-model-params: 11.94 | optimizer: 97.30 | batch-generator: 14.30
 iteration    20200/  200000 | consumed samples:       646400 | elapsed time per iteration (ms): 252.8 | learning rate: 8.181E-05 | global batch size:    32 | lm loss: 5.668776E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.20 | backward-params-all-reduce: 62.32 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.54 | optimizer-unscale-and-check-inf: 14.80 | optimizer-clip-main-grad: 7.52 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 95.04 | batch-generator: 8.14
 iteration    20300/  200000 | consumed samples:       649600 | elapsed time per iteration (ms): 255.1 | learning rate: 8.172E-05 | global batch size:    32 | lm loss: 6.006235E-05 | loss scale: 67108864.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.07 | backward-params-all-reduce: 63.20 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.65 | optimizer-unscale-and-check-inf: 17.23 | optimizer-clip-main-grad: 7.37 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.40 | batch-generator: 8.03
 iteration    20400/  200000 | consumed samples:       652800 | elapsed time per iteration (ms): 255.7 | learning rate: 8.162E-05 | global batch size:    32 | lm loss: 5.997133E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.15 | backward-params-all-reduce: 64.10 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 18.74 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 98.11 | batch-generator: 8.00
 iteration    20500/  200000 | consumed samples:       656000 | elapsed time per iteration (ms): 255.8 | learning rate: 8.153E-05 | global batch size:    32 | lm loss: 5.806594E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.05 | backward-params-all-reduce: 63.72 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 19.02 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 98.34 | batch-generator: 7.96
-------------------------------------------------------------------------------------------------
 validation loss at iteration 20500 | lm loss value: 5.770736E-05 | lm loss PPL: 1.000058E+00 | 
 at iteration 20500, match long value: 0.0073038775558253765 | match short value: 0.06169005625396952 
-------------------------------------------------------------------------------------------------------
 iteration    20600/  200000 | consumed samples:       659200 | elapsed time per iteration (ms): 271.7 | learning rate: 8.144E-05 | global batch size:    32 | lm loss: 5.281864E-05 | loss scale: 67108864.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.74 | backward-params-all-reduce: 63.15 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.89 | optimizer-unscale-and-check-inf: 19.58 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.14 | optimizer: 98.95 | batch-generator: 16.47
 iteration    20700/  200000 | consumed samples:       662400 | elapsed time per iteration (ms): 266.8 | learning rate: 8.135E-05 | global batch size:    32 | lm loss: 5.304739E-05 | loss scale: 67108864.0 | grad norm: 0.013 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.70 | backward-params-all-reduce: 62.09 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.05 | optimizer-unscale-and-check-inf: 19.60 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 13.58 | optimizer: 100.55 | batch-generator: 8.01
 iteration    20800/  200000 | consumed samples:       665600 | elapsed time per iteration (ms): 263.4 | learning rate: 8.126E-05 | global batch size:    32 | lm loss: 5.331791E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.42 | backward-params-all-reduce: 64.57 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.00 | optimizer-unscale-and-check-inf: 19.67 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 99.18 | batch-generator: 8.00
 iteration    20900/  200000 | consumed samples:       668800 | elapsed time per iteration (ms): 259.4 | learning rate: 8.117E-05 | global batch size:    32 | lm loss: 5.156939E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.21 | backward-params-all-reduce: 65.18 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 19.79 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 99.14 | batch-generator: 7.95
 iteration    21000/  200000 | consumed samples:       672000 | elapsed time per iteration (ms): 257.0 | learning rate: 8.108E-05 | global batch size:    32 | lm loss: 5.829480E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.13 | backward-params-all-reduce: 63.89 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.14 | optimizer-unscale-and-check-inf: 19.10 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.75 | batch-generator: 7.98
-------------------------------------------------------------------------------------------------
 validation loss at iteration 21000 | lm loss value: 5.415575E-05 | lm loss PPL: 1.000054E+00 | 
 at iteration 21000, match long value: 0.008251873967696517 | match short value: 0.012148238321843538 
-------------------------------------------------------------------------------------------------------
 iteration    21100/  200000 | consumed samples:       675200 | elapsed time per iteration (ms): 271.3 | learning rate: 8.099E-05 | global batch size:    32 | lm loss: 5.339563E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.13 | backward-params-all-reduce: 63.35 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.89 | optimizer-unscale-and-check-inf: 20.84 | optimizer-clip-main-grad: 7.43 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 100.33 | batch-generator: 17.59
 iteration    21200/  200000 | consumed samples:       678400 | elapsed time per iteration (ms): 255.5 | learning rate: 8.090E-05 | global batch size:    32 | lm loss: 4.789048E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.26 | backward-params-all-reduce: 62.51 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.83 | optimizer-unscale-and-check-inf: 20.82 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 100.18 | batch-generator: 8.10
 iteration    21300/  200000 | consumed samples:       681600 | elapsed time per iteration (ms): 254.0 | learning rate: 8.081E-05 | global batch size:    32 | lm loss: 5.455955E-05 | loss scale: 67108864.0 | grad norm: 0.008 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.25 | backward-params-all-reduce: 63.03 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 20.02 | optimizer-clip-main-grad: 7.20 | optimizer-copy-main-to-model-params: 11.94 | optimizer: 98.04 | batch-generator: 8.05
 iteration    21400/  200000 | consumed samples:       684800 | elapsed time per iteration (ms): 267.3 | learning rate: 8.072E-05 | global batch size:    32 | lm loss: 4.961278E-05 | loss scale: 67108864.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.84 | backward-params-all-reduce: 62.95 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.88 | optimizer-unscale-and-check-inf: 20.29 | optimizer-clip-main-grad: 7.39 | optimizer-copy-main-to-model-params: 13.63 | optimizer: 101.18 | batch-generator: 8.04
 iteration    21500/  200000 | consumed samples:       688000 | elapsed time per iteration (ms): 256.8 | learning rate: 8.062E-05 | global batch size:    32 | lm loss: 4.782526E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.23 | backward-params-all-reduce: 63.16 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.04 | optimizer-unscale-and-check-inf: 21.28 | optimizer-clip-main-grad: 7.38 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 100.88 | batch-generator: 8.16
-------------------------------------------------------------------------------------------------
 validation loss at iteration 21500 | lm loss value: 5.800679E-05 | lm loss PPL: 1.000058E+00 | 
 at iteration 21500, match long value: 0.006799287915294638 | match short value: 0.006768384572802991 
-------------------------------------------------------------------------------------------------------
 iteration    21600/  200000 | consumed samples:       691200 | elapsed time per iteration (ms): 269.2 | learning rate: 8.053E-05 | global batch size:    32 | lm loss: 4.710309E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.86 | backward-params-all-reduce: 62.87 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.94 | optimizer-unscale-and-check-inf: 20.67 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 100.09 | batch-generator: 13.88
 iteration    21700/  200000 | consumed samples:       694400 | elapsed time per iteration (ms): 258.9 | learning rate: 8.044E-05 | global batch size:    32 | lm loss: 4.770754E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.07 | backward-params-all-reduce: 63.38 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.19 | optimizer-unscale-and-check-inf: 18.84 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.55 | batch-generator: 7.94
 iteration    21800/  200000 | consumed samples:       697600 | elapsed time per iteration (ms): 255.3 | learning rate: 8.035E-05 | global batch size:    32 | lm loss: 5.159236E-05 | loss scale: 67108864.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.63 | backward-params-all-reduce: 61.68 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.94 | optimizer-unscale-and-check-inf: 20.23 | optimizer-clip-main-grad: 7.38 | optimizer-copy-main-to-model-params: 12.19 | optimizer: 99.75 | batch-generator: 7.99
 iteration    21900/  200000 | consumed samples:       700800 | elapsed time per iteration (ms): 254.8 | learning rate: 8.026E-05 | global batch size:    32 | lm loss: 4.748707E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.15 | backward-params-all-reduce: 62.03 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 20.60 | optimizer-clip-main-grad: 7.39 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 100.00 | batch-generator: 8.11
 iteration    22000/  200000 | consumed samples:       704000 | elapsed time per iteration (ms): 254.9 | learning rate: 8.017E-05 | global batch size:    32 | lm loss: 4.974124E-05 | loss scale: 67108864.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.59 | backward-params-all-reduce: 62.28 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.89 | optimizer-unscale-and-check-inf: 19.80 | optimizer-clip-main-grad: 7.37 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 99.23 | batch-generator: 8.02
-------------------------------------------------------------------------------------------------
 validation loss at iteration 22000 | lm loss value: 5.395243E-05 | lm loss PPL: 1.000054E+00 | 
 at iteration 22000, match long value: 0.008007302817025459 | match short value: -0.01905329649375194 
-------------------------------------------------------------------------------------------------------
 iteration    22100/  200000 | consumed samples:       707200 | elapsed time per iteration (ms): 272.7 | learning rate: 8.008E-05 | global batch size:    32 | lm loss: 5.207122E-05 | loss scale: 67108864.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.42 | backward-params-all-reduce: 62.27 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 19.15 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 13.59 | optimizer: 99.91 | batch-generator: 17.80
 iteration    22200/  200000 | consumed samples:       710400 | elapsed time per iteration (ms): 262.9 | learning rate: 7.999E-05 | global batch size:    32 | lm loss: 6.116937E-05 | loss scale: 67108864.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.50 | backward-params-all-reduce: 63.12 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 19.68 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.99 | batch-generator: 7.97
 iteration    22300/  200000 | consumed samples:       713600 | elapsed time per iteration (ms): 254.0 | learning rate: 7.990E-05 | global batch size:    32 | lm loss: 4.934849E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.24 | backward-params-all-reduce: 62.68 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.89 | optimizer-unscale-and-check-inf: 19.51 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.89 | batch-generator: 7.89
 iteration    22400/  200000 | consumed samples:       716800 | elapsed time per iteration (ms): 257.0 | learning rate: 7.980E-05 | global batch size:    32 | lm loss: 4.686916E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.29 | backward-params-all-reduce: 63.19 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.89 | optimizer-unscale-and-check-inf: 19.12 | optimizer-clip-main-grad: 7.37 | optimizer-copy-main-to-model-params: 12.19 | optimizer: 98.57 | batch-generator: 7.98
 iteration    22500/  200000 | consumed samples:       720000 | elapsed time per iteration (ms): 255.1 | learning rate: 7.971E-05 | global batch size:    32 | lm loss: 4.790605E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.04 | backward-params-all-reduce: 61.96 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 19.25 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.60 | batch-generator: 7.91
-------------------------------------------------------------------------------------------------
 validation loss at iteration 22500 | lm loss value: 5.351189E-05 | lm loss PPL: 1.000054E+00 | 
 at iteration 22500, match long value: 0.0033841231677090504 | match short value: -0.020227410213924244 
---------------------------------------------------------------------------------------------------------
 iteration    22600/  200000 | consumed samples:       723200 | elapsed time per iteration (ms): 268.4 | learning rate: 7.962E-05 | global batch size:    32 | lm loss: 5.382743E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.19 | backward-params-all-reduce: 63.37 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.01 | optimizer-unscale-and-check-inf: 18.60 | optimizer-clip-main-grad: 7.15 | optimizer-copy-main-to-model-params: 11.93 | optimizer: 96.72 | batch-generator: 13.81
 iteration    22700/  200000 | consumed samples:       726400 | elapsed time per iteration (ms): 253.6 | learning rate: 7.953E-05 | global batch size:    32 | lm loss: 4.843843E-05 | loss scale: 67108864.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.05 | backward-params-all-reduce: 62.81 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 17.24 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 96.56 | batch-generator: 8.02
 iteration    22800/  200000 | consumed samples:       729600 | elapsed time per iteration (ms): 256.7 | learning rate: 7.944E-05 | global batch size:    32 | lm loss: 4.904001E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.07 | backward-params-all-reduce: 62.69 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.31 | optimizer-unscale-and-check-inf: 18.95 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.76 | batch-generator: 7.92
 iteration    22900/  200000 | consumed samples:       732800 | elapsed time per iteration (ms): 264.4 | learning rate: 7.935E-05 | global batch size:    32 | lm loss: 5.213238E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.76 | backward-params-all-reduce: 62.04 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.21 | optimizer-unscale-and-check-inf: 18.67 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 13.62 | optimizer: 99.82 | batch-generator: 7.92
 iteration    23000/  200000 | consumed samples:       736000 | elapsed time per iteration (ms): 255.9 | learning rate: 7.926E-05 | global batch size:    32 | lm loss: 4.610702E-05 | loss scale: 67108864.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.04 | backward-params-all-reduce: 62.59 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.90 | optimizer-unscale-and-check-inf: 19.96 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 99.36 | batch-generator: 7.93
-------------------------------------------------------------------------------------------------
 validation loss at iteration 23000 | lm loss value: 5.912409E-05 | lm loss PPL: 1.000059E+00 | 
 at iteration 23000, match long value: -0.004064764637181343 | match short value: -0.03721919826915813 
--------------------------------------------------------------------------------------------------------
 iteration    23100/  200000 | consumed samples:       739200 | elapsed time per iteration (ms): 265.7 | learning rate: 7.917E-05 | global batch size:    32 | lm loss: 4.489732E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.09 | backward-params-all-reduce: 63.01 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.97 | optimizer-unscale-and-check-inf: 17.74 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.15 | batch-generator: 14.27
 iteration    23200/  200000 | consumed samples:       742400 | elapsed time per iteration (ms): 256.6 | learning rate: 7.908E-05 | global batch size:    32 | lm loss: 4.946021E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.03 | backward-params-all-reduce: 63.09 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.97 | optimizer-unscale-and-check-inf: 17.69 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 97.16 | batch-generator: 8.07
 iteration    23300/  200000 | consumed samples:       745600 | elapsed time per iteration (ms): 254.2 | learning rate: 7.899E-05 | global batch size:    32 | lm loss: 4.831385E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.38 | backward-params-all-reduce: 62.52 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.96 | optimizer-unscale-and-check-inf: 17.80 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 97.26 | batch-generator: 8.02
 iteration    23400/  200000 | consumed samples:       748800 | elapsed time per iteration (ms): 254.1 | learning rate: 7.890E-05 | global batch size:    32 | lm loss: 5.311719E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.17 | backward-params-all-reduce: 62.57 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 18.37 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.73 | batch-generator: 7.97
 iteration    23500/  200000 | consumed samples:       752000 | elapsed time per iteration (ms): 251.8 | learning rate: 7.880E-05 | global batch size:    32 | lm loss: 4.951873E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.09 | backward-params-all-reduce: 61.96 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.03 | optimizer-unscale-and-check-inf: 17.25 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 96.80 | batch-generator: 7.98
-------------------------------------------------------------------------------------------------
 validation loss at iteration 23500 | lm loss value: 5.305619E-05 | lm loss PPL: 1.000053E+00 | 
 at iteration 23500, match long value: 0.0037732816221843117 | match short value: 0.00223861246389141 
-------------------------------------------------------------------------------------------------------
 iteration    23600/  200000 | consumed samples:       755200 | elapsed time per iteration (ms): 276.3 | learning rate: 7.871E-05 | global batch size:    32 | lm loss: 4.859340E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 53.29 | backward-params-all-reduce: 61.92 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.26 | optimizer-unscale-and-check-inf: 17.48 | optimizer-clip-main-grad: 7.16 | optimizer-copy-main-to-model-params: 13.45 | optimizer: 97.38 | batch-generator: 18.41
 iteration    23700/  200000 | consumed samples:       758400 | elapsed time per iteration (ms): 253.7 | learning rate: 7.862E-05 | global batch size:    32 | lm loss: 4.727597E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.18 | backward-params-all-reduce: 62.08 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.91 | optimizer-unscale-and-check-inf: 18.92 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.35 | batch-generator: 7.96
 iteration    23800/  200000 | consumed samples:       761600 | elapsed time per iteration (ms): 253.7 | learning rate: 7.853E-05 | global batch size:    32 | lm loss: 4.252710E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.17 | backward-params-all-reduce: 62.78 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 17.98 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.32 | batch-generator: 7.94
 iteration    23900/  200000 | consumed samples:       764800 | elapsed time per iteration (ms): 256.7 | learning rate: 7.844E-05 | global batch size:    32 | lm loss: 4.404779E-05 | loss scale: 67108864.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.17 | backward-params-all-reduce: 64.29 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.95 | optimizer-unscale-and-check-inf: 18.46 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.93 | batch-generator: 7.90
 iteration    24000/  200000 | consumed samples:       768000 | elapsed time per iteration (ms): 256.2 | learning rate: 7.835E-05 | global batch size:    32 | lm loss: 4.412825E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.02 | backward-params-all-reduce: 63.39 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.63 | optimizer-unscale-and-check-inf: 17.02 | optimizer-clip-main-grad: 7.45 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.26 | batch-generator: 8.07
-------------------------------------------------------------------------------------------------
 validation loss at iteration 24000 | lm loss value: 4.960861E-05 | lm loss PPL: 1.000050E+00 | 
 at iteration 24000, match long value: 0.007723715372129153 | match short value: 0.024699309418844396 
-------------------------------------------------------------------------------------------------------
 iteration    24100/  200000 | consumed samples:       771200 | elapsed time per iteration (ms): 267.7 | learning rate: 7.826E-05 | global batch size:    32 | lm loss: 4.612754E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.22 | backward-params-all-reduce: 64.07 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.97 | optimizer-unscale-and-check-inf: 17.97 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.42 | batch-generator: 15.32
 iteration    24200/  200000 | consumed samples:       774400 | elapsed time per iteration (ms): 254.8 | learning rate: 7.817E-05 | global batch size:    32 | lm loss: 5.519264E-05 | loss scale: 67108864.0 | grad norm: 0.014 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.12 | backward-params-all-reduce: 62.43 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.88 | optimizer-unscale-and-check-inf: 17.49 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 96.84 | batch-generator: 7.82
 iteration    24300/  200000 | consumed samples:       777600 | elapsed time per iteration (ms): 256.4 | learning rate: 7.808E-05 | global batch size:    32 | lm loss: 5.715200E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.10 | backward-params-all-reduce: 63.30 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 16.80 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 13.68 | optimizer: 97.62 | batch-generator: 7.88
 iteration    24400/  200000 | consumed samples:       780800 | elapsed time per iteration (ms): 262.7 | learning rate: 7.799E-05 | global batch size:    32 | lm loss: 4.634574E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.60 | backward-params-all-reduce: 63.56 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.25 | optimizer-unscale-and-check-inf: 19.02 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.73 | batch-generator: 7.89
 iteration    24500/  200000 | consumed samples:       784000 | elapsed time per iteration (ms): 256.3 | learning rate: 7.789E-05 | global batch size:    32 | lm loss: 4.517197E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.56 | backward-params-all-reduce: 61.80 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 13.10 | optimizer-unscale-and-check-inf: 18.44 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 99.01 | batch-generator: 7.91
-------------------------------------------------------------------------------------------------
 validation loss at iteration 24500 | lm loss value: 5.086154E-05 | lm loss PPL: 1.000051E+00 | 
 at iteration 24500, match long value: 0.00169066119626734 | match short value: -0.03088609924594526 
------------------------------------------------------------------------------------------------------
 iteration    24600/  200000 | consumed samples:       787200 | elapsed time per iteration (ms): 273.3 | learning rate: 7.780E-05 | global batch size:    32 | lm loss: 4.418841E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.24 | backward-params-all-reduce: 62.93 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.83 | optimizer-unscale-and-check-inf: 20.11 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 99.43 | batch-generator: 18.75
 iteration    24700/  200000 | consumed samples:       790400 | elapsed time per iteration (ms): 255.1 | learning rate: 7.771E-05 | global batch size:    32 | lm loss: 3.919413E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.11 | backward-params-all-reduce: 62.58 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.88 | optimizer-unscale-and-check-inf: 14.91 | optimizer-clip-main-grad: 7.25 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 94.19 | batch-generator: 7.67
 iteration    24800/  200000 | consumed samples:       793600 | elapsed time per iteration (ms): 256.1 | learning rate: 7.762E-05 | global batch size:    32 | lm loss: 4.069051E-05 | loss scale: 134217728.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.17 | backward-params-all-reduce: 63.14 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.98 | optimizer-unscale-and-check-inf: 18.20 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.63 | batch-generator: 7.90
 iteration    24900/  200000 | consumed samples:       796800 | elapsed time per iteration (ms): 257.1 | learning rate: 7.753E-05 | global batch size:    32 | lm loss: 4.507159E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.02 | backward-params-all-reduce: 64.10 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.89 | optimizer-unscale-and-check-inf: 19.39 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.73 | batch-generator: 7.96
 iteration    25000/  200000 | consumed samples:       800000 | elapsed time per iteration (ms): 257.6 | learning rate: 7.744E-05 | global batch size:    32 | lm loss: 4.180867E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.12 | backward-params-all-reduce: 63.69 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.91 | optimizer-unscale-and-check-inf: 15.65 | optimizer-clip-main-grad: 7.27 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 94.99 | batch-generator: 7.84
-------------------------------------------------------------------------------------------------
 validation loss at iteration 25000 | lm loss value: 4.454450E-05 | lm loss PPL: 1.000045E+00 | 
 at iteration 25000, match long value: 0.006378430558706886 | match short value: 0.012534215135122314 
-------------------------------------------------------------------------------------------------------
 iteration    25100/  200000 | consumed samples:       803200 | elapsed time per iteration (ms): 282.7 | learning rate: 7.735E-05 | global batch size:    32 | lm loss: 4.107833E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.85 | backward-params-all-reduce: 64.91 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.90 | optimizer-unscale-and-check-inf: 18.27 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 13.60 | optimizer: 99.08 | batch-generator: 17.29
 iteration    25200/  200000 | consumed samples:       806400 | elapsed time per iteration (ms): 255.0 | learning rate: 7.726E-05 | global batch size:    32 | lm loss: 4.152207E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.01 | backward-params-all-reduce: 62.76 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.05 | optimizer-unscale-and-check-inf: 18.46 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.99 | batch-generator: 8.00
 iteration    25300/  200000 | consumed samples:       809600 | elapsed time per iteration (ms): 256.2 | learning rate: 7.717E-05 | global batch size:    32 | lm loss: 5.723707E-05 | loss scale: 67108864.0 | grad norm: 0.011 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.99 | backward-params-all-reduce: 65.52 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 18.88 | optimizer-clip-main-grad: 7.20 | optimizer-copy-main-to-model-params: 11.92 | optimizer: 96.87 | batch-generator: 7.97
 iteration    25400/  200000 | consumed samples:       812800 | elapsed time per iteration (ms): 265.0 | learning rate: 7.708E-05 | global batch size:    32 | lm loss: 4.302297E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.20 | backward-params-all-reduce: 63.27 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.99 | optimizer-unscale-and-check-inf: 19.46 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.95 | batch-generator: 7.99
 iteration    25500/  200000 | consumed samples:       816000 | elapsed time per iteration (ms): 262.9 | learning rate: 7.699E-05 | global batch size:    32 | lm loss: 4.055590E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.09 | backward-params-all-reduce: 64.77 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.35 | optimizer-unscale-and-check-inf: 19.12 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.93 | batch-generator: 7.94
-------------------------------------------------------------------------------------------------
 validation loss at iteration 25500 | lm loss value: 4.968767E-05 | lm loss PPL: 1.000050E+00 | 
 at iteration 25500, match long value: 0.0051008316757610575 | match short value: -0.008664879453363698 
---------------------------------------------------------------------------------------------------------
 iteration    25600/  200000 | consumed samples:       819200 | elapsed time per iteration (ms): 265.5 | learning rate: 7.689E-05 | global batch size:    32 | lm loss: 4.538171E-05 | loss scale: 67108864.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.10 | backward-params-all-reduce: 62.86 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.66 | optimizer-unscale-and-check-inf: 16.87 | optimizer-clip-main-grad: 7.26 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 96.93 | batch-generator: 13.97
 iteration    25700/  200000 | consumed samples:       822400 | elapsed time per iteration (ms): 255.2 | learning rate: 7.680E-05 | global batch size:    32 | lm loss: 4.581844E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.05 | backward-params-all-reduce: 62.14 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 18.00 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.31 | batch-generator: 7.93
 iteration    25800/  200000 | consumed samples:       825600 | elapsed time per iteration (ms): 266.5 | learning rate: 7.671E-05 | global batch size:    32 | lm loss: 4.526191E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.63 | backward-params-all-reduce: 62.78 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 17.95 | optimizer-clip-main-grad: 7.27 | optimizer-copy-main-to-model-params: 13.60 | optimizer: 98.65 | batch-generator: 7.92
 iteration    25900/  200000 | consumed samples:       828800 | elapsed time per iteration (ms): 254.5 | learning rate: 7.662E-05 | global batch size:    32 | lm loss: 3.992010E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.14 | backward-params-all-reduce: 62.28 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 18.16 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.46 | batch-generator: 7.88
 iteration    26000/  200000 | consumed samples:       832000 | elapsed time per iteration (ms): 256.0 | learning rate: 7.653E-05 | global batch size:    32 | lm loss: 4.226687E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.05 | backward-params-all-reduce: 62.02 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.90 | optimizer-unscale-and-check-inf: 18.13 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.47 | batch-generator: 7.85
-------------------------------------------------------------------------------------------------
 validation loss at iteration 26000 | lm loss value: 4.447964E-05 | lm loss PPL: 1.000044E+00 | 
 at iteration 26000, match long value: 0.0017543016088920759 | match short value: -0.016394585881187692 
---------------------------------------------------------------------------------------------------------
 iteration    26100/  200000 | consumed samples:       835200 | elapsed time per iteration (ms): 265.8 | learning rate: 7.644E-05 | global batch size:    32 | lm loss: 4.347528E-05 | loss scale: 67108864.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.02 | backward-params-all-reduce: 62.74 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.14 | optimizer-unscale-and-check-inf: 17.81 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.41 | batch-generator: 13.74
 iteration    26200/  200000 | consumed samples:       838400 | elapsed time per iteration (ms): 254.9 | learning rate: 7.635E-05 | global batch size:    32 | lm loss: 4.269677E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.01 | backward-params-all-reduce: 63.37 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 17.36 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 96.68 | batch-generator: 7.97
 iteration    26300/  200000 | consumed samples:       841600 | elapsed time per iteration (ms): 258.8 | learning rate: 7.626E-05 | global batch size:    32 | lm loss: 3.984977E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.97 | backward-params-all-reduce: 62.80 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.07 | optimizer-unscale-and-check-inf: 18.86 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.37 | batch-generator: 7.87
 iteration    26400/  200000 | consumed samples:       844800 | elapsed time per iteration (ms): 256.9 | learning rate: 7.617E-05 | global batch size:    32 | lm loss: 4.266507E-05 | loss scale: 134217728.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.09 | backward-params-all-reduce: 63.73 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 17.80 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.11 | batch-generator: 7.94
 iteration    26500/  200000 | consumed samples:       848000 | elapsed time per iteration (ms): 260.6 | learning rate: 7.608E-05 | global batch size:    32 | lm loss: 4.556951E-05 | loss scale: 67108864.0 | grad norm: 0.010 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.58 | backward-params-all-reduce: 62.48 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.91 | optimizer-unscale-and-check-inf: 16.32 | optimizer-clip-main-grad: 7.13 | optimizer-copy-main-to-model-params: 13.33 | optimizer: 95.74 | batch-generator: 7.89
-------------------------------------------------------------------------------------------------
 validation loss at iteration 26500 | lm loss value: 5.458554E-05 | lm loss PPL: 1.000055E+00 | 
 at iteration 26500, match long value: -0.00011318670336718379 | match short value: -0.01681357988135727 
----------------------------------------------------------------------------------------------------------
 iteration    26600/  200000 | consumed samples:       851200 | elapsed time per iteration (ms): 273.1 | learning rate: 7.599E-05 | global batch size:    32 | lm loss: 4.573400E-05 | loss scale: 67108864.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.34 | backward-params-all-reduce: 63.41 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 16.89 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 96.18 | batch-generator: 17.50
 iteration    26700/  200000 | consumed samples:       854400 | elapsed time per iteration (ms): 255.3 | learning rate: 7.589E-05 | global batch size:    32 | lm loss: 4.167315E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.00 | backward-params-all-reduce: 62.92 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 17.54 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 96.90 | batch-generator: 7.88
 iteration    26800/  200000 | consumed samples:       857600 | elapsed time per iteration (ms): 253.2 | learning rate: 7.580E-05 | global batch size:    32 | lm loss: 4.136033E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.15 | backward-params-all-reduce: 61.86 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.40 | optimizer-unscale-and-check-inf: 15.92 | optimizer-clip-main-grad: 7.26 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 95.76 | batch-generator: 7.85
 iteration    26900/  200000 | consumed samples:       860800 | elapsed time per iteration (ms): 253.7 | learning rate: 7.571E-05 | global batch size:    32 | lm loss: 3.920047E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.08 | backward-params-all-reduce: 63.15 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.91 | optimizer-unscale-and-check-inf: 15.63 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 95.01 | batch-generator: 7.98
 iteration    27000/  200000 | consumed samples:       864000 | elapsed time per iteration (ms): 267.1 | learning rate: 7.562E-05 | global batch size:    32 | lm loss: 4.410219E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.43 | backward-params-all-reduce: 64.10 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.98 | optimizer-unscale-and-check-inf: 17.96 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.45 | batch-generator: 7.93
-------------------------------------------------------------------------------------------------
 validation loss at iteration 27000 | lm loss value: 5.635290E-05 | lm loss PPL: 1.000056E+00 | 
 at iteration 27000, match long value: 0.002041138469605165 | match short value: 0.02232269430617666 
------------------------------------------------------------------------------------------------------
 iteration    27100/  200000 | consumed samples:       867200 | elapsed time per iteration (ms): 266.0 | learning rate: 7.553E-05 | global batch size:    32 | lm loss: 4.201565E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.03 | backward-params-all-reduce: 63.75 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.97 | optimizer-unscale-and-check-inf: 16.07 | optimizer-clip-main-grad: 7.28 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 95.48 | batch-generator: 14.05
 iteration    27200/  200000 | consumed samples:       870400 | elapsed time per iteration (ms): 253.9 | learning rate: 7.544E-05 | global batch size:    32 | lm loss: 4.066287E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.03 | backward-params-all-reduce: 62.27 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 16.90 | optimizer-clip-main-grad: 7.27 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 96.19 | batch-generator: 7.97
 iteration    27300/  200000 | consumed samples:       873600 | elapsed time per iteration (ms): 271.4 | learning rate: 7.535E-05 | global batch size:    32 | lm loss: 3.884232E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.81 | backward-params-all-reduce: 63.63 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.97 | optimizer-unscale-and-check-inf: 18.53 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 13.56 | optimizer: 99.36 | batch-generator: 7.99
 iteration    27400/  200000 | consumed samples:       876800 | elapsed time per iteration (ms): 257.6 | learning rate: 7.526E-05 | global batch size:    32 | lm loss: 4.126361E-05 | loss scale: 67108864.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.99 | backward-params-all-reduce: 63.50 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.96 | optimizer-unscale-and-check-inf: 18.46 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.90 | batch-generator: 7.94
 iteration    27500/  200000 | consumed samples:       880000 | elapsed time per iteration (ms): 260.7 | learning rate: 7.517E-05 | global batch size:    32 | lm loss: 4.057938E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.19 | backward-params-all-reduce: 62.84 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.89 | optimizer-unscale-and-check-inf: 18.60 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.98 | batch-generator: 7.94
-------------------------------------------------------------------------------------------------
 validation loss at iteration 27500 | lm loss value: 4.356258E-05 | lm loss PPL: 1.000044E+00 | 
 at iteration 27500, match long value: 0.008486607467689266 | match short value: -0.07124843077947411 
-------------------------------------------------------------------------------------------------------
 iteration    27600/  200000 | consumed samples:       883200 | elapsed time per iteration (ms): 266.7 | learning rate: 7.507E-05 | global batch size:    32 | lm loss: 4.112982E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.15 | backward-params-all-reduce: 62.30 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.97 | optimizer-unscale-and-check-inf: 18.35 | optimizer-clip-main-grad: 7.37 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.86 | batch-generator: 14.74
 iteration    27700/  200000 | consumed samples:       886400 | elapsed time per iteration (ms): 257.1 | learning rate: 7.498E-05 | global batch size:    32 | lm loss: 4.318501E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.15 | backward-params-all-reduce: 63.14 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.02 | optimizer-unscale-and-check-inf: 17.38 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 96.89 | batch-generator: 8.04
 iteration    27800/  200000 | consumed samples:       889600 | elapsed time per iteration (ms): 258.9 | learning rate: 7.489E-05 | global batch size:    32 | lm loss: 4.028536E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.50 | backward-params-all-reduce: 62.87 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.21 | optimizer-unscale-and-check-inf: 18.54 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.22 | batch-generator: 7.98
 iteration    27900/  200000 | consumed samples:       892800 | elapsed time per iteration (ms): 256.0 | learning rate: 7.480E-05 | global batch size:    32 | lm loss: 3.803995E-05 | loss scale: 134217728.0 | grad norm: 0.009 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.02 | backward-params-all-reduce: 63.54 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 17.97 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.31 | batch-generator: 7.95
 iteration    28000/  200000 | consumed samples:       896000 | elapsed time per iteration (ms): 265.4 | learning rate: 7.471E-05 | global batch size:    32 | lm loss: 4.132549E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.70 | backward-params-all-reduce: 64.50 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.92 | optimizer-unscale-and-check-inf: 16.21 | optimizer-clip-main-grad: 7.26 | optimizer-copy-main-to-model-params: 13.59 | optimizer: 96.98 | batch-generator: 8.23
-------------------------------------------------------------------------------------------------
 validation loss at iteration 28000 | lm loss value: 4.427883E-05 | lm loss PPL: 1.000044E+00 | 
 at iteration 28000, match long value: 0.0027032986096586814 | match short value: 0.027805488284142667 
--------------------------------------------------------------------------------------------------------
 iteration    28100/  200000 | consumed samples:       899200 | elapsed time per iteration (ms): 274.9 | learning rate: 7.462E-05 | global batch size:    32 | lm loss: 3.890705E-05 | loss scale: 134217728.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.20 | backward-params-all-reduce: 63.11 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 20.48 | optimizer-clip-main-grad: 7.39 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 99.93 | batch-generator: 22.79
 iteration    28200/  200000 | consumed samples:       902400 | elapsed time per iteration (ms): 253.4 | learning rate: 7.453E-05 | global batch size:    32 | lm loss: 3.814301E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.07 | backward-params-all-reduce: 62.15 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.90 | optimizer-unscale-and-check-inf: 19.18 | optimizer-clip-main-grad: 7.35 | optimizer-copy-main-to-model-params: 12.22 | optimizer: 98.65 | batch-generator: 8.13
 iteration    28300/  200000 | consumed samples:       905600 | elapsed time per iteration (ms): 254.0 | learning rate: 7.444E-05 | global batch size:    32 | lm loss: 3.452283E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.15 | backward-params-all-reduce: 61.96 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.02 | optimizer-unscale-and-check-inf: 18.01 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 97.53 | batch-generator: 8.05
 iteration    28400/  200000 | consumed samples:       908800 | elapsed time per iteration (ms): 256.7 | learning rate: 7.435E-05 | global batch size:    32 | lm loss: 3.573254E-05 | loss scale: 134217728.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.17 | backward-params-all-reduce: 64.31 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 19.26 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.19 | optimizer: 98.62 | batch-generator: 8.04
 iteration    28500/  200000 | consumed samples:       912000 | elapsed time per iteration (ms): 252.2 | learning rate: 7.426E-05 | global batch size:    32 | lm loss: 3.800385E-05 | loss scale: 134217728.0 | grad norm: 0.006 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.23 | backward-params-all-reduce: 61.94 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.46 | optimizer-unscale-and-check-inf: 17.94 | optimizer-clip-main-grad: 7.18 | optimizer-copy-main-to-model-params: 11.93 | optimizer: 96.57 | batch-generator: 8.07
-------------------------------------------------------------------------------------------------
 validation loss at iteration 28500 | lm loss value: 4.445969E-05 | lm loss PPL: 1.000044E+00 | 
 at iteration 28500, match long value: 0.0064893749665126 | match short value: 0.01316302669253692 
----------------------------------------------------------------------------------------------------
 iteration    28600/  200000 | consumed samples:       915200 | elapsed time per iteration (ms): 275.0 | learning rate: 7.417E-05 | global batch size:    32 | lm loss: 3.717318E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.38 | backward-params-all-reduce: 65.28 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.28 | optimizer-unscale-and-check-inf: 18.23 | optimizer-clip-main-grad: 7.28 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 97.93 | batch-generator: 14.39
 iteration    28700/  200000 | consumed samples:       918400 | elapsed time per iteration (ms): 267.3 | learning rate: 7.407E-05 | global batch size:    32 | lm loss: 3.956180E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.32 | backward-params-all-reduce: 64.22 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.00 | optimizer-unscale-and-check-inf: 18.06 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 13.59 | optimizer: 98.93 | batch-generator: 7.97
 iteration    28800/  200000 | consumed samples:       921600 | elapsed time per iteration (ms): 257.3 | learning rate: 7.398E-05 | global batch size:    32 | lm loss: 3.853486E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.40 | backward-params-all-reduce: 65.33 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 18.22 | optimizer-clip-main-grad: 7.24 | optimizer-copy-main-to-model-params: 12.05 | optimizer: 96.89 | batch-generator: 7.97
 iteration    28900/  200000 | consumed samples:       924800 | elapsed time per iteration (ms): 255.7 | learning rate: 7.389E-05 | global batch size:    32 | lm loss: 4.087412E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.06 | backward-params-all-reduce: 63.44 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.11 | optimizer-unscale-and-check-inf: 19.30 | optimizer-clip-main-grad: 7.37 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 98.97 | batch-generator: 8.00
 iteration    29000/  200000 | consumed samples:       928000 | elapsed time per iteration (ms): 253.5 | learning rate: 7.380E-05 | global batch size:    32 | lm loss: 3.724194E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.16 | backward-params-all-reduce: 63.14 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.95 | optimizer-unscale-and-check-inf: 18.45 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 97.89 | batch-generator: 8.11
-------------------------------------------------------------------------------------------------
 validation loss at iteration 29000 | lm loss value: 4.319816E-05 | lm loss PPL: 1.000043E+00 | 
 at iteration 29000, match long value: 0.0063117003508792666 | match short value: -0.006408793675016987 
---------------------------------------------------------------------------------------------------------
 iteration    29100/  200000 | consumed samples:       931200 | elapsed time per iteration (ms): 265.7 | learning rate: 7.371E-05 | global batch size:    32 | lm loss: 3.893455E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.49 | backward-params-all-reduce: 64.18 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.88 | optimizer-unscale-and-check-inf: 15.08 | optimizer-clip-main-grad: 7.27 | optimizer-copy-main-to-model-params: 12.19 | optimizer: 94.44 | batch-generator: 14.94
 iteration    29200/  200000 | consumed samples:       934400 | elapsed time per iteration (ms): 255.7 | learning rate: 7.362E-05 | global batch size:    32 | lm loss: 3.572672E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.49 | backward-params-all-reduce: 62.93 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.91 | optimizer-unscale-and-check-inf: 17.96 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.20 | optimizer: 97.42 | batch-generator: 8.04
 iteration    29300/  200000 | consumed samples:       937600 | elapsed time per iteration (ms): 253.9 | learning rate: 7.353E-05 | global batch size:    32 | lm loss: 3.573279E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.09 | backward-params-all-reduce: 61.72 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.53 | optimizer-unscale-and-check-inf: 16.57 | optimizer-clip-main-grad: 7.28 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 96.60 | batch-generator: 7.97
 iteration    29400/  200000 | consumed samples:       940800 | elapsed time per iteration (ms): 255.5 | learning rate: 7.344E-05 | global batch size:    32 | lm loss: 4.214080E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.21 | backward-params-all-reduce: 61.50 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 18.29 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 97.69 | batch-generator: 8.06
 iteration    29500/  200000 | consumed samples:       944000 | elapsed time per iteration (ms): 264.7 | learning rate: 7.335E-05 | global batch size:    32 | lm loss: 3.722520E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.87 | backward-params-all-reduce: 61.83 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.48 | optimizer-unscale-and-check-inf: 17.70 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 13.62 | optimizer: 99.10 | batch-generator: 8.14
-------------------------------------------------------------------------------------------------
 validation loss at iteration 29500 | lm loss value: 4.302030E-05 | lm loss PPL: 1.000043E+00 | 
 at iteration 29500, match long value: 0.0004915401288318183 | match short value: -0.05055275723608059 
--------------------------------------------------------------------------------------------------------
 iteration    29600/  200000 | consumed samples:       947200 | elapsed time per iteration (ms): 271.4 | learning rate: 7.326E-05 | global batch size:    32 | lm loss: 3.462962E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.33 | backward-params-all-reduce: 61.68 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.89 | optimizer-unscale-and-check-inf: 19.06 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.47 | batch-generator: 17.89
 iteration    29700/  200000 | consumed samples:       950400 | elapsed time per iteration (ms): 252.3 | learning rate: 7.316E-05 | global batch size:    32 | lm loss: 3.551946E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.92 | backward-params-all-reduce: 61.86 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 17.91 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 97.27 | batch-generator: 7.96
 iteration    29800/  200000 | consumed samples:       953600 | elapsed time per iteration (ms): 254.9 | learning rate: 7.307E-05 | global batch size:    32 | lm loss: 3.940863E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.05 | backward-params-all-reduce: 62.62 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.95 | optimizer-unscale-and-check-inf: 18.26 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 97.71 | batch-generator: 7.94
 iteration    29900/  200000 | consumed samples:       956800 | elapsed time per iteration (ms): 252.4 | learning rate: 7.298E-05 | global batch size:    32 | lm loss: 3.847142E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.08 | backward-params-all-reduce: 62.40 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.97 | optimizer-unscale-and-check-inf: 17.09 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 96.55 | batch-generator: 7.96
 iteration    30000/  200000 | consumed samples:       960000 | elapsed time per iteration (ms): 254.8 | learning rate: 7.289E-05 | global batch size:    32 | lm loss: 3.643310E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.34 | backward-params-all-reduce: 62.57 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.00 | optimizer-unscale-and-check-inf: 18.57 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.03 | batch-generator: 7.95
-------------------------------------------------------------------------------------------------
 validation loss at iteration 30000 | lm loss value: 4.444430E-05 | lm loss PPL: 1.000044E+00 | 
 at iteration 30000, match long value: -0.004487139401765313 | match short value: 0.03409549700944755 
-------------------------------------------------------------------------------------------------------
 iteration    30100/  200000 | consumed samples:       963200 | elapsed time per iteration (ms): 264.9 | learning rate: 7.280E-05 | global batch size:    32 | lm loss: 3.834319E-05 | loss scale: 134217728.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.31 | backward-params-all-reduce: 62.93 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 18.12 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.19 | optimizer: 97.46 | batch-generator: 13.38
 iteration    30200/  200000 | consumed samples:       966400 | elapsed time per iteration (ms): 262.4 | learning rate: 7.271E-05 | global batch size:    32 | lm loss: 4.030302E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.18 | backward-params-all-reduce: 63.65 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.17 | optimizer-unscale-and-check-inf: 15.25 | optimizer-clip-main-grad: 7.10 | optimizer-copy-main-to-model-params: 13.37 | optimizer: 94.94 | batch-generator: 8.04
 iteration    30300/  200000 | consumed samples:       969600 | elapsed time per iteration (ms): 254.4 | learning rate: 7.262E-05 | global batch size:    32 | lm loss: 3.440072E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.11 | backward-params-all-reduce: 64.26 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.18 | optimizer-unscale-and-check-inf: 16.41 | optimizer-clip-main-grad: 7.27 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 96.05 | batch-generator: 8.07
 iteration    30400/  200000 | consumed samples:       972800 | elapsed time per iteration (ms): 253.3 | learning rate: 7.253E-05 | global batch size:    32 | lm loss: 3.595054E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.10 | backward-params-all-reduce: 62.49 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 17.48 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 96.81 | batch-generator: 8.00
 iteration    30500/  200000 | consumed samples:       976000 | elapsed time per iteration (ms): 255.0 | learning rate: 7.244E-05 | global batch size:    32 | lm loss: 4.148982E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.19 | backward-params-all-reduce: 63.70 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.22 | optimizer-unscale-and-check-inf: 17.07 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 96.78 | batch-generator: 8.01
-------------------------------------------------------------------------------------------------
 validation loss at iteration 30500 | lm loss value: 5.173289E-05 | lm loss PPL: 1.000052E+00 | 
 at iteration 30500, match long value: 0.002517692626743159 | match short value: 0.017959721566599956 
-------------------------------------------------------------------------------------------------------
 iteration    30600/  200000 | consumed samples:       979200 | elapsed time per iteration (ms): 268.1 | learning rate: 7.235E-05 | global batch size:    32 | lm loss: 3.646320E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.43 | backward-params-all-reduce: 64.30 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.95 | optimizer-unscale-and-check-inf: 17.56 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 96.99 | batch-generator: 14.17
 iteration    30700/  200000 | consumed samples:       982400 | elapsed time per iteration (ms): 254.2 | learning rate: 7.226E-05 | global batch size:    32 | lm loss: 3.686936E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.02 | backward-params-all-reduce: 62.66 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.32 | optimizer-unscale-and-check-inf: 16.65 | optimizer-clip-main-grad: 7.28 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 96.44 | batch-generator: 8.01
 iteration    30800/  200000 | consumed samples:       985600 | elapsed time per iteration (ms): 252.6 | learning rate: 7.216E-05 | global batch size:    32 | lm loss: 3.600029E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.12 | backward-params-all-reduce: 62.76 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.07 | optimizer-unscale-and-check-inf: 16.49 | optimizer-clip-main-grad: 7.27 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 96.00 | batch-generator: 8.00
 iteration    30900/  200000 | consumed samples:       988800 | elapsed time per iteration (ms): 264.7 | learning rate: 7.207E-05 | global batch size:    32 | lm loss: 3.801184E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.80 | backward-params-all-reduce: 63.47 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 16.58 | optimizer-clip-main-grad: 7.28 | optimizer-copy-main-to-model-params: 13.58 | optimizer: 97.30 | batch-generator: 7.98
 iteration    31000/  200000 | consumed samples:       992000 | elapsed time per iteration (ms): 256.4 | learning rate: 7.198E-05 | global batch size:    32 | lm loss: 4.098569E-05 | loss scale: 67108864.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.17 | backward-params-all-reduce: 63.73 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.97 | optimizer-unscale-and-check-inf: 17.31 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 96.74 | batch-generator: 8.02
-------------------------------------------------------------------------------------------------
 validation loss at iteration 31000 | lm loss value: 4.547176E-05 | lm loss PPL: 1.000045E+00 | 
 at iteration 31000, match long value: 0.004422887963325425 | match short value: -0.025151326330330924 
--------------------------------------------------------------------------------------------------------
 iteration    31100/  200000 | consumed samples:       995200 | elapsed time per iteration (ms): 267.2 | learning rate: 7.189E-05 | global batch size:    32 | lm loss: 3.844213E-05 | loss scale: 67108864.0 | grad norm: 0.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.42 | backward-params-all-reduce: 63.30 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.01 | optimizer-unscale-and-check-inf: 17.48 | optimizer-clip-main-grad: 7.31 | optimizer-copy-main-to-model-params: 12.19 | optimizer: 97.00 | batch-generator: 12.95
 iteration    31200/  200000 | consumed samples:       998400 | elapsed time per iteration (ms): 255.4 | learning rate: 7.180E-05 | global batch size:    32 | lm loss: 3.630338E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.15 | backward-params-all-reduce: 62.01 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 17.75 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.08 | batch-generator: 7.98
 iteration    31300/  200000 | consumed samples:      1001600 | elapsed time per iteration (ms): 254.6 | learning rate: 7.171E-05 | global batch size:    32 | lm loss: 3.559090E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.17 | backward-params-all-reduce: 61.89 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.91 | optimizer-unscale-and-check-inf: 17.71 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.11 | batch-generator: 8.03
 iteration    31400/  200000 | consumed samples:      1004800 | elapsed time per iteration (ms): 255.0 | learning rate: 7.162E-05 | global batch size:    32 | lm loss: 3.683938E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.99 | backward-params-all-reduce: 62.10 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.13 | optimizer-unscale-and-check-inf: 16.91 | optimizer-clip-main-grad: 7.27 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 96.49 | batch-generator: 8.06
 iteration    31500/  200000 | consumed samples:      1008000 | elapsed time per iteration (ms): 253.9 | learning rate: 7.153E-05 | global batch size:    32 | lm loss: 3.554221E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.09 | backward-params-all-reduce: 62.76 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.33 | optimizer-unscale-and-check-inf: 16.75 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.18 | optimizer: 96.54 | batch-generator: 8.01
-------------------------------------------------------------------------------------------------
 validation loss at iteration 31500 | lm loss value: 4.092950E-05 | lm loss PPL: 1.000041E+00 | 
 at iteration 31500, match long value: 0.004929102973587837 | match short value: 0.021199103495079596 
-------------------------------------------------------------------------------------------------------
 iteration    31600/  200000 | consumed samples:      1011200 | elapsed time per iteration (ms): 281.1 | learning rate: 7.144E-05 | global batch size:    32 | lm loss: 3.142951E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.36 | backward-params-all-reduce: 63.47 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 20.17 | optimizer-clip-main-grad: 7.43 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 99.58 | batch-generator: 23.39
 iteration    31700/  200000 | consumed samples:      1014400 | elapsed time per iteration (ms): 267.2 | learning rate: 7.134E-05 | global batch size:    32 | lm loss: 3.333776E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.28 | backward-params-all-reduce: 62.92 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.01 | optimizer-unscale-and-check-inf: 19.00 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 13.58 | optimizer: 99.93 | batch-generator: 8.03
 iteration    31800/  200000 | consumed samples:      1017600 | elapsed time per iteration (ms): 260.3 | learning rate: 7.125E-05 | global batch size:    32 | lm loss: 3.277648E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.23 | backward-params-all-reduce: 63.63 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.01 | optimizer-unscale-and-check-inf: 19.58 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 99.09 | batch-generator: 7.98
 iteration    31900/  200000 | consumed samples:      1020800 | elapsed time per iteration (ms): 254.9 | learning rate: 7.116E-05 | global batch size:    32 | lm loss: 3.294502E-05 | loss scale: 134217728.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.15 | backward-params-all-reduce: 62.37 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.06 | optimizer-unscale-and-check-inf: 18.53 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 98.09 | batch-generator: 8.02
 iteration    32000/  200000 | consumed samples:      1024000 | elapsed time per iteration (ms): 255.2 | learning rate: 7.107E-05 | global batch size:    32 | lm loss: 3.400393E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.09 | backward-params-all-reduce: 63.30 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 12.27 | optimizer-unscale-and-check-inf: 17.24 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 96.98 | batch-generator: 7.95
-------------------------------------------------------------------------------------------------
 validation loss at iteration 32000 | lm loss value: 4.127223E-05 | lm loss PPL: 1.000041E+00 | 
 at iteration 32000, match long value: 0.0015459442472887522 | match short value: 0.002560528264935769 
--------------------------------------------------------------------------------------------------------
 iteration    32100/  200000 | consumed samples:      1027200 | elapsed time per iteration (ms): 268.5 | learning rate: 7.098E-05 | global batch size:    32 | lm loss: 3.139248E-05 | loss scale: 134217728.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 52.10 | backward-params-all-reduce: 63.92 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.90 | optimizer-unscale-and-check-inf: 18.24 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 97.62 | batch-generator: 13.60
 iteration    32200/  200000 | consumed samples:      1030400 | elapsed time per iteration (ms): 264.8 | learning rate: 7.089E-05 | global batch size:    32 | lm loss: 3.152473E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.13 | backward-params-all-reduce: 64.06 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.87 | optimizer-unscale-and-check-inf: 18.85 | optimizer-clip-main-grad: 7.21 | optimizer-copy-main-to-model-params: 11.92 | optimizer: 96.89 | batch-generator: 8.02
 iteration    32300/  200000 | consumed samples:      1033600 | elapsed time per iteration (ms): 257.1 | learning rate: 7.080E-05 | global batch size:    32 | lm loss: 3.371204E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.19 | backward-params-all-reduce: 62.89 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.26 | optimizer-unscale-and-check-inf: 19.12 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.87 | batch-generator: 7.96
 iteration    32400/  200000 | consumed samples:      1036800 | elapsed time per iteration (ms): 267.8 | learning rate: 7.071E-05 | global batch size:    32 | lm loss: 3.182555E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.49 | backward-params-all-reduce: 65.01 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.89 | optimizer-unscale-and-check-inf: 16.93 | optimizer-clip-main-grad: 7.26 | optimizer-copy-main-to-model-params: 13.59 | optimizer: 97.68 | batch-generator: 8.02
 iteration    32500/  200000 | consumed samples:      1040000 | elapsed time per iteration (ms): 256.2 | learning rate: 7.062E-05 | global batch size:    32 | lm loss: 3.206884E-05 | loss scale: 134217728.0 | grad norm: 0.006 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.01 | backward-params-all-reduce: 63.37 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 19.41 | optimizer-clip-main-grad: 7.36 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.77 | batch-generator: 7.99
-------------------------------------------------------------------------------------------------
 validation loss at iteration 32500 | lm loss value: 4.143666E-05 | lm loss PPL: 1.000041E+00 | 
 at iteration 32500, match long value: -0.0002639269952939131 | match short value: -0.025230011048611816 
----------------------------------------------------------------------------------------------------------
 iteration    32600/  200000 | consumed samples:      1043200 | elapsed time per iteration (ms): 266.6 | learning rate: 7.053E-05 | global batch size:    32 | lm loss: 3.220162E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.92 | backward-params-all-reduce: 62.43 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.90 | optimizer-unscale-and-check-inf: 18.75 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.14 | batch-generator: 14.28
 iteration    32700/  200000 | consumed samples:      1046400 | elapsed time per iteration (ms): 255.3 | learning rate: 7.044E-05 | global batch size:    32 | lm loss: 3.348765E-05 | loss scale: 134217728.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.15 | backward-params-all-reduce: 63.07 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.95 | optimizer-unscale-and-check-inf: 17.24 | optimizer-clip-main-grad: 7.34 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 96.71 | batch-generator: 7.95
 iteration    32800/  200000 | consumed samples:      1049600 | elapsed time per iteration (ms): 254.6 | learning rate: 7.035E-05 | global batch size:    32 | lm loss: 3.655143E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.05 | backward-params-all-reduce: 63.22 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.06 | optimizer-unscale-and-check-inf: 17.38 | optimizer-clip-main-grad: 7.19 | optimizer-copy-main-to-model-params: 12.04 | optimizer: 96.22 | batch-generator: 7.90
 iteration    32900/  200000 | consumed samples:      1052800 | elapsed time per iteration (ms): 257.5 | learning rate: 7.025E-05 | global batch size:    32 | lm loss: 3.232338E-05 | loss scale: 67108864.0 | grad norm: 0.008 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 50.92 | backward-params-all-reduce: 63.74 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.07 | optimizer-unscale-and-check-inf: 19.13 | optimizer-clip-main-grad: 7.33 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.69 | batch-generator: 7.91
 iteration    33000/  200000 | consumed samples:      1056000 | elapsed time per iteration (ms): 255.7 | learning rate: 7.016E-05 | global batch size:    32 | lm loss: 3.219025E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.44 | backward-params-all-reduce: 62.74 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 12.01 | optimizer-unscale-and-check-inf: 18.11 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 97.59 | batch-generator: 7.99
-------------------------------------------------------------------------------------------------
 validation loss at iteration 33000 | lm loss value: 4.260302E-05 | lm loss PPL: 1.000043E+00 | 
 at iteration 33000, match long value: -0.0009070953846630863 | match short value: 0.004038096157541125 
---------------------------------------------------------------------------------------------------------
 iteration    33100/  200000 | consumed samples:      1059200 | elapsed time per iteration (ms): 281.9 | learning rate: 7.007E-05 | global batch size:    32 | lm loss: 3.010757E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.58 | backward-params-all-reduce: 64.25 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.91 | optimizer-unscale-and-check-inf: 16.57 | optimizer-clip-main-grad: 7.28 | optimizer-copy-main-to-model-params: 13.63 | optimizer: 97.40 | batch-generator: 18.68
 iteration    33200/  200000 | consumed samples:      1062400 | elapsed time per iteration (ms): 264.6 | learning rate: 6.998E-05 | global batch size:    32 | lm loss: 3.388016E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.00 | backward-params-all-reduce: 62.96 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.84 | optimizer-unscale-and-check-inf: 17.28 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 96.58 | batch-generator: 7.90
 iteration    33300/  200000 | consumed samples:      1065600 | elapsed time per iteration (ms): 256.4 | learning rate: 6.989E-05 | global batch size:    32 | lm loss: 3.351020E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.02 | backward-params-all-reduce: 63.43 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.92 | optimizer-unscale-and-check-inf: 17.26 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 96.65 | batch-generator: 7.96
 iteration    33400/  200000 | consumed samples:      1068800 | elapsed time per iteration (ms): 257.1 | learning rate: 6.980E-05 | global batch size:    32 | lm loss: 3.297861E-05 | loss scale: 67108864.0 | grad norm: 0.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.01 | backward-params-all-reduce: 63.85 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 17.53 | optimizer-clip-main-grad: 7.29 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 96.84 | batch-generator: 7.89
 iteration    33500/  200000 | consumed samples:      1072000 | elapsed time per iteration (ms): 262.1 | learning rate: 6.971E-05 | global batch size:    32 | lm loss: 3.415573E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.54 | backward-params-all-reduce: 63.80 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.90 | optimizer-unscale-and-check-inf: 18.67 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.16 | optimizer: 98.06 | batch-generator: 7.92
-------------------------------------------------------------------------------------------------
 validation loss at iteration 33500 | lm loss value: 4.065591E-05 | lm loss PPL: 1.000041E+00 | 
 at iteration 33500, match long value: -0.003047295150065658 | match short value: 0.013756657455341461 
--------------------------------------------------------------------------------------------------------
 iteration    33600/  200000 | consumed samples:      1075200 | elapsed time per iteration (ms): 265.3 | learning rate: 6.962E-05 | global batch size:    32 | lm loss: 3.224002E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.17 | backward-params-all-reduce: 63.22 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.87 | optimizer-unscale-and-check-inf: 16.75 | optimizer-clip-main-grad: 7.25 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 96.05 | batch-generator: 13.78
 iteration    33700/  200000 | consumed samples:      1078400 | elapsed time per iteration (ms): 250.9 | learning rate: 6.953E-05 | global batch size:    32 | lm loss: 3.676066E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.20 | backward-params-all-reduce: 63.30 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.89 | optimizer-unscale-and-check-inf: 14.85 | optimizer-clip-main-grad: 7.24 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 94.14 | batch-generator: 8.15
 iteration    33800/  200000 | consumed samples:      1081600 | elapsed time per iteration (ms): 260.2 | learning rate: 6.943E-05 | global batch size:    32 | lm loss: 3.294751E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.62 | backward-params-all-reduce: 63.63 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.98 | optimizer-unscale-and-check-inf: 17.17 | optimizer-clip-main-grad: 7.28 | optimizer-copy-main-to-model-params: 13.63 | optimizer: 98.05 | batch-generator: 7.98
 iteration    33900/  200000 | consumed samples:      1084800 | elapsed time per iteration (ms): 263.9 | learning rate: 6.934E-05 | global batch size:    32 | lm loss: 3.527971E-05 | loss scale: 134217728.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.59 | backward-params-all-reduce: 63.89 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.90 | optimizer-unscale-and-check-inf: 17.97 | optimizer-clip-main-grad: 7.32 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 97.33 | batch-generator: 7.95
 iteration    34000/  200000 | consumed samples:      1088000 | elapsed time per iteration (ms): 253.9 | learning rate: 6.925E-05 | global batch size:    32 | lm loss: 3.764044E-05 | loss scale: 67108864.0 | grad norm: 0.005 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.04 | backward-params-all-reduce: 62.78 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.85 | optimizer-unscale-and-check-inf: 17.37 | optimizer-clip-main-grad: 7.17 | optimizer-copy-main-to-model-params: 11.92 | optimizer: 95.35 | batch-generator: 7.90
-------------------------------------------------------------------------------------------------
 validation loss at iteration 34000 | lm loss value: 4.275737E-05 | lm loss PPL: 1.000043E+00 | 
 at iteration 34000, match long value: 0.0019668993171689858 | match short value: -0.02137328451137291 
--------------------------------------------------------------------------------------------------------
 iteration    34100/  200000 | consumed samples:      1091200 | elapsed time per iteration (ms): 265.5 | learning rate: 6.916E-05 | global batch size:    32 | lm loss: 3.151519E-05 | loss scale: 67108864.0 | grad norm: 0.002 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.06 | backward-params-all-reduce: 62.66 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 16.52 | optimizer-clip-main-grad: 7.28 | optimizer-copy-main-to-model-params: 12.15 | optimizer: 95.82 | batch-generator: 13.71
 iteration    34200/  200000 | consumed samples:      1094400 | elapsed time per iteration (ms): 256.6 | learning rate: 6.907E-05 | global batch size:    32 | lm loss: 3.133670E-05 | loss scale: 67108864.0 | grad norm: 0.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 51.17 | backward-params-all-reduce: 62.45 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 11.86 | optimizer-unscale-and-check-inf: 17.31 | optimizer-clip-main-grad: 7.30 | optimizer-copy-main-to-model-params: 12.17 | optimizer: 96.63 | batch-generator: 7.99
