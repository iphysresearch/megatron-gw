Fetching real event data...
Fetching real event data...
Fetching real event data...
Fetching real event data...
Fetching real event data...
Fetching real event data...
Fetching real event data...
Fetching real event data...
using world size: 8, data-parallel-size: 8, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.999
  adam_eps ........................................ 1e-08
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  bert_binary_head ................................ False
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... False
  checkpoint_num_layers ........................... 1
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  data_impl ....................................... mmap
  data_parallel_size .............................. 8
  data_path ....................................... ['../bigdata/']
  dataloader_type ................................. cyclic
  DDP_impl ........................................ local
  decoder_seq_length .............................. None
  distribute_checkpointed_activations ............. False
  distributed_backend ............................. nccl
  embedding_path .................................. None
  encoder_seq_length .............................. 127
  eod_mask_loss ................................... False
  eval_interval ................................... 500
  eval_iters ...................................... 1
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  ffn_hidden_size ................................. 4096
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  global_batch_size ............................... 128
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 1024
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_dim ......................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  init_method_std ................................. 0.02
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  kv_channels ..................................... 64
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ mask
  local_rank ...................................... 0
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 100
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0001
  lr_decay_iters .................................. 99000
  lr_decay_samples ................................ None
  lr_decay_style .................................. linear
  lr_warmup_fraction .............................. 0.002
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 128
  mask_prob ....................................... 0.15
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 127
  merge_file ...................................... None
  micro_batch_size ................................ 16
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mmap_warmup ..................................... False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_save_optim ................................... None
  no_save_rng ..................................... None
  num_attention_heads ............................. 16
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_layers ...................................... 24
  num_layers_per_virtual_pipeline_stage ........... None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  override_lr_scheduler ........................... False
  params_dtype .................................... torch.float16
  patch_dim ....................................... 16
  pipeline_model_parallel_size .................... 1
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  sample_rate ..................................... 1.0
  save ............................................ mask
  save_interval ................................... 100000
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  segment_length .................................. 2048
  seq_length ...................................... 127
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  split ........................................... 949,50,1
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. None
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  titles_data_path ................................ None
  tokenizer_type .................................. BertWordPieceLowerCase
  train_iters ..................................... 200000
  train_samples ................................... None
  use_checkpoint_lr_scheduler ..................... False
  use_contiguous_buffers_in_ddp ................... False
  use_cpu_initialization .......................... None
  use_one_sent_docs ............................... False
  virtual_pipeline_model_parallel_size ............ None
  vocab_extra_ids ................................. 0
  vocab_file ...................................... ./bert_vocab_files/bert-base-uncased-vocab.txt
  weight_decay .................................... 0.01
  world_size ...................................... 8
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 1
> building BertWordPieceLowerCase tokenizer ...
 > padded vocab (size: 30524) with 68 dummy tokens (new size: 30592)
> initializing torch distributed ...
> initializing tensor model parallel with size 1
> initializing pipeline model parallel with size 1
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
> compiling and loading fused kernels ...
Detected CUDA files, patching ldflags
Emitting ninja build file /workspace/zhouy/megatron-gw/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_upper_triang_masked_softmax_cuda...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module scaled_upper_triang_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /workspace/zhouy/megatron-gw/megatron/fused_kernels/build/build.ninja...
Building extension module scaled_masked_softmax_cuda...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module scaled_masked_softmax_cuda...
Detected CUDA files, patching ldflags
Emitting ninja build file /workspace/zhouy/megatron-gw/megatron/fused_kernels/build/build.ninja...
Building extension module fused_mix_prec_layer_norm_cuda...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_mix_prec_layer_norm_cuda...
>>> done with compiling and loading fused kernels. Compilation time: 13.840 seconds
time to initialize megatron (seconds): 53.079
[after megatron is initialized] datetime: 2021-12-27 22:01:25 
building BERT model ...
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 305460224
> learning rate decay style: linear
WARNING: could not find the metadata file mask/latest_checkpointed_iteration.txt 
    will not load any checkpoints and will start from random
time (ms) | load-checkpoint: 0.41
[after model, optimizer, and learning rate scheduler are built] datetime: 2021-12-27 22:01:25 
> building train, validation, and test datasets ...
> building train, validation, and test datasets for BERT ...
Fetching real event data...
Fetching real event data...Fetching real event data...

Fetching real event data...Fetching real event data...

Fetching real event data...
Fetching real event data...
Fetching real event data...
Fetching real event data...
Fetching real event data...
Fetching real event data...
Fetching real event data...
Fetching real event data...
Fetching real event data...Fetching real event data...

Fetching real event data...
> finished creating BERT datasets ...
[after dataloaders are built] datetime: 2021-12-27 22:01:41 
done with setup ...
training ...
time (ms) | model-and-optimizer-setup: 413.96 | train/valid/test-data-iterators-setup: 15453.00
[before the start of training step] datetime: 2021-12-27 22:01:41 
 iteration      100/  200000 | consumed samples:        12800 | elapsed time per iteration (ms): 1324.8 | learning rate: 4.596E-05 | global batch size:   128 | lm loss: 6.781084E-03 | loss scale: 16777216.0 | grad norm: 0.002 | number of skipped iterations:   9 | number of nan iterations:   0 |
time (ms) | backward-compute: 82.98 | backward-params-all-reduce: 50.05 | backward-embedding-all-reduce: 0.10 | optimizer-copy-to-main-grad: 6.19 | optimizer-unscale-and-check-inf: 6.25 | optimizer-clip-main-grad: 4.73 | optimizer-copy-main-to-model-params: 6.11 | optimizer: 36.38 | batch-generator: 926.20
[Rank 0] (after 100 iterations) memory (MB) | allocated: 5827.00732421875 | max allocated: 7575.01171875 | reserved: 8220.0 | max reserved: 8220.0
 iteration      200/  200000 | consumed samples:        25600 | elapsed time per iteration (ms): 1294.2 | learning rate: 9.646E-05 | global batch size:   128 | lm loss: 9.457428E-05 | loss scale: 16777216.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 80.87 | backward-params-all-reduce: 77.05 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 5.86 | optimizer-unscale-and-check-inf: 5.12 | optimizer-clip-main-grad: 4.94 | optimizer-copy-main-to-model-params: 5.92 | optimizer: 35.59 | batch-generator: 854.40
 iteration      300/  200000 | consumed samples:        38400 | elapsed time per iteration (ms): 1336.2 | learning rate: 9.992E-05 | global batch size:   128 | lm loss: 4.835173E-05 | loss scale: 16777216.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 88.73 | backward-params-all-reduce: 44.09 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.23 | optimizer-unscale-and-check-inf: 5.25 | optimizer-clip-main-grad: 5.35 | optimizer-copy-main-to-model-params: 6.40 | optimizer: 37.24 | batch-generator: 982.96
 iteration      400/  200000 | consumed samples:        51200 | elapsed time per iteration (ms): 1289.8 | learning rate: 9.982E-05 | global batch size:   128 | lm loss: 3.338342E-05 | loss scale: 16777216.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 90.55 | backward-params-all-reduce: 43.83 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.12 | optimizer-unscale-and-check-inf: 5.23 | optimizer-clip-main-grad: 5.22 | optimizer-copy-main-to-model-params: 8.05 | optimizer: 38.46 | batch-generator: 933.54
 iteration      500/  200000 | consumed samples:        64000 | elapsed time per iteration (ms): 1322.6 | learning rate: 9.973E-05 | global batch size:   128 | lm loss: 2.597019E-05 | loss scale: 16777216.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 91.30 | backward-params-all-reduce: 45.39 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.20 | optimizer-unscale-and-check-inf: 5.10 | optimizer-clip-main-grad: 5.25 | optimizer-copy-main-to-model-params: 6.54 | optimizer: 37.02 | batch-generator: 933.46
 at iteration 500, match long value: 0.021985422235679754 | match short value: 0.21888091101074983 
----------------------------------------------------------------------------------------------------
 iteration      600/  200000 | consumed samples:        76800 | elapsed time per iteration (ms): 1296.6 | learning rate: 9.964E-05 | global batch size:   128 | lm loss: 2.520428E-06 | loss scale: 16777216.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 89.23 | backward-params-all-reduce: 43.54 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.87 | optimizer-unscale-and-check-inf: 5.27 | optimizer-clip-main-grad: 5.10 | optimizer-copy-main-to-model-params: 6.40 | optimizer: 36.52 | batch-generator: 931.92
 iteration      700/  200000 | consumed samples:        89600 | elapsed time per iteration (ms): 1272.9 | learning rate: 9.955E-05 | global batch size:   128 | lm loss: 2.468316E-06 | loss scale: 16777216.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 80.38 | backward-params-all-reduce: 43.03 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 6.15 | optimizer-unscale-and-check-inf: 5.04 | optimizer-clip-main-grad: 5.15 | optimizer-copy-main-to-model-params: 6.40 | optimizer: 36.55 | batch-generator: 898.91
 iteration      800/  200000 | consumed samples:       102400 | elapsed time per iteration (ms): 1370.7 | learning rate: 9.946E-05 | global batch size:   128 | lm loss: 2.446143E-06 | loss scale: 16777216.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 90.10 | backward-params-all-reduce: 44.38 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.31 | optimizer-unscale-and-check-inf: 5.27 | optimizer-clip-main-grad: 5.17 | optimizer-copy-main-to-model-params: 7.90 | optimizer: 38.57 | batch-generator: 993.59
 iteration      900/  200000 | consumed samples:       115200 | elapsed time per iteration (ms): 1249.7 | learning rate: 9.937E-05 | global batch size:   128 | lm loss: 2.427623E-06 | loss scale: 16777216.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 76.80 | backward-params-all-reduce: 48.15 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.99 | optimizer-unscale-and-check-inf: 5.10 | optimizer-clip-main-grad: 5.09 | optimizer-copy-main-to-model-params: 6.49 | optimizer: 36.45 | batch-generator: 913.64
 iteration     1000/  200000 | consumed samples:       128000 | elapsed time per iteration (ms): 1288.7 | learning rate: 9.928E-05 | global batch size:   128 | lm loss: 2.410683E-06 | loss scale: 16777216.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 70.02 | backward-params-all-reduce: 62.20 | backward-embedding-all-reduce: 0.12 | optimizer-copy-to-main-grad: 6.30 | optimizer-unscale-and-check-inf: 5.24 | optimizer-clip-main-grad: 5.49 | optimizer-copy-main-to-model-params: 6.58 | optimizer: 37.77 | batch-generator: 902.03
 at iteration 1000, match long value: 0.031456037558295326 | match short value: 0.29877596685985597 
-----------------------------------------------------------------------------------------------------
 iteration     1100/  200000 | consumed samples:       140800 | elapsed time per iteration (ms): 1268.8 | learning rate: 9.919E-05 | global batch size:   128 | lm loss: 2.393465E-06 | loss scale: 33554432.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 94.30 | backward-params-all-reduce: 51.67 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 6.05 | optimizer-unscale-and-check-inf: 4.79 | optimizer-clip-main-grad: 4.51 | optimizer-copy-main-to-model-params: 11.79 | optimizer: 40.82 | batch-generator: 867.71
 iteration     1200/  200000 | consumed samples:       153600 | elapsed time per iteration (ms): 1270.9 | learning rate: 9.910E-05 | global batch size:   128 | lm loss: 2.378380E-06 | loss scale: 33554432.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 86.68 | backward-params-all-reduce: 40.18 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 6.46 | optimizer-unscale-and-check-inf: 5.20 | optimizer-clip-main-grad: 5.40 | optimizer-copy-main-to-model-params: 6.19 | optimizer: 37.21 | batch-generator: 931.81
 iteration     1300/  200000 | consumed samples:       166400 | elapsed time per iteration (ms): 1273.8 | learning rate: 9.900E-05 | global batch size:   128 | lm loss: 2.363518E-06 | loss scale: 33554432.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 84.62 | backward-params-all-reduce: 43.04 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.16 | optimizer-unscale-and-check-inf: 5.12 | optimizer-clip-main-grad: 5.21 | optimizer-copy-main-to-model-params: 6.60 | optimizer: 36.98 | batch-generator: 910.70
 iteration     1400/  200000 | consumed samples:       179200 | elapsed time per iteration (ms): 1349.9 | learning rate: 9.891E-05 | global batch size:   128 | lm loss: 2.349912E-06 | loss scale: 33554432.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 87.05 | backward-params-all-reduce: 47.08 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.69 | optimizer-unscale-and-check-inf: 4.94 | optimizer-clip-main-grad: 4.83 | optimizer-copy-main-to-model-params: 5.95 | optimizer: 35.01 | batch-generator: 964.66
 iteration     1500/  200000 | consumed samples:       192000 | elapsed time per iteration (ms): 1366.8 | learning rate: 9.882E-05 | global batch size:   128 | lm loss: 2.336160E-06 | loss scale: 33554432.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 75.02 | backward-params-all-reduce: 64.22 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.43 | optimizer-unscale-and-check-inf: 4.85 | optimizer-clip-main-grad: 4.46 | optimizer-copy-main-to-model-params: 5.66 | optimizer: 33.99 | batch-generator: 973.99
 at iteration 1500, match long value: 0.03573711797947798 | match short value: 0.34103109325740927 
----------------------------------------------------------------------------------------------------
 iteration     1600/  200000 | consumed samples:       204800 | elapsed time per iteration (ms): 1273.8 | learning rate: 9.873E-05 | global batch size:   128 | lm loss: 2.320364E-06 | loss scale: 33554432.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 67.94 | backward-params-all-reduce: 53.89 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 4.81 | optimizer-unscale-and-check-inf: 4.71 | optimizer-clip-main-grad: 4.14 | optimizer-copy-main-to-model-params: 5.37 | optimizer: 32.42 | batch-generator: 883.20
 iteration     1700/  200000 | consumed samples:       217600 | elapsed time per iteration (ms): 1345.8 | learning rate: 9.864E-05 | global batch size:   128 | lm loss: 2.304749E-06 | loss scale: 33554432.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 68.55 | backward-params-all-reduce: 60.39 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 4.95 | optimizer-unscale-and-check-inf: 4.68 | optimizer-clip-main-grad: 4.22 | optimizer-copy-main-to-model-params: 5.32 | optimizer: 32.78 | batch-generator: 929.69
 iteration     1800/  200000 | consumed samples:       230400 | elapsed time per iteration (ms): 1274.8 | learning rate: 9.855E-05 | global batch size:   128 | lm loss: 2.290860E-06 | loss scale: 33554432.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 66.14 | backward-params-all-reduce: 58.90 | backward-embedding-all-reduce: 0.06 | optimizer-copy-to-main-grad: 4.80 | optimizer-unscale-and-check-inf: 4.60 | optimizer-clip-main-grad: 4.23 | optimizer-copy-main-to-model-params: 5.33 | optimizer: 32.35 | batch-generator: 860.67
 iteration     1900/  200000 | consumed samples:       243200 | elapsed time per iteration (ms): 1304.5 | learning rate: 9.846E-05 | global batch size:   128 | lm loss: 2.275443E-06 | loss scale: 33554432.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 66.08 | backward-params-all-reduce: 61.79 | backward-embedding-all-reduce: 0.06 | optimizer-copy-to-main-grad: 4.84 | optimizer-unscale-and-check-inf: 4.61 | optimizer-clip-main-grad: 4.09 | optimizer-copy-main-to-model-params: 5.30 | optimizer: 32.17 | batch-generator: 875.79
 iteration     2000/  200000 | consumed samples:       256000 | elapsed time per iteration (ms): 1299.2 | learning rate: 9.837E-05 | global batch size:   128 | lm loss: 2.260428E-06 | loss scale: 33554432.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 67.17 | backward-params-all-reduce: 60.05 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.09 | optimizer-unscale-and-check-inf: 4.70 | optimizer-clip-main-grad: 4.25 | optimizer-copy-main-to-model-params: 5.42 | optimizer: 32.99 | batch-generator: 880.79
 at iteration 2000, match long value: 0.040944284332777345 | match short value: 0.3883809279313936 
----------------------------------------------------------------------------------------------------
 iteration     2100/  200000 | consumed samples:       268800 | elapsed time per iteration (ms): 1334.7 | learning rate: 9.828E-05 | global batch size:   128 | lm loss: 2.245814E-06 | loss scale: 67108864.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 74.59 | backward-params-all-reduce: 57.09 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.54 | optimizer-unscale-and-check-inf: 5.00 | optimizer-clip-main-grad: 4.83 | optimizer-copy-main-to-model-params: 5.96 | optimizer: 35.07 | batch-generator: 928.77
 iteration     2200/  200000 | consumed samples:       281600 | elapsed time per iteration (ms): 1255.8 | learning rate: 9.818E-05 | global batch size:   128 | lm loss: 2.231586E-06 | loss scale: 67108864.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 64.04 | backward-params-all-reduce: 53.92 | backward-embedding-all-reduce: 0.06 | optimizer-copy-to-main-grad: 4.44 | optimizer-unscale-and-check-inf: 4.47 | optimizer-clip-main-grad: 3.79 | optimizer-copy-main-to-model-params: 5.00 | optimizer: 31.06 | batch-generator: 853.59
 iteration     2300/  200000 | consumed samples:       294400 | elapsed time per iteration (ms): 1389.1 | learning rate: 9.809E-05 | global batch size:   128 | lm loss: 2.254790E-06 | loss scale: 67108864.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 72.75 | backward-params-all-reduce: 57.94 | backward-embedding-all-reduce: 0.06 | optimizer-copy-to-main-grad: 5.08 | optimizer-unscale-and-check-inf: 4.72 | optimizer-clip-main-grad: 4.19 | optimizer-copy-main-to-model-params: 5.40 | optimizer: 32.78 | batch-generator: 957.30
 iteration     2400/  200000 | consumed samples:       307200 | elapsed time per iteration (ms): 1348.6 | learning rate: 9.800E-05 | global batch size:   128 | lm loss: 2.436252E-06 | loss scale: 67108864.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 71.55 | backward-params-all-reduce: 66.74 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.42 | optimizer-unscale-and-check-inf: 4.82 | optimizer-clip-main-grad: 4.69 | optimizer-copy-main-to-model-params: 5.90 | optimizer: 34.44 | batch-generator: 924.80
 iteration     2500/  200000 | consumed samples:       320000 | elapsed time per iteration (ms): 1347.3 | learning rate: 9.791E-05 | global batch size:   128 | lm loss: 2.667278E-06 | loss scale: 67108864.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 72.87 | backward-params-all-reduce: 59.70 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.23 | optimizer-unscale-and-check-inf: 4.71 | optimizer-clip-main-grad: 4.28 | optimizer-copy-main-to-model-params: 5.49 | optimizer: 33.24 | batch-generator: 929.74
 at iteration 2500, match long value: 0.042571021950174606 | match short value: 0.3970759962801873 
----------------------------------------------------------------------------------------------------
 iteration     2600/  200000 | consumed samples:       332800 | elapsed time per iteration (ms): 1269.2 | learning rate: 9.782E-05 | global batch size:   128 | lm loss: 2.766270E-06 | loss scale: 67108864.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 64.96 | backward-params-all-reduce: 62.43 | backward-embedding-all-reduce: 0.06 | optimizer-copy-to-main-grad: 4.70 | optimizer-unscale-and-check-inf: 4.54 | optimizer-clip-main-grad: 3.96 | optimizer-copy-main-to-model-params: 5.19 | optimizer: 31.69 | batch-generator: 830.43
 iteration     2700/  200000 | consumed samples:       345600 | elapsed time per iteration (ms): 1346.2 | learning rate: 9.773E-05 | global batch size:   128 | lm loss: 2.621244E-06 | loss scale: 67108864.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 72.91 | backward-params-all-reduce: 65.79 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.65 | optimizer-unscale-and-check-inf: 4.78 | optimizer-clip-main-grad: 4.45 | optimizer-copy-main-to-model-params: 5.86 | optimizer: 34.45 | batch-generator: 914.26
 iteration     2800/  200000 | consumed samples:       358400 | elapsed time per iteration (ms): 1231.2 | learning rate: 9.764E-05 | global batch size:   128 | lm loss: 2.626274E-06 | loss scale: 67108864.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 62.89 | backward-params-all-reduce: 61.16 | backward-embedding-all-reduce: 0.06 | optimizer-copy-to-main-grad: 4.48 | optimizer-unscale-and-check-inf: 4.56 | optimizer-clip-main-grad: 3.94 | optimizer-copy-main-to-model-params: 5.07 | optimizer: 31.40 | batch-generator: 797.48
 iteration     2900/  200000 | consumed samples:       371200 | elapsed time per iteration (ms): 1225.9 | learning rate: 9.755E-05 | global batch size:   128 | lm loss: 2.781679E-06 | loss scale: 67108864.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 64.03 | backward-params-all-reduce: 62.16 | backward-embedding-all-reduce: 0.06 | optimizer-copy-to-main-grad: 4.36 | optimizer-unscale-and-check-inf: 4.42 | optimizer-clip-main-grad: 3.80 | optimizer-copy-main-to-model-params: 4.93 | optimizer: 30.72 | batch-generator: 786.75
 iteration     3000/  200000 | consumed samples:       384000 | elapsed time per iteration (ms): 1330.3 | learning rate: 9.746E-05 | global batch size:   128 | lm loss: 2.713004E-06 | loss scale: 67108864.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 66.56 | backward-params-all-reduce: 69.71 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 4.89 | optimizer-unscale-and-check-inf: 4.63 | optimizer-clip-main-grad: 4.22 | optimizer-copy-main-to-model-params: 5.27 | optimizer: 32.47 | batch-generator: 847.29
 at iteration 3000, match long value: 0.05060222086312255 | match short value: 0.4527191528743547 
---------------------------------------------------------------------------------------------------
 iteration     3100/  200000 | consumed samples:       396800 | elapsed time per iteration (ms): 1255.0 | learning rate: 9.736E-05 | global batch size:   128 | lm loss: 2.761820E-06 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 64.79 | backward-params-all-reduce: 68.44 | backward-embedding-all-reduce: 0.06 | optimizer-copy-to-main-grad: 4.73 | optimizer-unscale-and-check-inf: 4.59 | optimizer-clip-main-grad: 4.15 | optimizer-copy-main-to-model-params: 5.14 | optimizer: 31.92 | batch-generator: 798.47
 iteration     3200/  200000 | consumed samples:       409600 | elapsed time per iteration (ms): 1295.0 | learning rate: 9.727E-05 | global batch size:   128 | lm loss: 2.691041E-06 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 69.14 | backward-params-all-reduce: 63.07 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.13 | optimizer-unscale-and-check-inf: 4.83 | optimizer-clip-main-grad: 4.26 | optimizer-copy-main-to-model-params: 5.43 | optimizer: 33.08 | batch-generator: 874.67
 iteration     3300/  200000 | consumed samples:       422400 | elapsed time per iteration (ms): 1294.4 | learning rate: 9.718E-05 | global batch size:   128 | lm loss: 2.735962E-06 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 73.64 | backward-params-all-reduce: 60.02 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.23 | optimizer-unscale-and-check-inf: 4.76 | optimizer-clip-main-grad: 4.57 | optimizer-copy-main-to-model-params: 7.19 | optimizer: 35.30 | batch-generator: 890.54
 iteration     3400/  200000 | consumed samples:       435200 | elapsed time per iteration (ms): 1230.5 | learning rate: 9.709E-05 | global batch size:   128 | lm loss: 2.759205E-06 | loss scale: 134217728.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 70.48 | backward-params-all-reduce: 56.04 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 4.90 | optimizer-unscale-and-check-inf: 4.59 | optimizer-clip-main-grad: 4.32 | optimizer-copy-main-to-model-params: 5.50 | optimizer: 32.74 | batch-generator: 825.18
 iteration     3500/  200000 | consumed samples:       448000 | elapsed time per iteration (ms): 1100.8 | learning rate: 9.700E-05 | global batch size:   128 | lm loss: 2.755506E-06 | loss scale: 134217728.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 72.14 | backward-params-all-reduce: 50.04 | backward-embedding-all-reduce: 0.06 | optimizer-copy-to-main-grad: 5.08 | optimizer-unscale-and-check-inf: 4.52 | optimizer-clip-main-grad: 4.11 | optimizer-copy-main-to-model-params: 5.51 | optimizer: 32.59 | batch-generator: 761.11
 at iteration 3500, match long value: 0.050377169460405644 | match short value: 0.4795181202789474 
----------------------------------------------------------------------------------------------------
 iteration     3600/  200000 | consumed samples:       460800 | elapsed time per iteration (ms): 1178.2 | learning rate: 9.691E-05 | global batch size:   128 | lm loss: 2.737833E-06 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 74.37 | backward-params-all-reduce: 49.65 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.52 | optimizer-unscale-and-check-inf: 4.65 | optimizer-clip-main-grad: 4.38 | optimizer-copy-main-to-model-params: 5.48 | optimizer: 33.56 | batch-generator: 824.42
 iteration     3700/  200000 | consumed samples:       473600 | elapsed time per iteration (ms): 1402.2 | learning rate: 9.682E-05 | global batch size:   128 | lm loss: 2.639685E-06 | loss scale: 134217728.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 76.94 | backward-params-all-reduce: 61.27 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.84 | optimizer-unscale-and-check-inf: 5.24 | optimizer-clip-main-grad: 4.93 | optimizer-copy-main-to-model-params: 7.76 | optimizer: 37.50 | batch-generator: 982.67
 iteration     3800/  200000 | consumed samples:       486400 | elapsed time per iteration (ms): 1351.2 | learning rate: 9.673E-05 | global batch size:   128 | lm loss: 2.832246E-06 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 74.80 | backward-params-all-reduce: 55.30 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.21 | optimizer-unscale-and-check-inf: 4.79 | optimizer-clip-main-grad: 4.44 | optimizer-copy-main-to-model-params: 5.65 | optimizer: 33.91 | batch-generator: 948.29
 iteration     3900/  200000 | consumed samples:       499200 | elapsed time per iteration (ms): 1374.7 | learning rate: 9.664E-05 | global batch size:   128 | lm loss: 2.618807E-06 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 77.47 | backward-params-all-reduce: 58.74 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.19 | optimizer-unscale-and-check-inf: 4.71 | optimizer-clip-main-grad: 4.39 | optimizer-copy-main-to-model-params: 5.60 | optimizer: 33.45 | batch-generator: 975.18
 iteration     4000/  200000 | consumed samples:       512000 | elapsed time per iteration (ms): 1384.1 | learning rate: 9.654E-05 | global batch size:   128 | lm loss: 2.735699E-06 | loss scale: 134217728.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 82.09 | backward-params-all-reduce: 55.79 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.51 | optimizer-unscale-and-check-inf: 4.83 | optimizer-clip-main-grad: 4.54 | optimizer-copy-main-to-model-params: 5.71 | optimizer: 34.23 | batch-generator: 989.02
 at iteration 4000, match long value: 0.06484133642914426 | match short value: 0.5436528091025068 
---------------------------------------------------------------------------------------------------
 iteration     4100/  200000 | consumed samples:       524800 | elapsed time per iteration (ms): 1296.7 | learning rate: 9.645E-05 | global batch size:   128 | lm loss: 2.712196E-06 | loss scale: 268435456.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 77.09 | backward-params-all-reduce: 54.22 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.30 | optimizer-unscale-and-check-inf: 4.74 | optimizer-clip-main-grad: 4.37 | optimizer-copy-main-to-model-params: 5.67 | optimizer: 33.66 | batch-generator: 914.58
 iteration     4200/  200000 | consumed samples:       537600 | elapsed time per iteration (ms): 1373.8 | learning rate: 9.636E-05 | global batch size:   128 | lm loss: 2.472264E-06 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 73.23 | backward-params-all-reduce: 67.58 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.52 | optimizer-unscale-and-check-inf: 4.96 | optimizer-clip-main-grad: 4.66 | optimizer-copy-main-to-model-params: 5.61 | optimizer: 34.31 | batch-generator: 950.85
 iteration     4300/  200000 | consumed samples:       550400 | elapsed time per iteration (ms): 1345.2 | learning rate: 9.627E-05 | global batch size:   128 | lm loss: 2.652953E-06 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 69.35 | backward-params-all-reduce: 68.72 | backward-embedding-all-reduce: 0.06 | optimizer-copy-to-main-grad: 4.82 | optimizer-unscale-and-check-inf: 4.64 | optimizer-clip-main-grad: 4.28 | optimizer-copy-main-to-model-params: 5.36 | optimizer: 32.56 | batch-generator: 887.85
 iteration     4400/  200000 | consumed samples:       563200 | elapsed time per iteration (ms): 1269.5 | learning rate: 9.618E-05 | global batch size:   128 | lm loss: 2.537894E-06 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 68.94 | backward-params-all-reduce: 60.48 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.18 | optimizer-unscale-and-check-inf: 4.77 | optimizer-clip-main-grad: 4.51 | optimizer-copy-main-to-model-params: 5.54 | optimizer: 33.62 | batch-generator: 848.31
 iteration     4500/  200000 | consumed samples:       576000 | elapsed time per iteration (ms): 1299.2 | learning rate: 9.609E-05 | global batch size:   128 | lm loss: 2.551349E-06 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 72.00 | backward-params-all-reduce: 61.59 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.24 | optimizer-unscale-and-check-inf: 4.76 | optimizer-clip-main-grad: 4.45 | optimizer-copy-main-to-model-params: 5.88 | optimizer: 33.89 | batch-generator: 899.57
 at iteration 4500, match long value: 0.06805514246103499 | match short value: 0.5772335923650196 
---------------------------------------------------------------------------------------------------
 iteration     4600/  200000 | consumed samples:       588800 | elapsed time per iteration (ms): 1340.1 | learning rate: 9.600E-05 | global batch size:   128 | lm loss: 2.528090E-06 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 75.00 | backward-params-all-reduce: 59.08 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.20 | optimizer-unscale-and-check-inf: 4.82 | optimizer-clip-main-grad: 4.36 | optimizer-copy-main-to-model-params: 6.93 | optimizer: 34.80 | batch-generator: 918.10
 iteration     4700/  200000 | consumed samples:       601600 | elapsed time per iteration (ms): 1312.3 | learning rate: 9.591E-05 | global batch size:   128 | lm loss: 2.496617E-06 | loss scale: 268435456.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 66.75 | backward-params-all-reduce: 63.59 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 4.88 | optimizer-unscale-and-check-inf: 4.73 | optimizer-clip-main-grad: 4.44 | optimizer-copy-main-to-model-params: 5.37 | optimizer: 32.89 | batch-generator: 864.83
 iteration     4800/  200000 | consumed samples:       614400 | elapsed time per iteration (ms): 1186.8 | learning rate: 9.582E-05 | global batch size:   128 | lm loss: 2.537818E-06 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 71.65 | backward-params-all-reduce: 53.94 | backward-embedding-all-reduce: 0.06 | optimizer-copy-to-main-grad: 5.21 | optimizer-unscale-and-check-inf: 4.53 | optimizer-clip-main-grad: 4.14 | optimizer-copy-main-to-model-params: 5.45 | optimizer: 32.76 | batch-generator: 797.22
 iteration     4900/  200000 | consumed samples:       627200 | elapsed time per iteration (ms): 1324.9 | learning rate: 9.573E-05 | global batch size:   128 | lm loss: 2.373286E-06 | loss scale: 268435456.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 73.06 | backward-params-all-reduce: 56.80 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.30 | optimizer-unscale-and-check-inf: 4.82 | optimizer-clip-main-grad: 4.46 | optimizer-copy-main-to-model-params: 5.57 | optimizer: 33.76 | batch-generator: 922.41
 iteration     5000/  200000 | consumed samples:       640000 | elapsed time per iteration (ms): 1331.5 | learning rate: 9.563E-05 | global batch size:   128 | lm loss: 2.212323E-06 | loss scale: 268435456.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 69.74 | backward-params-all-reduce: 61.32 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.08 | optimizer-unscale-and-check-inf: 4.75 | optimizer-clip-main-grad: 4.62 | optimizer-copy-main-to-model-params: 5.46 | optimizer: 33.50 | batch-generator: 912.34
 at iteration 5000, match long value: 0.09570162579370908 | match short value: 0.7098900928420343 
---------------------------------------------------------------------------------------------------
 iteration     5100/  200000 | consumed samples:       652800 | elapsed time per iteration (ms): 1350.2 | learning rate: 9.554E-05 | global batch size:   128 | lm loss: 2.377855E-06 | loss scale: 536870912.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 75.02 | backward-params-all-reduce: 57.73 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.55 | optimizer-unscale-and-check-inf: 4.85 | optimizer-clip-main-grad: 4.67 | optimizer-copy-main-to-model-params: 5.80 | optimizer: 34.50 | batch-generator: 921.50
 iteration     5200/  200000 | consumed samples:       665600 | elapsed time per iteration (ms): 1303.6 | learning rate: 9.545E-05 | global batch size:   128 | lm loss: 2.292150E-06 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 78.90 | backward-params-all-reduce: 52.75 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.41 | optimizer-unscale-and-check-inf: 4.78 | optimizer-clip-main-grad: 4.42 | optimizer-copy-main-to-model-params: 5.50 | optimizer: 33.70 | batch-generator: 904.80
 iteration     5300/  200000 | consumed samples:       678400 | elapsed time per iteration (ms): 1335.1 | learning rate: 9.536E-05 | global batch size:   128 | lm loss: 2.185718E-06 | loss scale: 536870912.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 79.26 | backward-params-all-reduce: 48.93 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.53 | optimizer-unscale-and-check-inf: 4.91 | optimizer-clip-main-grad: 4.65 | optimizer-copy-main-to-model-params: 9.81 | optimizer: 38.79 | batch-generator: 947.69
 iteration     5400/  200000 | consumed samples:       691200 | elapsed time per iteration (ms): 1300.0 | learning rate: 9.527E-05 | global batch size:   128 | lm loss: 2.266283E-06 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 70.89 | backward-params-all-reduce: 60.48 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.15 | optimizer-unscale-and-check-inf: 4.73 | optimizer-clip-main-grad: 4.37 | optimizer-copy-main-to-model-params: 5.49 | optimizer: 33.30 | batch-generator: 888.09
 iteration     5500/  200000 | consumed samples:       704000 | elapsed time per iteration (ms): 1186.9 | learning rate: 9.518E-05 | global batch size:   128 | lm loss: 2.084457E-06 | loss scale: 536870912.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 69.77 | backward-params-all-reduce: 54.88 | backward-embedding-all-reduce: 0.06 | optimizer-copy-to-main-grad: 4.76 | optimizer-unscale-and-check-inf: 4.66 | optimizer-clip-main-grad: 3.99 | optimizer-copy-main-to-model-params: 5.24 | optimizer: 32.01 | batch-generator: 793.66
 at iteration 5500, match long value: 0.11739385355554081 | match short value: 0.7881721595875291 
---------------------------------------------------------------------------------------------------
 iteration     5600/  200000 | consumed samples:       716800 | elapsed time per iteration (ms): 1326.8 | learning rate: 9.509E-05 | global batch size:   128 | lm loss: 2.176969E-06 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 77.64 | backward-params-all-reduce: 53.78 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.09 | optimizer-unscale-and-check-inf: 4.67 | optimizer-clip-main-grad: 4.29 | optimizer-copy-main-to-model-params: 5.38 | optimizer: 32.91 | batch-generator: 881.97
 iteration     5700/  200000 | consumed samples:       729600 | elapsed time per iteration (ms): 1279.8 | learning rate: 9.500E-05 | global batch size:   128 | lm loss: 1.991875E-06 | loss scale: 536870912.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 73.82 | backward-params-all-reduce: 59.06 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.38 | optimizer-unscale-and-check-inf: 4.71 | optimizer-clip-main-grad: 4.39 | optimizer-copy-main-to-model-params: 5.63 | optimizer: 33.71 | batch-generator: 886.34
 iteration     5800/  200000 | consumed samples:       742400 | elapsed time per iteration (ms): 1322.7 | learning rate: 9.491E-05 | global batch size:   128 | lm loss: 2.025814E-06 | loss scale: 536870912.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 74.25 | backward-params-all-reduce: 59.85 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.27 | optimizer-unscale-and-check-inf: 4.94 | optimizer-clip-main-grad: 4.52 | optimizer-copy-main-to-model-params: 5.53 | optimizer: 33.80 | batch-generator: 915.90
 iteration     5900/  200000 | consumed samples:       755200 | elapsed time per iteration (ms): 1177.7 | learning rate: 9.481E-05 | global batch size:   128 | lm loss: 1.927240E-06 | loss scale: 536870912.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 65.28 | backward-params-all-reduce: 53.16 | backward-embedding-all-reduce: 0.14 | optimizer-copy-to-main-grad: 4.69 | optimizer-unscale-and-check-inf: 4.51 | optimizer-clip-main-grad: 4.03 | optimizer-copy-main-to-model-params: 7.14 | optimizer: 33.76 | batch-generator: 781.90
 iteration     6000/  200000 | consumed samples:       768000 | elapsed time per iteration (ms): 1241.5 | learning rate: 9.472E-05 | global batch size:   128 | lm loss: 1.865932E-06 | loss scale: 536870912.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 72.25 | backward-params-all-reduce: 65.77 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 4.73 | optimizer-unscale-and-check-inf: 4.52 | optimizer-clip-main-grad: 4.05 | optimizer-copy-main-to-model-params: 5.10 | optimizer: 31.81 | batch-generator: 787.37
 at iteration 6000, match long value: 0.12373635861901564 | match short value: 0.8333065197091193 
---------------------------------------------------------------------------------------------------
 iteration     6100/  200000 | consumed samples:       780800 | elapsed time per iteration (ms): 1264.1 | learning rate: 9.463E-05 | global batch size:   128 | lm loss: 1.881823E-06 | loss scale: 1073741824.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 67.41 | backward-params-all-reduce: 57.43 | backward-embedding-all-reduce: 0.06 | optimizer-copy-to-main-grad: 4.81 | optimizer-unscale-and-check-inf: 4.53 | optimizer-clip-main-grad: 4.01 | optimizer-copy-main-to-model-params: 5.36 | optimizer: 32.08 | batch-generator: 875.00
 iteration     6200/  200000 | consumed samples:       793600 | elapsed time per iteration (ms): 1300.6 | learning rate: 9.454E-05 | global batch size:   128 | lm loss: 1.738173E-06 | loss scale: 1073741824.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 67.50 | backward-params-all-reduce: 62.57 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 4.96 | optimizer-unscale-and-check-inf: 4.69 | optimizer-clip-main-grad: 4.35 | optimizer-copy-main-to-model-params: 5.50 | optimizer: 32.99 | batch-generator: 871.48
 iteration     6300/  200000 | consumed samples:       806400 | elapsed time per iteration (ms): 1338.4 | learning rate: 9.445E-05 | global batch size:   128 | lm loss: 1.873499E-06 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 71.98 | backward-params-all-reduce: 58.59 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.42 | optimizer-unscale-and-check-inf: 4.75 | optimizer-clip-main-grad: 4.44 | optimizer-copy-main-to-model-params: 5.58 | optimizer: 33.72 | batch-generator: 944.34
 iteration     6400/  200000 | consumed samples:       819200 | elapsed time per iteration (ms): 1338.4 | learning rate: 9.436E-05 | global batch size:   128 | lm loss: 1.711088E-06 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 72.69 | backward-params-all-reduce: 59.95 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.07 | optimizer-unscale-and-check-inf: 4.84 | optimizer-clip-main-grad: 4.31 | optimizer-copy-main-to-model-params: 5.43 | optimizer: 33.15 | batch-generator: 936.50
 iteration     6500/  200000 | consumed samples:       832000 | elapsed time per iteration (ms): 1354.9 | learning rate: 9.427E-05 | global batch size:   128 | lm loss: 1.702395E-06 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 70.59 | backward-params-all-reduce: 59.97 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.58 | optimizer-unscale-and-check-inf: 4.93 | optimizer-clip-main-grad: 4.76 | optimizer-copy-main-to-model-params: 5.79 | optimizer: 34.74 | batch-generator: 951.98
 at iteration 6500, match long value: 0.11606811739894586 | match short value: 0.8471062765738789 
---------------------------------------------------------------------------------------------------
 iteration     6600/  200000 | consumed samples:       844800 | elapsed time per iteration (ms): 1387.4 | learning rate: 9.418E-05 | global batch size:   128 | lm loss: 1.771714E-06 | loss scale: 1073741824.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 81.86 | backward-params-all-reduce: 54.30 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.62 | optimizer-unscale-and-check-inf: 4.88 | optimizer-clip-main-grad: 4.67 | optimizer-copy-main-to-model-params: 11.81 | optimizer: 40.71 | batch-generator: 998.49
 iteration     6700/  200000 | consumed samples:       857600 | elapsed time per iteration (ms): 1334.9 | learning rate: 9.409E-05 | global batch size:   128 | lm loss: 1.673900E-06 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 72.81 | backward-params-all-reduce: 60.13 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.30 | optimizer-unscale-and-check-inf: 4.83 | optimizer-clip-main-grad: 4.59 | optimizer-copy-main-to-model-params: 5.67 | optimizer: 33.96 | batch-generator: 919.36
 iteration     6800/  200000 | consumed samples:       870400 | elapsed time per iteration (ms): 1336.6 | learning rate: 9.399E-05 | global batch size:   128 | lm loss: 1.530185E-06 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 68.95 | backward-params-all-reduce: 63.34 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.10 | optimizer-unscale-and-check-inf: 4.75 | optimizer-clip-main-grad: 4.44 | optimizer-copy-main-to-model-params: 5.66 | optimizer: 33.51 | batch-generator: 922.02
 iteration     6900/  200000 | consumed samples:       883200 | elapsed time per iteration (ms): 1332.8 | learning rate: 9.390E-05 | global batch size:   128 | lm loss: 1.611499E-06 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 82.92 | backward-params-all-reduce: 50.07 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.91 | optimizer-unscale-and-check-inf: 4.98 | optimizer-clip-main-grad: 5.03 | optimizer-copy-main-to-model-params: 7.68 | optimizer: 37.42 | batch-generator: 933.92
 iteration     7000/  200000 | consumed samples:       896000 | elapsed time per iteration (ms): 1392.4 | learning rate: 9.381E-05 | global batch size:   128 | lm loss: 1.626658E-06 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 78.63 | backward-params-all-reduce: 61.85 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.50 | optimizer-unscale-and-check-inf: 4.83 | optimizer-clip-main-grad: 4.57 | optimizer-copy-main-to-model-params: 5.81 | optimizer: 34.44 | batch-generator: 955.49
 at iteration 7000, match long value: 0.1458081717248698 | match short value: 0.891991860175137 
-------------------------------------------------------------------------------------------------
 iteration     7100/  200000 | consumed samples:       908800 | elapsed time per iteration (ms): 1332.3 | learning rate: 9.372E-05 | global batch size:   128 | lm loss: 1.558867E-06 | loss scale: 2147483648.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 76.06 | backward-params-all-reduce: 55.49 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 5.56 | optimizer-unscale-and-check-inf: 4.79 | optimizer-clip-main-grad: 4.49 | optimizer-copy-main-to-model-params: 5.53 | optimizer: 33.99 | batch-generator: 936.82
 iteration     7200/  200000 | consumed samples:       921600 | elapsed time per iteration (ms): 1305.3 | learning rate: 9.363E-05 | global batch size:   128 | lm loss: 1.573385E-06 | loss scale: 2147483648.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 73.53 | backward-params-all-reduce: 61.67 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.28 | optimizer-unscale-and-check-inf: 4.68 | optimizer-clip-main-grad: 4.35 | optimizer-copy-main-to-model-params: 5.47 | optimizer: 33.26 | batch-generator: 867.35
 iteration     7300/  200000 | consumed samples:       934400 | elapsed time per iteration (ms): 1243.9 | learning rate: 9.354E-05 | global batch size:   128 | lm loss: 1.539970E-06 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 73.79 | backward-params-all-reduce: 52.42 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.10 | optimizer-unscale-and-check-inf: 4.61 | optimizer-clip-main-grad: 4.32 | optimizer-copy-main-to-model-params: 5.40 | optimizer: 32.90 | batch-generator: 841.55
 iteration     7400/  200000 | consumed samples:       947200 | elapsed time per iteration (ms): 1386.3 | learning rate: 9.345E-05 | global batch size:   128 | lm loss: 1.459028E-06 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 79.50 | backward-params-all-reduce: 53.44 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.67 | optimizer-unscale-and-check-inf: 4.96 | optimizer-clip-main-grad: 4.86 | optimizer-copy-main-to-model-params: 5.82 | optimizer: 35.21 | batch-generator: 975.75
 iteration     7500/  200000 | consumed samples:       960000 | elapsed time per iteration (ms): 1354.4 | learning rate: 9.336E-05 | global batch size:   128 | lm loss: 1.485594E-06 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 76.79 | backward-params-all-reduce: 50.39 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.58 | optimizer-unscale-and-check-inf: 5.26 | optimizer-clip-main-grad: 4.77 | optimizer-copy-main-to-model-params: 7.20 | optimizer: 36.52 | batch-generator: 949.26
 at iteration 7500, match long value: 0.1749823335308473 | match short value: 0.916329993971235 
-------------------------------------------------------------------------------------------------
 iteration     7600/  200000 | consumed samples:       972800 | elapsed time per iteration (ms): 1327.1 | learning rate: 9.327E-05 | global batch size:   128 | lm loss: 1.401069E-06 | loss scale: 2147483648.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 76.91 | backward-params-all-reduce: 72.75 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.19 | optimizer-unscale-and-check-inf: 4.76 | optimizer-clip-main-grad: 4.38 | optimizer-copy-main-to-model-params: 5.99 | optimizer: 33.96 | batch-generator: 904.62
 iteration     7700/  200000 | consumed samples:       985600 | elapsed time per iteration (ms): 1285.2 | learning rate: 9.317E-05 | global batch size:   128 | lm loss: 1.496171E-06 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 70.13 | backward-params-all-reduce: 58.26 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.44 | optimizer-unscale-and-check-inf: 4.83 | optimizer-clip-main-grad: 4.61 | optimizer-copy-main-to-model-params: 5.79 | optimizer: 34.33 | batch-generator: 905.68
 iteration     7800/  200000 | consumed samples:       998400 | elapsed time per iteration (ms): 1294.8 | learning rate: 9.308E-05 | global batch size:   128 | lm loss: 1.396783E-06 | loss scale: 2147483648.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 79.39 | backward-params-all-reduce: 50.70 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 6.07 | optimizer-unscale-and-check-inf: 5.00 | optimizer-clip-main-grad: 4.84 | optimizer-copy-main-to-model-params: 5.84 | optimizer: 35.42 | batch-generator: 919.66
 iteration     7900/  200000 | consumed samples:      1011200 | elapsed time per iteration (ms): 1279.2 | learning rate: 9.299E-05 | global batch size:   128 | lm loss: 1.387307E-06 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 77.56 | backward-params-all-reduce: 53.48 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.12 | optimizer-unscale-and-check-inf: 4.71 | optimizer-clip-main-grad: 4.31 | optimizer-copy-main-to-model-params: 5.75 | optimizer: 33.42 | batch-generator: 873.71
 iteration     8000/  200000 | consumed samples:      1024000 | elapsed time per iteration (ms): 1360.2 | learning rate: 9.290E-05 | global batch size:   128 | lm loss: 1.363391E-06 | loss scale: 2147483648.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 74.43 | backward-params-all-reduce: 59.72 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.34 | optimizer-unscale-and-check-inf: 4.85 | optimizer-clip-main-grad: 4.74 | optimizer-copy-main-to-model-params: 5.82 | optimizer: 34.45 | batch-generator: 929.03
 at iteration 8000, match long value: 0.16317929071604786 | match short value: 0.8946000015390508 
---------------------------------------------------------------------------------------------------
 iteration     8100/  200000 | consumed samples:      1036800 | elapsed time per iteration (ms): 1288.4 | learning rate: 9.281E-05 | global batch size:   128 | lm loss: 1.367826E-06 | loss scale: 4294967296.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 72.15 | backward-params-all-reduce: 53.17 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 4.94 | optimizer-unscale-and-check-inf: 4.78 | optimizer-clip-main-grad: 4.36 | optimizer-copy-main-to-model-params: 5.54 | optimizer: 33.22 | batch-generator: 901.15
 iteration     8200/  200000 | consumed samples:      1049600 | elapsed time per iteration (ms): 1159.8 | learning rate: 9.272E-05 | global batch size:   128 | lm loss: 1.319006E-06 | loss scale: 4294967296.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 67.79 | backward-params-all-reduce: 54.02 | backward-embedding-all-reduce: 0.06 | optimizer-copy-to-main-grad: 4.40 | optimizer-unscale-and-check-inf: 4.42 | optimizer-clip-main-grad: 3.84 | optimizer-copy-main-to-model-params: 5.18 | optimizer: 31.28 | batch-generator: 774.83
 iteration     8300/  200000 | consumed samples:      1062400 | elapsed time per iteration (ms): 1288.0 | learning rate: 9.263E-05 | global batch size:   128 | lm loss: 1.403934E-06 | loss scale: 4294967296.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 77.07 | backward-params-all-reduce: 58.92 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.30 | optimizer-unscale-and-check-inf: 4.69 | optimizer-clip-main-grad: 4.64 | optimizer-copy-main-to-model-params: 5.49 | optimizer: 33.66 | batch-generator: 844.89
 iteration     8400/  200000 | consumed samples:      1075200 | elapsed time per iteration (ms): 1284.1 | learning rate: 9.254E-05 | global batch size:   128 | lm loss: 1.218220E-06 | loss scale: 4294967296.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 73.25 | backward-params-all-reduce: 54.18 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 4.77 | optimizer-clip-main-grad: 4.79 | optimizer-copy-main-to-model-params: 5.49 | optimizer: 33.87 | batch-generator: 885.75
 iteration     8500/  200000 | consumed samples:      1088000 | elapsed time per iteration (ms): 1038.0 | learning rate: 9.245E-05 | global batch size:   128 | lm loss: 1.422773E-06 | loss scale: 4294967296.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 68.27 | backward-params-all-reduce: 39.70 | backward-embedding-all-reduce: 0.06 | optimizer-copy-to-main-grad: 4.61 | optimizer-unscale-and-check-inf: 4.32 | optimizer-clip-main-grad: 3.75 | optimizer-copy-main-to-model-params: 5.00 | optimizer: 31.09 | batch-generator: 697.85
 at iteration 8500, match long value: 0.2312107554119326 | match short value: 0.937034567929141 
-------------------------------------------------------------------------------------------------
 iteration     8600/  200000 | consumed samples:      1100800 | elapsed time per iteration (ms): 1262.4 | learning rate: 9.235E-05 | global batch size:   128 | lm loss: 1.193996E-06 | loss scale: 4294967296.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 76.17 | backward-params-all-reduce: 51.00 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 5.43 | optimizer-unscale-and-check-inf: 4.81 | optimizer-clip-main-grad: 4.56 | optimizer-copy-main-to-model-params: 5.42 | optimizer: 33.81 | batch-generator: 870.74
 iteration     8700/  200000 | consumed samples:      1113600 | elapsed time per iteration (ms): 1310.6 | learning rate: 9.226E-05 | global batch size:   128 | lm loss: 1.328377E-06 | loss scale: 4294967296.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 75.60 | backward-params-all-reduce: 53.62 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.85 | optimizer-unscale-and-check-inf: 5.01 | optimizer-clip-main-grad: 4.66 | optimizer-copy-main-to-model-params: 5.78 | optimizer: 34.98 | batch-generator: 956.48
 iteration     8800/  200000 | consumed samples:      1126400 | elapsed time per iteration (ms): 1377.2 | learning rate: 9.218E-05 | global batch size:   128 | lm loss: 1.225963E-06 | loss scale: 1073741824.0 | grad norm: 0.000 | number of skipped iterations:   3 | number of nan iterations:   0 |
time (ms) | backward-compute: 82.59 | backward-params-all-reduce: 54.93 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.88 | optimizer-unscale-and-check-inf: 5.01 | optimizer-clip-main-grad: 4.74 | optimizer-copy-main-to-model-params: 5.89 | optimizer: 34.95 | batch-generator: 970.89
 iteration     8900/  200000 | consumed samples:      1139200 | elapsed time per iteration (ms): 1355.7 | learning rate: 9.208E-05 | global batch size:   128 | lm loss: 1.222179E-06 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 74.25 | backward-params-all-reduce: 59.13 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.54 | optimizer-unscale-and-check-inf: 4.84 | optimizer-clip-main-grad: 4.49 | optimizer-copy-main-to-model-params: 5.71 | optimizer: 34.25 | batch-generator: 948.53
 iteration     9000/  200000 | consumed samples:      1152000 | elapsed time per iteration (ms): 1321.6 | learning rate: 9.199E-05 | global batch size:   128 | lm loss: 1.231033E-06 | loss scale: 1073741824.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 70.01 | backward-params-all-reduce: 59.43 | backward-embedding-all-reduce: 0.06 | optimizer-copy-to-main-grad: 5.25 | optimizer-unscale-and-check-inf: 4.73 | optimizer-clip-main-grad: 4.39 | optimizer-copy-main-to-model-params: 5.43 | optimizer: 33.39 | batch-generator: 924.65
 at iteration 9000, match long value: 0.23273028333055307 | match short value: 0.9266400054896513 
---------------------------------------------------------------------------------------------------
 iteration     9100/  200000 | consumed samples:      1164800 | elapsed time per iteration (ms): 1347.7 | learning rate: 9.190E-05 | global batch size:   128 | lm loss: 1.232889E-06 | loss scale: 1073741824.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 70.72 | backward-params-all-reduce: 62.24 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.11 | optimizer-unscale-and-check-inf: 4.72 | optimizer-clip-main-grad: 4.33 | optimizer-copy-main-to-model-params: 10.03 | optimizer: 37.69 | batch-generator: 925.52
 iteration     9200/  200000 | consumed samples:      1177600 | elapsed time per iteration (ms): 1339.8 | learning rate: 9.181E-05 | global batch size:   128 | lm loss: 1.172583E-06 | loss scale: 1073741824.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 77.84 | backward-params-all-reduce: 53.90 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.63 | optimizer-unscale-and-check-inf: 5.16 | optimizer-clip-main-grad: 4.83 | optimizer-copy-main-to-model-params: 5.89 | optimizer: 35.14 | batch-generator: 951.74
 iteration     9300/  200000 | consumed samples:      1190400 | elapsed time per iteration (ms): 1368.0 | learning rate: 9.172E-05 | global batch size:   128 | lm loss: 1.190131E-06 | loss scale: 1073741824.0 | grad norm: 0.001 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 79.10 | backward-params-all-reduce: 56.34 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 6.32 | optimizer-unscale-and-check-inf: 5.01 | optimizer-clip-main-grad: 4.77 | optimizer-copy-main-to-model-params: 5.94 | optimizer: 35.78 | batch-generator: 970.36
 iteration     9400/  200000 | consumed samples:      1203200 | elapsed time per iteration (ms): 1299.2 | learning rate: 9.163E-05 | global batch size:   128 | lm loss: 1.160298E-06 | loss scale: 1073741824.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 74.76 | backward-params-all-reduce: 57.72 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.36 | optimizer-unscale-and-check-inf: 4.89 | optimizer-clip-main-grad: 4.58 | optimizer-copy-main-to-model-params: 5.59 | optimizer: 34.06 | batch-generator: 889.43
 iteration     9500/  200000 | consumed samples:      1216000 | elapsed time per iteration (ms): 1323.3 | learning rate: 9.154E-05 | global batch size:   128 | lm loss: 1.183312E-06 | loss scale: 1073741824.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 77.11 | backward-params-all-reduce: 55.48 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.42 | optimizer-unscale-and-check-inf: 4.84 | optimizer-clip-main-grad: 4.48 | optimizer-copy-main-to-model-params: 5.70 | optimizer: 34.15 | batch-generator: 935.98
 at iteration 9500, match long value: 0.2900290285339472 | match short value: 0.9498334635051827 
--------------------------------------------------------------------------------------------------
 iteration     9600/  200000 | consumed samples:      1228800 | elapsed time per iteration (ms): 1348.5 | learning rate: 9.145E-05 | global batch size:   128 | lm loss: 1.100978E-06 | loss scale: 1073741824.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 77.04 | backward-params-all-reduce: 57.17 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.51 | optimizer-unscale-and-check-inf: 4.90 | optimizer-clip-main-grad: 4.68 | optimizer-copy-main-to-model-params: 5.84 | optimizer: 34.63 | batch-generator: 959.05
 iteration     9700/  200000 | consumed samples:      1241600 | elapsed time per iteration (ms): 1328.6 | learning rate: 9.136E-05 | global batch size:   128 | lm loss: 1.115093E-06 | loss scale: 1073741824.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 68.60 | backward-params-all-reduce: 61.25 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 4.91 | optimizer-clip-main-grad: 4.32 | optimizer-copy-main-to-model-params: 5.54 | optimizer: 33.48 | batch-generator: 896.10
 iteration     9800/  200000 | consumed samples:      1254400 | elapsed time per iteration (ms): 1183.6 | learning rate: 9.126E-05 | global batch size:   128 | lm loss: 1.079374E-06 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 72.30 | backward-params-all-reduce: 49.77 | backward-embedding-all-reduce: 0.06 | optimizer-copy-to-main-grad: 4.76 | optimizer-unscale-and-check-inf: 4.53 | optimizer-clip-main-grad: 3.98 | optimizer-copy-main-to-model-params: 11.53 | optimizer: 38.18 | batch-generator: 813.55
 iteration     9900/  200000 | consumed samples:      1267200 | elapsed time per iteration (ms): 1353.2 | learning rate: 9.117E-05 | global batch size:   128 | lm loss: 1.093278E-06 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 75.22 | backward-params-all-reduce: 56.79 | backward-embedding-all-reduce: 0.20 | optimizer-copy-to-main-grad: 5.64 | optimizer-unscale-and-check-inf: 4.96 | optimizer-clip-main-grad: 4.73 | optimizer-copy-main-to-model-params: 5.78 | optimizer: 35.06 | batch-generator: 979.69
 iteration    10000/  200000 | consumed samples:      1280000 | elapsed time per iteration (ms): 1279.0 | learning rate: 9.108E-05 | global batch size:   128 | lm loss: 1.151333E-06 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 70.11 | backward-params-all-reduce: 58.47 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.36 | optimizer-unscale-and-check-inf: 4.79 | optimizer-clip-main-grad: 4.46 | optimizer-copy-main-to-model-params: 5.58 | optimizer: 33.75 | batch-generator: 899.24
 at iteration 10000, match long value: 0.2921062539885335 | match short value: 0.939256330746182 
--------------------------------------------------------------------------------------------------
 iteration    10100/  200000 | consumed samples:      1292800 | elapsed time per iteration (ms): 1295.7 | learning rate: 9.099E-05 | global batch size:   128 | lm loss: 1.035797E-06 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 66.36 | backward-params-all-reduce: 57.85 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 4.90 | optimizer-unscale-and-check-inf: 4.71 | optimizer-clip-main-grad: 4.27 | optimizer-copy-main-to-model-params: 5.35 | optimizer: 32.69 | batch-generator: 893.88
 iteration    10200/  200000 | consumed samples:      1305600 | elapsed time per iteration (ms): 1236.5 | learning rate: 9.090E-05 | global batch size:   128 | lm loss: 1.042823E-06 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 69.46 | backward-params-all-reduce: 55.28 | backward-embedding-all-reduce: 0.06 | optimizer-copy-to-main-grad: 4.87 | optimizer-unscale-and-check-inf: 4.65 | optimizer-clip-main-grad: 4.19 | optimizer-copy-main-to-model-params: 5.32 | optimizer: 32.48 | batch-generator: 858.51
 iteration    10300/  200000 | consumed samples:      1318400 | elapsed time per iteration (ms): 1197.6 | learning rate: 9.081E-05 | global batch size:   128 | lm loss: 1.027552E-06 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 68.63 | backward-params-all-reduce: 64.49 | backward-embedding-all-reduce: 0.06 | optimizer-copy-to-main-grad: 4.63 | optimizer-unscale-and-check-inf: 4.45 | optimizer-clip-main-grad: 3.96 | optimizer-copy-main-to-model-params: 5.12 | optimizer: 31.66 | batch-generator: 805.27
 iteration    10400/  200000 | consumed samples:      1331200 | elapsed time per iteration (ms): 1334.2 | learning rate: 9.072E-05 | global batch size:   128 | lm loss: 1.003708E-06 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 76.00 | backward-params-all-reduce: 53.78 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.45 | optimizer-unscale-and-check-inf: 4.97 | optimizer-clip-main-grad: 4.60 | optimizer-copy-main-to-model-params: 5.79 | optimizer: 34.44 | batch-generator: 949.58
 iteration    10500/  200000 | consumed samples:      1344000 | elapsed time per iteration (ms): 1313.2 | learning rate: 9.063E-05 | global batch size:   128 | lm loss: 1.045350E-06 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 69.82 | backward-params-all-reduce: 57.41 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.03 | optimizer-unscale-and-check-inf: 4.76 | optimizer-clip-main-grad: 4.37 | optimizer-copy-main-to-model-params: 5.65 | optimizer: 33.44 | batch-generator: 874.74
 at iteration 10500, match long value: 0.2993704339732267 | match short value: 0.9344968685238364 
---------------------------------------------------------------------------------------------------
 iteration    10600/  200000 | consumed samples:      1356800 | elapsed time per iteration (ms): 1166.0 | learning rate: 9.054E-05 | global batch size:   128 | lm loss: 1.027890E-06 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 72.96 | backward-params-all-reduce: 53.20 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 7.78 | optimizer-unscale-and-check-inf: 5.04 | optimizer-clip-main-grad: 5.36 | optimizer-copy-main-to-model-params: 5.94 | optimizer: 37.69 | batch-generator: 760.93
 iteration    10700/  200000 | consumed samples:      1369600 | elapsed time per iteration (ms): 1153.9 | learning rate: 9.044E-05 | global batch size:   128 | lm loss: 9.916501E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 77.00 | backward-params-all-reduce: 47.35 | backward-embedding-all-reduce: 0.06 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 4.53 | optimizer-clip-main-grad: 4.08 | optimizer-copy-main-to-model-params: 5.29 | optimizer: 32.47 | batch-generator: 749.92
 iteration    10800/  200000 | consumed samples:      1382400 | elapsed time per iteration (ms): 1246.0 | learning rate: 9.036E-05 | global batch size:   128 | lm loss: 9.831506E-07 | loss scale: 2147483648.0 | grad norm: 0.001 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 69.89 | backward-params-all-reduce: 62.21 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.63 | optimizer-unscale-and-check-inf: 5.08 | optimizer-clip-main-grad: 4.52 | optimizer-copy-main-to-model-params: 5.56 | optimizer: 34.28 | batch-generator: 856.91
 iteration    10900/  200000 | consumed samples:      1395200 | elapsed time per iteration (ms): 985.5 | learning rate: 9.026E-05 | global batch size:   128 | lm loss: 9.750132E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 68.95 | backward-params-all-reduce: 42.75 | backward-embedding-all-reduce: 0.05 | optimizer-copy-to-main-grad: 4.49 | optimizer-unscale-and-check-inf: 5.55 | optimizer-clip-main-grad: 4.88 | optimizer-copy-main-to-model-params: 5.16 | optimizer: 33.28 | batch-generator: 642.36
 iteration    11000/  200000 | consumed samples:      1408000 | elapsed time per iteration (ms): 1210.9 | learning rate: 9.017E-05 | global batch size:   128 | lm loss: 9.292122E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 75.17 | backward-params-all-reduce: 60.74 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.14 | optimizer-unscale-and-check-inf: 4.66 | optimizer-clip-main-grad: 4.24 | optimizer-copy-main-to-model-params: 5.56 | optimizer: 33.17 | batch-generator: 798.04
 at iteration 11000, match long value: 0.3066376147002129 | match short value: 0.9314206859045611 
---------------------------------------------------------------------------------------------------
 iteration    11100/  200000 | consumed samples:      1420800 | elapsed time per iteration (ms): 1225.2 | learning rate: 9.008E-05 | global batch size:   128 | lm loss: 9.425681E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 78.43 | backward-params-all-reduce: 43.71 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.37 | optimizer-unscale-and-check-inf: 4.92 | optimizer-clip-main-grad: 4.66 | optimizer-copy-main-to-model-params: 7.72 | optimizer: 36.35 | batch-generator: 857.36
 iteration    11200/  200000 | consumed samples:      1433600 | elapsed time per iteration (ms): 1283.0 | learning rate: 8.999E-05 | global batch size:   128 | lm loss: 9.694601E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 84.99 | backward-params-all-reduce: 43.63 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.13 | optimizer-unscale-and-check-inf: 5.09 | optimizer-clip-main-grad: 5.02 | optimizer-copy-main-to-model-params: 6.19 | optimizer: 36.31 | batch-generator: 941.90
 iteration    11300/  200000 | consumed samples:      1446400 | elapsed time per iteration (ms): 1347.1 | learning rate: 8.990E-05 | global batch size:   128 | lm loss: 9.359473E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 80.52 | backward-params-all-reduce: 55.88 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 5.98 | optimizer-unscale-and-check-inf: 5.20 | optimizer-clip-main-grad: 5.20 | optimizer-copy-main-to-model-params: 6.37 | optimizer: 36.73 | batch-generator: 984.68
 iteration    11400/  200000 | consumed samples:      1459200 | elapsed time per iteration (ms): 1344.5 | learning rate: 8.981E-05 | global batch size:   128 | lm loss: 8.993915E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 77.63 | backward-params-all-reduce: 53.54 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 5.92 | optimizer-unscale-and-check-inf: 5.13 | optimizer-clip-main-grad: 4.97 | optimizer-copy-main-to-model-params: 7.95 | optimizer: 37.79 | batch-generator: 954.16
 iteration    11500/  200000 | consumed samples:      1472000 | elapsed time per iteration (ms): 1348.6 | learning rate: 8.972E-05 | global batch size:   128 | lm loss: 8.905398E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 78.54 | backward-params-all-reduce: 53.53 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.12 | optimizer-unscale-and-check-inf: 5.15 | optimizer-clip-main-grad: 5.07 | optimizer-copy-main-to-model-params: 6.21 | optimizer: 36.48 | batch-generator: 989.62
 at iteration 11500, match long value: 0.3568522246499686 | match short value: 0.9467440912855242 
---------------------------------------------------------------------------------------------------
 iteration    11600/  200000 | consumed samples:      1484800 | elapsed time per iteration (ms): 1349.5 | learning rate: 8.963E-05 | global batch size:   128 | lm loss: 8.617448E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 82.20 | backward-params-all-reduce: 47.26 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.44 | optimizer-unscale-and-check-inf: 5.18 | optimizer-clip-main-grad: 5.39 | optimizer-copy-main-to-model-params: 6.62 | optimizer: 37.72 | batch-generator: 998.56
 iteration    11700/  200000 | consumed samples:      1497600 | elapsed time per iteration (ms): 1316.3 | learning rate: 8.954E-05 | global batch size:   128 | lm loss: 8.454069E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 81.17 | backward-params-all-reduce: 48.81 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 5.90 | optimizer-unscale-and-check-inf: 5.10 | optimizer-clip-main-grad: 4.99 | optimizer-copy-main-to-model-params: 6.03 | optimizer: 35.81 | batch-generator: 944.41
 iteration    11800/  200000 | consumed samples:      1510400 | elapsed time per iteration (ms): 1319.7 | learning rate: 8.944E-05 | global batch size:   128 | lm loss: 8.981652E-07 | loss scale: 4294967296.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 77.05 | backward-params-all-reduce: 50.52 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.79 | optimizer-unscale-and-check-inf: 5.01 | optimizer-clip-main-grad: 4.80 | optimizer-copy-main-to-model-params: 5.83 | optimizer: 35.25 | batch-generator: 950.97
 iteration    11900/  200000 | consumed samples:      1523200 | elapsed time per iteration (ms): 1312.6 | learning rate: 8.935E-05 | global batch size:   128 | lm loss: 8.248925E-07 | loss scale: 4294967296.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 80.15 | backward-params-all-reduce: 48.76 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.71 | optimizer-unscale-and-check-inf: 5.13 | optimizer-clip-main-grad: 4.93 | optimizer-copy-main-to-model-params: 6.35 | optimizer: 36.00 | batch-generator: 946.02
 iteration    12000/  200000 | consumed samples:      1536000 | elapsed time per iteration (ms): 1291.3 | learning rate: 8.926E-05 | global batch size:   128 | lm loss: 9.009500E-07 | loss scale: 4294967296.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 79.76 | backward-params-all-reduce: 43.67 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.18 | optimizer-unscale-and-check-inf: 5.12 | optimizer-clip-main-grad: 5.31 | optimizer-copy-main-to-model-params: 6.26 | optimizer: 37.21 | batch-generator: 918.29
 at iteration 12000, match long value: 0.37518064656921424 | match short value: 0.9476674528607583 
----------------------------------------------------------------------------------------------------
 iteration    12100/  200000 | consumed samples:      1548800 | elapsed time per iteration (ms): 1307.6 | learning rate: 8.917E-05 | global batch size:   128 | lm loss: 8.540632E-07 | loss scale: 4294967296.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 84.70 | backward-params-all-reduce: 41.65 | backward-embedding-all-reduce: 0.12 | optimizer-copy-to-main-grad: 6.09 | optimizer-unscale-and-check-inf: 5.15 | optimizer-clip-main-grad: 5.27 | optimizer-copy-main-to-model-params: 6.58 | optimizer: 37.16 | batch-generator: 949.52
 iteration    12200/  200000 | consumed samples:      1561600 | elapsed time per iteration (ms): 1229.9 | learning rate: 8.908E-05 | global batch size:   128 | lm loss: 8.101253E-07 | loss scale: 4294967296.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 71.93 | backward-params-all-reduce: 52.15 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.58 | optimizer-unscale-and-check-inf: 4.84 | optimizer-clip-main-grad: 4.68 | optimizer-copy-main-to-model-params: 5.92 | optimizer: 34.74 | batch-generator: 849.99
 iteration    12300/  200000 | consumed samples:      1574400 | elapsed time per iteration (ms): 1293.0 | learning rate: 8.899E-05 | global batch size:   128 | lm loss: 8.344723E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 84.20 | backward-params-all-reduce: 44.47 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.94 | optimizer-unscale-and-check-inf: 4.97 | optimizer-clip-main-grad: 4.80 | optimizer-copy-main-to-model-params: 5.87 | optimizer: 35.00 | batch-generator: 925.11
 iteration    12400/  200000 | consumed samples:      1587200 | elapsed time per iteration (ms): 1293.7 | learning rate: 8.890E-05 | global batch size:   128 | lm loss: 8.802400E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 80.48 | backward-params-all-reduce: 43.49 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 5.90 | optimizer-unscale-and-check-inf: 5.07 | optimizer-clip-main-grad: 5.09 | optimizer-copy-main-to-model-params: 8.40 | optimizer: 38.30 | batch-generator: 910.86
 iteration    12500/  200000 | consumed samples:      1600000 | elapsed time per iteration (ms): 1311.7 | learning rate: 8.881E-05 | global batch size:   128 | lm loss: 7.294305E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 76.02 | backward-params-all-reduce: 54.56 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 6.16 | optimizer-unscale-and-check-inf: 5.09 | optimizer-clip-main-grad: 5.02 | optimizer-copy-main-to-model-params: 6.03 | optimizer: 36.16 | batch-generator: 941.05
 at iteration 12500, match long value: 0.3500608806167824 | match short value: 0.9323933324704888 
---------------------------------------------------------------------------------------------------
 iteration    12600/  200000 | consumed samples:      1612800 | elapsed time per iteration (ms): 1050.3 | learning rate: 8.872E-05 | global batch size:   128 | lm loss: 8.498920E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 67.49 | backward-params-all-reduce: 42.80 | backward-embedding-all-reduce: 0.05 | optimizer-copy-to-main-grad: 4.85 | optimizer-unscale-and-check-inf: 4.44 | optimizer-clip-main-grad: 3.85 | optimizer-copy-main-to-model-params: 5.09 | optimizer: 31.64 | batch-generator: 730.21
 iteration    12700/  200000 | consumed samples:      1625600 | elapsed time per iteration (ms): 1380.2 | learning rate: 8.863E-05 | global batch size:   128 | lm loss: 7.900302E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 79.02 | backward-params-all-reduce: 56.91 | backward-embedding-all-reduce: 0.13 | optimizer-copy-to-main-grad: 5.92 | optimizer-unscale-and-check-inf: 5.05 | optimizer-clip-main-grad: 4.92 | optimizer-copy-main-to-model-params: 5.96 | optimizer: 35.66 | batch-generator: 978.84
 iteration    12800/  200000 | consumed samples:      1638400 | elapsed time per iteration (ms): 1330.7 | learning rate: 8.854E-05 | global batch size:   128 | lm loss: 7.919814E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 72.87 | backward-params-all-reduce: 63.96 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.28 | optimizer-unscale-and-check-inf: 5.35 | optimizer-clip-main-grad: 5.19 | optimizer-copy-main-to-model-params: 6.58 | optimizer: 37.39 | batch-generator: 943.76
 iteration    12900/  200000 | consumed samples:      1651200 | elapsed time per iteration (ms): 1341.9 | learning rate: 8.844E-05 | global batch size:   128 | lm loss: 7.740324E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 74.66 | backward-params-all-reduce: 61.36 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.41 | optimizer-unscale-and-check-inf: 5.14 | optimizer-clip-main-grad: 5.38 | optimizer-copy-main-to-model-params: 6.53 | optimizer: 37.51 | batch-generator: 967.20
 iteration    13000/  200000 | consumed samples:      1664000 | elapsed time per iteration (ms): 1335.4 | learning rate: 8.835E-05 | global batch size:   128 | lm loss: 7.911772E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 72.88 | backward-params-all-reduce: 56.72 | backward-embedding-all-reduce: 0.10 | optimizer-copy-to-main-grad: 6.48 | optimizer-unscale-and-check-inf: 5.29 | optimizer-clip-main-grad: 5.27 | optimizer-copy-main-to-model-params: 7.78 | optimizer: 38.86 | batch-generator: 951.51
 at iteration 13000, match long value: 0.3617360851363962 | match short value: 0.9312801108104588 
---------------------------------------------------------------------------------------------------
 iteration    13100/  200000 | consumed samples:      1676800 | elapsed time per iteration (ms): 1200.7 | learning rate: 8.826E-05 | global batch size:   128 | lm loss: 7.610275E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 71.03 | backward-params-all-reduce: 48.61 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.67 | optimizer-unscale-and-check-inf: 4.87 | optimizer-clip-main-grad: 4.66 | optimizer-copy-main-to-model-params: 5.91 | optimizer: 34.92 | batch-generator: 855.58
 iteration    13200/  200000 | consumed samples:      1689600 | elapsed time per iteration (ms): 1087.0 | learning rate: 8.817E-05 | global batch size:   128 | lm loss: 7.640821E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 69.82 | backward-params-all-reduce: 43.42 | backward-embedding-all-reduce: 0.06 | optimizer-copy-to-main-grad: 5.07 | optimizer-unscale-and-check-inf: 4.58 | optimizer-clip-main-grad: 4.36 | optimizer-copy-main-to-model-params: 5.59 | optimizer: 33.24 | batch-generator: 760.35
 iteration    13300/  200000 | consumed samples:      1702400 | elapsed time per iteration (ms): 1053.0 | learning rate: 8.808E-05 | global batch size:   128 | lm loss: 7.558297E-07 | loss scale: 4294967296.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 66.86 | backward-params-all-reduce: 45.57 | backward-embedding-all-reduce: 0.06 | optimizer-copy-to-main-grad: 5.34 | optimizer-unscale-and-check-inf: 4.49 | optimizer-clip-main-grad: 4.00 | optimizer-copy-main-to-model-params: 5.27 | optimizer: 32.47 | batch-generator: 708.08
 iteration    13400/  200000 | consumed samples:      1715200 | elapsed time per iteration (ms): 1268.3 | learning rate: 8.799E-05 | global batch size:   128 | lm loss: 8.314666E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 72.52 | backward-params-all-reduce: 50.58 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.31 | optimizer-unscale-and-check-inf: 5.43 | optimizer-clip-main-grad: 4.96 | optimizer-copy-main-to-model-params: 6.68 | optimizer: 36.94 | batch-generator: 907.04
 iteration    13500/  200000 | consumed samples:      1728000 | elapsed time per iteration (ms): 1183.9 | learning rate: 8.790E-05 | global batch size:   128 | lm loss: 7.058417E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 75.06 | backward-params-all-reduce: 49.29 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.60 | optimizer-unscale-and-check-inf: 4.76 | optimizer-clip-main-grad: 4.67 | optimizer-copy-main-to-model-params: 5.89 | optimizer: 34.60 | batch-generator: 830.85
 at iteration 13500, match long value: 0.42147960698115666 | match short value: 0.9525548496520698 
----------------------------------------------------------------------------------------------------
 iteration    13600/  200000 | consumed samples:      1740800 | elapsed time per iteration (ms): 1347.1 | learning rate: 8.781E-05 | global batch size:   128 | lm loss: 7.034651E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 75.33 | backward-params-all-reduce: 57.79 | backward-embedding-all-reduce: 0.12 | optimizer-copy-to-main-grad: 6.44 | optimizer-unscale-and-check-inf: 5.13 | optimizer-clip-main-grad: 5.33 | optimizer-copy-main-to-model-params: 12.21 | optimizer: 43.13 | batch-generator: 940.84
 iteration    13700/  200000 | consumed samples:      1753600 | elapsed time per iteration (ms): 1289.7 | learning rate: 8.772E-05 | global batch size:   128 | lm loss: 7.328540E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 75.57 | backward-params-all-reduce: 50.72 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.78 | optimizer-unscale-and-check-inf: 4.90 | optimizer-clip-main-grad: 4.76 | optimizer-copy-main-to-model-params: 6.18 | optimizer: 35.40 | batch-generator: 926.03
 iteration    13800/  200000 | consumed samples:      1766400 | elapsed time per iteration (ms): 1336.2 | learning rate: 8.763E-05 | global batch size:   128 | lm loss: 7.582983E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 73.54 | backward-params-all-reduce: 51.50 | backward-embedding-all-reduce: 0.10 | optimizer-copy-to-main-grad: 6.21 | optimizer-unscale-and-check-inf: 5.23 | optimizer-clip-main-grad: 5.15 | optimizer-copy-main-to-model-params: 6.26 | optimizer: 36.88 | batch-generator: 988.65
 iteration    13900/  200000 | consumed samples:      1779200 | elapsed time per iteration (ms): 1347.7 | learning rate: 8.754E-05 | global batch size:   128 | lm loss: 7.027454E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 74.81 | backward-params-all-reduce: 52.26 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 6.07 | optimizer-unscale-and-check-inf: 5.10 | optimizer-clip-main-grad: 5.05 | optimizer-copy-main-to-model-params: 6.16 | optimizer: 36.51 | batch-generator: 987.73
 iteration    14000/  200000 | consumed samples:      1792000 | elapsed time per iteration (ms): 1358.2 | learning rate: 8.744E-05 | global batch size:   128 | lm loss: 6.860925E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 77.24 | backward-params-all-reduce: 47.71 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 5.90 | optimizer-unscale-and-check-inf: 5.14 | optimizer-clip-main-grad: 5.08 | optimizer-copy-main-to-model-params: 6.27 | optimizer: 36.27 | batch-generator: 980.32
 at iteration 14000, match long value: 0.40974557462965966 | match short value: 0.9458779693485547 
----------------------------------------------------------------------------------------------------
 iteration    14100/  200000 | consumed samples:      1804800 | elapsed time per iteration (ms): 1330.3 | learning rate: 8.735E-05 | global batch size:   128 | lm loss: 7.379078E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 72.44 | backward-params-all-reduce: 48.37 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.17 | optimizer-unscale-and-check-inf: 5.19 | optimizer-clip-main-grad: 5.13 | optimizer-copy-main-to-model-params: 6.22 | optimizer: 36.64 | batch-generator: 989.70
 iteration    14200/  200000 | consumed samples:      1817600 | elapsed time per iteration (ms): 1374.2 | learning rate: 8.726E-05 | global batch size:   128 | lm loss: 7.246525E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 75.81 | backward-params-all-reduce: 54.79 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 6.21 | optimizer-unscale-and-check-inf: 5.29 | optimizer-clip-main-grad: 5.20 | optimizer-copy-main-to-model-params: 6.30 | optimizer: 36.95 | batch-generator: 1003.47
 iteration    14300/  200000 | consumed samples:      1830400 | elapsed time per iteration (ms): 1382.2 | learning rate: 8.717E-05 | global batch size:   128 | lm loss: 6.812800E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 78.39 | backward-params-all-reduce: 56.43 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 6.26 | optimizer-unscale-and-check-inf: 5.25 | optimizer-clip-main-grad: 5.04 | optimizer-copy-main-to-model-params: 7.72 | optimizer: 38.25 | batch-generator: 981.14
 iteration    14400/  200000 | consumed samples:      1843200 | elapsed time per iteration (ms): 1351.2 | learning rate: 8.708E-05 | global batch size:   128 | lm loss: 6.544898E-07 | loss scale: 4294967296.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 73.81 | backward-params-all-reduce: 52.10 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 6.12 | optimizer-unscale-and-check-inf: 5.12 | optimizer-clip-main-grad: 5.14 | optimizer-copy-main-to-model-params: 6.04 | optimizer: 36.26 | batch-generator: 984.65
 iteration    14500/  200000 | consumed samples:      1856000 | elapsed time per iteration (ms): 1327.3 | learning rate: 8.699E-05 | global batch size:   128 | lm loss: 7.091094E-07 | loss scale: 4294967296.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 73.50 | backward-params-all-reduce: 52.45 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.08 | optimizer-unscale-and-check-inf: 5.18 | optimizer-clip-main-grad: 5.07 | optimizer-copy-main-to-model-params: 6.18 | optimizer: 36.39 | batch-generator: 963.16
 at iteration 14500, match long value: 0.43215540602564567 | match short value: 0.9508399524790013 
----------------------------------------------------------------------------------------------------
 iteration    14600/  200000 | consumed samples:      1868800 | elapsed time per iteration (ms): 1368.5 | learning rate: 8.690E-05 | global batch size:   128 | lm loss: 6.791489E-07 | loss scale: 4294967296.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 74.41 | backward-params-all-reduce: 55.19 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.22 | optimizer-unscale-and-check-inf: 5.24 | optimizer-clip-main-grad: 5.47 | optimizer-copy-main-to-model-params: 11.85 | optimizer: 42.84 | batch-generator: 980.43
 iteration    14700/  200000 | consumed samples:      1881600 | elapsed time per iteration (ms): 1302.9 | learning rate: 8.681E-05 | global batch size:   128 | lm loss: 6.995181E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 77.17 | backward-params-all-reduce: 52.48 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 6.01 | optimizer-unscale-and-check-inf: 4.90 | optimizer-clip-main-grad: 4.77 | optimizer-copy-main-to-model-params: 6.17 | optimizer: 35.42 | batch-generator: 942.29
 iteration    14800/  200000 | consumed samples:      1894400 | elapsed time per iteration (ms): 1384.8 | learning rate: 8.672E-05 | global batch size:   128 | lm loss: 6.651122E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 75.45 | backward-params-all-reduce: 58.02 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 6.11 | optimizer-unscale-and-check-inf: 5.10 | optimizer-clip-main-grad: 5.14 | optimizer-copy-main-to-model-params: 6.22 | optimizer: 36.45 | batch-generator: 986.50
 iteration    14900/  200000 | consumed samples:      1907200 | elapsed time per iteration (ms): 1334.6 | learning rate: 8.663E-05 | global batch size:   128 | lm loss: 6.744423E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 73.85 | backward-params-all-reduce: 55.32 | backward-embedding-all-reduce: 0.16 | optimizer-copy-to-main-grad: 7.03 | optimizer-unscale-and-check-inf: 4.97 | optimizer-clip-main-grad: 5.16 | optimizer-copy-main-to-model-params: 7.44 | optimizer: 38.60 | batch-generator: 935.14
 iteration    15000/  200000 | consumed samples:      1920000 | elapsed time per iteration (ms): 1293.4 | learning rate: 8.653E-05 | global batch size:   128 | lm loss: 6.682050E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 77.40 | backward-params-all-reduce: 55.08 | backward-embedding-all-reduce: 0.12 | optimizer-copy-to-main-grad: 5.98 | optimizer-unscale-and-check-inf: 5.00 | optimizer-clip-main-grad: 4.78 | optimizer-copy-main-to-model-params: 5.99 | optimizer: 35.50 | batch-generator: 900.22
 at iteration 15000, match long value: 0.3956448162198981 | match short value: 0.9318580050842915 
---------------------------------------------------------------------------------------------------
 iteration    15100/  200000 | consumed samples:      1932800 | elapsed time per iteration (ms): 1349.1 | learning rate: 8.644E-05 | global batch size:   128 | lm loss: 6.624679E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 76.28 | backward-params-all-reduce: 61.66 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 6.11 | optimizer-unscale-and-check-inf: 5.14 | optimizer-clip-main-grad: 4.96 | optimizer-copy-main-to-model-params: 6.11 | optimizer: 36.15 | batch-generator: 939.83
 iteration    15200/  200000 | consumed samples:      1945600 | elapsed time per iteration (ms): 1325.4 | learning rate: 8.635E-05 | global batch size:   128 | lm loss: 6.476167E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 72.53 | backward-params-all-reduce: 61.78 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.71 | optimizer-unscale-and-check-inf: 5.04 | optimizer-clip-main-grad: 5.00 | optimizer-copy-main-to-model-params: 6.32 | optimizer: 35.89 | batch-generator: 939.63
 iteration    15300/  200000 | consumed samples:      1958400 | elapsed time per iteration (ms): 1320.5 | learning rate: 8.626E-05 | global batch size:   128 | lm loss: 6.376577E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 76.16 | backward-params-all-reduce: 55.80 | backward-embedding-all-reduce: 0.10 | optimizer-copy-to-main-grad: 5.55 | optimizer-unscale-and-check-inf: 4.95 | optimizer-clip-main-grad: 4.75 | optimizer-copy-main-to-model-params: 5.91 | optimizer: 34.90 | batch-generator: 932.99
 iteration    15400/  200000 | consumed samples:      1971200 | elapsed time per iteration (ms): 1280.4 | learning rate: 8.617E-05 | global batch size:   128 | lm loss: 6.278549E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 77.62 | backward-params-all-reduce: 54.16 | backward-embedding-all-reduce: 0.12 | optimizer-copy-to-main-grad: 5.65 | optimizer-unscale-and-check-inf: 4.84 | optimizer-clip-main-grad: 4.71 | optimizer-copy-main-to-model-params: 5.72 | optimizer: 34.77 | batch-generator: 886.04
 iteration    15500/  200000 | consumed samples:      1984000 | elapsed time per iteration (ms): 1269.5 | learning rate: 8.608E-05 | global batch size:   128 | lm loss: 6.592747E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 76.26 | backward-params-all-reduce: 53.82 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.54 | optimizer-unscale-and-check-inf: 4.86 | optimizer-clip-main-grad: 4.65 | optimizer-copy-main-to-model-params: 5.79 | optimizer: 34.56 | batch-generator: 863.44
 at iteration 15500, match long value: 0.43488438258652445 | match short value: 0.9463224411174829 
----------------------------------------------------------------------------------------------------
 iteration    15600/  200000 | consumed samples:      1996800 | elapsed time per iteration (ms): 1277.0 | learning rate: 8.599E-05 | global batch size:   128 | lm loss: 6.251903E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 72.77 | backward-params-all-reduce: 54.24 | backward-embedding-all-reduce: 0.11 | optimizer-copy-to-main-grad: 5.92 | optimizer-unscale-and-check-inf: 4.91 | optimizer-clip-main-grad: 4.66 | optimizer-copy-main-to-model-params: 9.44 | optimizer: 38.84 | batch-generator: 882.01
 iteration    15700/  200000 | consumed samples:      2009600 | elapsed time per iteration (ms): 1374.4 | learning rate: 8.590E-05 | global batch size:   128 | lm loss: 6.289420E-07 | loss scale: 4294967296.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 76.94 | backward-params-all-reduce: 51.04 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.08 | optimizer-unscale-and-check-inf: 5.18 | optimizer-clip-main-grad: 5.15 | optimizer-copy-main-to-model-params: 6.50 | optimizer: 36.88 | batch-generator: 982.50
 iteration    15800/  200000 | consumed samples:      2022400 | elapsed time per iteration (ms): 1130.3 | learning rate: 8.581E-05 | global batch size:   128 | lm loss: 6.579031E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 70.28 | backward-params-all-reduce: 42.25 | backward-embedding-all-reduce: 0.06 | optimizer-copy-to-main-grad: 5.27 | optimizer-unscale-and-check-inf: 4.52 | optimizer-clip-main-grad: 4.08 | optimizer-copy-main-to-model-params: 5.39 | optimizer: 32.40 | batch-generator: 768.43
 iteration    15900/  200000 | consumed samples:      2035200 | elapsed time per iteration (ms): 1385.4 | learning rate: 8.572E-05 | global batch size:   128 | lm loss: 6.067373E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 69.52 | backward-params-all-reduce: 58.67 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 5.89 | optimizer-unscale-and-check-inf: 5.13 | optimizer-clip-main-grad: 5.09 | optimizer-copy-main-to-model-params: 9.50 | optimizer: 39.64 | batch-generator: 985.26
 iteration    16000/  200000 | consumed samples:      2048000 | elapsed time per iteration (ms): 1133.4 | learning rate: 8.563E-05 | global batch size:   128 | lm loss: 5.942552E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 70.17 | backward-params-all-reduce: 48.38 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.26 | optimizer-unscale-and-check-inf: 4.57 | optimizer-clip-main-grad: 4.18 | optimizer-copy-main-to-model-params: 5.41 | optimizer: 32.86 | batch-generator: 767.85
 at iteration 16000, match long value: 0.44678726522053586 | match short value: 0.9459384377545064 
----------------------------------------------------------------------------------------------------
 iteration    16100/  200000 | consumed samples:      2060800 | elapsed time per iteration (ms): 1357.7 | learning rate: 8.553E-05 | global batch size:   128 | lm loss: 6.389638E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 77.64 | backward-params-all-reduce: 56.20 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.99 | optimizer-unscale-and-check-inf: 5.23 | optimizer-clip-main-grad: 4.91 | optimizer-copy-main-to-model-params: 6.00 | optimizer: 35.90 | batch-generator: 967.10
 iteration    16200/  200000 | consumed samples:      2073600 | elapsed time per iteration (ms): 1378.0 | learning rate: 8.544E-05 | global batch size:   128 | lm loss: 6.187921E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 75.77 | backward-params-all-reduce: 53.06 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.16 | optimizer-unscale-and-check-inf: 5.20 | optimizer-clip-main-grad: 5.19 | optimizer-copy-main-to-model-params: 8.37 | optimizer: 38.86 | batch-generator: 973.80
 iteration    16300/  200000 | consumed samples:      2086400 | elapsed time per iteration (ms): 1393.8 | learning rate: 8.535E-05 | global batch size:   128 | lm loss: 6.151567E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 71.36 | backward-params-all-reduce: 60.75 | backward-embedding-all-reduce: 0.10 | optimizer-copy-to-main-grad: 6.18 | optimizer-unscale-and-check-inf: 5.35 | optimizer-clip-main-grad: 5.27 | optimizer-copy-main-to-model-params: 6.54 | optimizer: 37.36 | batch-generator: 985.13
 iteration    16400/  200000 | consumed samples:      2099200 | elapsed time per iteration (ms): 1389.2 | learning rate: 8.526E-05 | global batch size:   128 | lm loss: 5.924812E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 74.79 | backward-params-all-reduce: 54.20 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 6.24 | optimizer-unscale-and-check-inf: 5.16 | optimizer-clip-main-grad: 5.18 | optimizer-copy-main-to-model-params: 6.04 | optimizer: 36.58 | batch-generator: 997.19
 iteration    16500/  200000 | consumed samples:      2112000 | elapsed time per iteration (ms): 1430.2 | learning rate: 8.517E-05 | global batch size:   128 | lm loss: 5.805290E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 75.77 | backward-params-all-reduce: 58.49 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.29 | optimizer-unscale-and-check-inf: 5.39 | optimizer-clip-main-grad: 5.44 | optimizer-copy-main-to-model-params: 7.81 | optimizer: 38.94 | batch-generator: 1018.95
 at iteration 16500, match long value: 0.46233580176713 | match short value: 0.9503514819391933 
-------------------------------------------------------------------------------------------------
 iteration    16600/  200000 | consumed samples:      2124800 | elapsed time per iteration (ms): 1411.4 | learning rate: 8.508E-05 | global batch size:   128 | lm loss: 6.422341E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 74.32 | backward-params-all-reduce: 59.10 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.11 | optimizer-unscale-and-check-inf: 5.11 | optimizer-clip-main-grad: 5.41 | optimizer-copy-main-to-model-params: 6.28 | optimizer: 36.90 | batch-generator: 983.81
 iteration    16700/  200000 | consumed samples:      2137600 | elapsed time per iteration (ms): 1421.9 | learning rate: 8.499E-05 | global batch size:   128 | lm loss: 5.834840E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 74.09 | backward-params-all-reduce: 63.18 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.08 | optimizer-unscale-and-check-inf: 5.13 | optimizer-clip-main-grad: 5.16 | optimizer-copy-main-to-model-params: 6.18 | optimizer: 36.62 | batch-generator: 996.33
 iteration    16800/  200000 | consumed samples:      2150400 | elapsed time per iteration (ms): 1400.8 | learning rate: 8.490E-05 | global batch size:   128 | lm loss: 5.957806E-07 | loss scale: 4294967296.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 80.56 | backward-params-all-reduce: 54.72 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.50 | optimizer-unscale-and-check-inf: 5.23 | optimizer-clip-main-grad: 5.11 | optimizer-copy-main-to-model-params: 6.08 | optimizer: 37.20 | batch-generator: 996.34
 iteration    16900/  200000 | consumed samples:      2163200 | elapsed time per iteration (ms): 1389.6 | learning rate: 8.481E-05 | global batch size:   128 | lm loss: 5.553618E-07 | loss scale: 4294967296.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 75.70 | backward-params-all-reduce: 58.53 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.02 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.92 | optimizer-copy-main-to-model-params: 7.35 | optimizer: 37.62 | batch-generator: 958.09
 iteration    17000/  200000 | consumed samples:      2176000 | elapsed time per iteration (ms): 1437.2 | learning rate: 8.471E-05 | global batch size:   128 | lm loss: 6.067409E-07 | loss scale: 4294967296.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 82.96 | backward-params-all-reduce: 57.04 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.57 | optimizer-unscale-and-check-inf: 5.33 | optimizer-clip-main-grad: 5.45 | optimizer-copy-main-to-model-params: 6.49 | optimizer: 37.78 | batch-generator: 1027.66
 at iteration 17000, match long value: 0.4906434440876168 | match short value: 0.9563380208961294 
---------------------------------------------------------------------------------------------------
 iteration    17100/  200000 | consumed samples:      2188800 | elapsed time per iteration (ms): 1422.3 | learning rate: 8.462E-05 | global batch size:   128 | lm loss: 5.758319E-07 | loss scale: 4294967296.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 81.89 | backward-params-all-reduce: 53.39 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.71 | optimizer-unscale-and-check-inf: 5.41 | optimizer-clip-main-grad: 5.45 | optimizer-copy-main-to-model-params: 6.33 | optimizer: 38.03 | batch-generator: 1014.52
 iteration    17200/  200000 | consumed samples:      2201600 | elapsed time per iteration (ms): 1388.2 | learning rate: 8.453E-05 | global batch size:   128 | lm loss: 5.819520E-07 | loss scale: 4294967296.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 69.72 | backward-params-all-reduce: 66.40 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.09 | optimizer-unscale-and-check-inf: 4.80 | optimizer-clip-main-grad: 4.38 | optimizer-copy-main-to-model-params: 8.18 | optimizer: 36.12 | batch-generator: 915.52
 iteration    17300/  200000 | consumed samples:      2214400 | elapsed time per iteration (ms): 1200.6 | learning rate: 8.444E-05 | global batch size:   128 | lm loss: 5.421478E-07 | loss scale: 4294967296.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 67.00 | backward-params-all-reduce: 58.02 | backward-embedding-all-reduce: 0.05 | optimizer-copy-to-main-grad: 4.54 | optimizer-unscale-and-check-inf: 4.54 | optimizer-clip-main-grad: 3.84 | optimizer-copy-main-to-model-params: 5.04 | optimizer: 31.19 | batch-generator: 789.18
 iteration    17400/  200000 | consumed samples:      2227200 | elapsed time per iteration (ms): 1437.8 | learning rate: 8.435E-05 | global batch size:   128 | lm loss: 5.881242E-07 | loss scale: 2147483648.0 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 75.41 | backward-params-all-reduce: 65.64 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.28 | optimizer-unscale-and-check-inf: 4.83 | optimizer-clip-main-grad: 4.38 | optimizer-copy-main-to-model-params: 5.42 | optimizer: 33.23 | batch-generator: 961.93
 iteration    17500/  200000 | consumed samples:      2240000 | elapsed time per iteration (ms): 1435.4 | learning rate: 8.426E-05 | global batch size:   128 | lm loss: 5.773535E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 69.35 | backward-params-all-reduce: 66.50 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.22 | optimizer-unscale-and-check-inf: 4.76 | optimizer-clip-main-grad: 4.36 | optimizer-copy-main-to-model-params: 8.78 | optimizer: 36.67 | batch-generator: 923.05
 at iteration 17500, match long value: 0.47807864454240906 | match short value: 0.9503979473536902 
----------------------------------------------------------------------------------------------------
 iteration    17600/  200000 | consumed samples:      2252800 | elapsed time per iteration (ms): 1396.9 | learning rate: 8.417E-05 | global batch size:   128 | lm loss: 5.411399E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 66.43 | backward-params-all-reduce: 63.41 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 4.90 | optimizer-unscale-and-check-inf: 4.75 | optimizer-clip-main-grad: 4.30 | optimizer-copy-main-to-model-params: 5.23 | optimizer: 32.79 | batch-generator: 933.66
 iteration    17700/  200000 | consumed samples:      2265600 | elapsed time per iteration (ms): 1359.1 | learning rate: 8.408E-05 | global batch size:   128 | lm loss: 5.577007E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 70.17 | backward-params-all-reduce: 64.37 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 4.98 | optimizer-unscale-and-check-inf: 4.71 | optimizer-clip-main-grad: 4.48 | optimizer-copy-main-to-model-params: 5.55 | optimizer: 33.26 | batch-generator: 881.02
 iteration    17800/  200000 | consumed samples:      2278400 | elapsed time per iteration (ms): 1311.1 | learning rate: 8.399E-05 | global batch size:   128 | lm loss: 5.367867E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 71.21 | backward-params-all-reduce: 65.03 | backward-embedding-all-reduce: 0.06 | optimizer-copy-to-main-grad: 4.86 | optimizer-unscale-and-check-inf: 4.61 | optimizer-clip-main-grad: 4.16 | optimizer-copy-main-to-model-params: 6.86 | optimizer: 33.86 | batch-generator: 827.38
 iteration    17900/  200000 | consumed samples:      2291200 | elapsed time per iteration (ms): 1298.7 | learning rate: 8.390E-05 | global batch size:   128 | lm loss: 5.839910E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 68.24 | backward-params-all-reduce: 64.42 | backward-embedding-all-reduce: 0.06 | optimizer-copy-to-main-grad: 4.92 | optimizer-unscale-and-check-inf: 4.70 | optimizer-clip-main-grad: 4.13 | optimizer-copy-main-to-model-params: 5.26 | optimizer: 32.42 | batch-generator: 841.45
 iteration    18000/  200000 | consumed samples:      2304000 | elapsed time per iteration (ms): 1159.8 | learning rate: 8.381E-05 | global batch size:   128 | lm loss: 5.672478E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 68.69 | backward-params-all-reduce: 49.03 | backward-embedding-all-reduce: 0.06 | optimizer-copy-to-main-grad: 4.55 | optimizer-unscale-and-check-inf: 4.38 | optimizer-clip-main-grad: 3.78 | optimizer-copy-main-to-model-params: 5.06 | optimizer: 31.03 | batch-generator: 738.31
 at iteration 18000, match long value: 0.4727392607120244 | match short value: 0.9446616216039943 
---------------------------------------------------------------------------------------------------
 iteration    18100/  200000 | consumed samples:      2316800 | elapsed time per iteration (ms): 1125.4 | learning rate: 8.371E-05 | global batch size:   128 | lm loss: 5.222506E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 68.18 | backward-params-all-reduce: 46.09 | backward-embedding-all-reduce: 0.05 | optimizer-copy-to-main-grad: 4.60 | optimizer-unscale-and-check-inf: 4.22 | optimizer-clip-main-grad: 3.58 | optimizer-copy-main-to-model-params: 4.79 | optimizer: 30.36 | batch-generator: 697.77
 iteration    18200/  200000 | consumed samples:      2329600 | elapsed time per iteration (ms): 1005.8 | learning rate: 8.362E-05 | global batch size:   128 | lm loss: 5.449476E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 66.30 | backward-params-all-reduce: 51.21 | backward-embedding-all-reduce: 0.38 | optimizer-copy-to-main-grad: 6.11 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 5.29 | optimizer-copy-main-to-model-params: 9.14 | optimizer: 39.17 | batch-generator: 623.57
 iteration    18300/  200000 | consumed samples:      2342400 | elapsed time per iteration (ms): 1089.1 | learning rate: 8.353E-05 | global batch size:   128 | lm loss: 5.423204E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 73.53 | backward-params-all-reduce: 54.98 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 7.59 | optimizer-unscale-and-check-inf: 5.80 | optimizer-clip-main-grad: 4.45 | optimizer-copy-main-to-model-params: 6.14 | optimizer: 37.89 | batch-generator: 680.64
 iteration    18400/  200000 | consumed samples:      2355200 | elapsed time per iteration (ms): 1452.4 | learning rate: 8.344E-05 | global batch size:   128 | lm loss: 5.624999E-07 | loss scale: 1073741824.0 | grad norm: 0.000 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 85.95 | backward-params-all-reduce: 53.30 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.83 | optimizer-unscale-and-check-inf: 5.22 | optimizer-clip-main-grad: 4.85 | optimizer-copy-main-to-model-params: 5.91 | optimizer: 35.45 | batch-generator: 1004.24
 iteration    18500/  200000 | consumed samples:      2368000 | elapsed time per iteration (ms): 1469.2 | learning rate: 8.335E-05 | global batch size:   128 | lm loss: 5.115538E-07 | loss scale: 1073741824.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 81.01 | backward-params-all-reduce: 51.10 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.28 | optimizer-unscale-and-check-inf: 5.19 | optimizer-clip-main-grad: 5.31 | optimizer-copy-main-to-model-params: 6.33 | optimizer: 37.21 | batch-generator: 1027.83
 at iteration 18500, match long value: 0.48468595270997905 | match short value: 0.9473545133676724 
----------------------------------------------------------------------------------------------------
 iteration    18600/  200000 | consumed samples:      2380800 | elapsed time per iteration (ms): 1520.2 | learning rate: 8.326E-05 | global batch size:   128 | lm loss: 5.075437E-07 | loss scale: 1073741824.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 79.78 | backward-params-all-reduce: 57.85 | backward-embedding-all-reduce: 0.10 | optimizer-copy-to-main-grad: 6.42 | optimizer-unscale-and-check-inf: 5.54 | optimizer-clip-main-grad: 5.55 | optimizer-copy-main-to-model-params: 6.73 | optimizer: 38.29 | batch-generator: 1071.51
 iteration    18700/  200000 | consumed samples:      2393600 | elapsed time per iteration (ms): 1433.6 | learning rate: 8.317E-05 | global batch size:   128 | lm loss: 5.585025E-07 | loss scale: 1073741824.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 72.85 | backward-params-all-reduce: 53.78 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 5.88 | optimizer-unscale-and-check-inf: 5.11 | optimizer-clip-main-grad: 4.97 | optimizer-copy-main-to-model-params: 5.91 | optimizer: 35.65 | batch-generator: 1006.96
 iteration    18800/  200000 | consumed samples:      2406400 | elapsed time per iteration (ms): 1516.4 | learning rate: 8.308E-05 | global batch size:   128 | lm loss: 4.904253E-07 | loss scale: 1073741824.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 69.61 | backward-params-all-reduce: 63.04 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.47 | optimizer-unscale-and-check-inf: 5.37 | optimizer-clip-main-grad: 5.53 | optimizer-copy-main-to-model-params: 6.56 | optimizer: 38.01 | batch-generator: 1079.35
 iteration    18900/  200000 | consumed samples:      2419200 | elapsed time per iteration (ms): 1486.5 | learning rate: 8.299E-05 | global batch size:   128 | lm loss: 5.274460E-07 | loss scale: 1073741824.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 76.35 | backward-params-all-reduce: 55.11 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.29 | optimizer-unscale-and-check-inf: 5.35 | optimizer-clip-main-grad: 5.52 | optimizer-copy-main-to-model-params: 6.47 | optimizer: 37.66 | batch-generator: 1046.71
 iteration    19000/  200000 | consumed samples:      2432000 | elapsed time per iteration (ms): 1487.5 | learning rate: 8.290E-05 | global batch size:   128 | lm loss: 5.259613E-07 | loss scale: 1073741824.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 75.99 | backward-params-all-reduce: 54.77 | backward-embedding-all-reduce: 0.10 | optimizer-copy-to-main-grad: 6.55 | optimizer-unscale-and-check-inf: 5.54 | optimizer-clip-main-grad: 5.59 | optimizer-copy-main-to-model-params: 6.93 | optimizer: 38.79 | batch-generator: 1045.43
 at iteration 19000, match long value: 0.5136978287747025 | match short value: 0.9540680373711077 
---------------------------------------------------------------------------------------------------
 iteration    19100/  200000 | consumed samples:      2444800 | elapsed time per iteration (ms): 1507.5 | learning rate: 8.280E-05 | global batch size:   128 | lm loss: 5.042435E-07 | loss scale: 1073741824.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 74.11 | backward-params-all-reduce: 58.81 | backward-embedding-all-reduce: 0.10 | optimizer-copy-to-main-grad: 6.26 | optimizer-unscale-and-check-inf: 5.30 | optimizer-clip-main-grad: 5.23 | optimizer-copy-main-to-model-params: 6.27 | optimizer: 37.00 | batch-generator: 1038.92
 iteration    19200/  200000 | consumed samples:      2457600 | elapsed time per iteration (ms): 1481.5 | learning rate: 8.271E-05 | global batch size:   128 | lm loss: 5.247998E-07 | loss scale: 1073741824.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 75.31 | backward-params-all-reduce: 62.34 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.88 | optimizer-unscale-and-check-inf: 5.32 | optimizer-clip-main-grad: 5.73 | optimizer-copy-main-to-model-params: 6.55 | optimizer: 38.74 | batch-generator: 1018.65
 iteration    19300/  200000 | consumed samples:      2470400 | elapsed time per iteration (ms): 1411.0 | learning rate: 8.262E-05 | global batch size:   128 | lm loss: 4.994304E-07 | loss scale: 1073741824.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 71.41 | backward-params-all-reduce: 61.11 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.97 | optimizer-unscale-and-check-inf: 5.07 | optimizer-clip-main-grad: 5.31 | optimizer-copy-main-to-model-params: 6.15 | optimizer: 36.31 | batch-generator: 981.94
 iteration    19400/  200000 | consumed samples:      2483200 | elapsed time per iteration (ms): 1391.9 | learning rate: 8.253E-05 | global batch size:   128 | lm loss: 4.878901E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 75.38 | backward-params-all-reduce: 59.28 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 6.03 | optimizer-unscale-and-check-inf: 5.23 | optimizer-clip-main-grad: 4.91 | optimizer-copy-main-to-model-params: 6.31 | optimizer: 36.46 | batch-generator: 956.72
 iteration    19500/  200000 | consumed samples:      2496000 | elapsed time per iteration (ms): 1494.2 | learning rate: 8.244E-05 | global batch size:   128 | lm loss: 5.519416E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 74.39 | backward-params-all-reduce: 54.53 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.23 | optimizer-unscale-and-check-inf: 5.52 | optimizer-clip-main-grad: 5.64 | optimizer-copy-main-to-model-params: 6.31 | optimizer: 37.68 | batch-generator: 1065.44
 at iteration 19500, match long value: 0.5405854305472984 | match short value: 0.9590946686633438 
---------------------------------------------------------------------------------------------------
 iteration    19600/  200000 | consumed samples:      2508800 | elapsed time per iteration (ms): 1492.4 | learning rate: 8.235E-05 | global batch size:   128 | lm loss: 4.747795E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 79.86 | backward-params-all-reduce: 51.70 | backward-embedding-all-reduce: 0.10 | optimizer-copy-to-main-grad: 6.44 | optimizer-unscale-and-check-inf: 5.18 | optimizer-clip-main-grad: 5.24 | optimizer-copy-main-to-model-params: 6.24 | optimizer: 37.04 | batch-generator: 1021.96
 iteration    19700/  200000 | consumed samples:      2521600 | elapsed time per iteration (ms): 1482.3 | learning rate: 8.226E-05 | global batch size:   128 | lm loss: 5.236524E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 73.72 | backward-params-all-reduce: 56.40 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.41 | optimizer-unscale-and-check-inf: 5.29 | optimizer-clip-main-grad: 5.35 | optimizer-copy-main-to-model-params: 6.38 | optimizer: 37.39 | batch-generator: 1023.35
 iteration    19800/  200000 | consumed samples:      2534400 | elapsed time per iteration (ms): 1335.4 | learning rate: 8.217E-05 | global batch size:   128 | lm loss: 4.743958E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 72.85 | backward-params-all-reduce: 56.35 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.42 | optimizer-unscale-and-check-inf: 4.91 | optimizer-clip-main-grad: 4.65 | optimizer-copy-main-to-model-params: 5.70 | optimizer: 34.47 | batch-generator: 891.64
 iteration    19900/  200000 | consumed samples:      2547200 | elapsed time per iteration (ms): 1467.5 | learning rate: 8.208E-05 | global batch size:   128 | lm loss: 5.318351E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 71.99 | backward-params-all-reduce: 55.72 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.19 | optimizer-unscale-and-check-inf: 5.25 | optimizer-clip-main-grad: 5.32 | optimizer-copy-main-to-model-params: 6.35 | optimizer: 37.14 | batch-generator: 1025.43
 iteration    20000/  200000 | consumed samples:      2560000 | elapsed time per iteration (ms): 1402.9 | learning rate: 8.198E-05 | global batch size:   128 | lm loss: 4.387756E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 72.76 | backward-params-all-reduce: 59.02 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.90 | optimizer-unscale-and-check-inf: 4.94 | optimizer-clip-main-grad: 4.73 | optimizer-copy-main-to-model-params: 5.73 | optimizer: 35.03 | batch-generator: 934.11
 at iteration 20000, match long value: 0.5353873798004616 | match short value: 0.9554675810931965 
---------------------------------------------------------------------------------------------------
 iteration    20100/  200000 | consumed samples:      2572800 | elapsed time per iteration (ms): 1443.2 | learning rate: 8.189E-05 | global batch size:   128 | lm loss: 5.028093E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 75.84 | backward-params-all-reduce: 60.12 | backward-embedding-all-reduce: 0.16 | optimizer-copy-to-main-grad: 5.97 | optimizer-unscale-and-check-inf: 5.01 | optimizer-clip-main-grad: 5.13 | optimizer-copy-main-to-model-params: 6.02 | optimizer: 37.33 | batch-generator: 962.31
 iteration    20200/  200000 | consumed samples:      2585600 | elapsed time per iteration (ms): 1407.9 | learning rate: 8.180E-05 | global batch size:   128 | lm loss: 4.656363E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 69.87 | backward-params-all-reduce: 61.11 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.12 | optimizer-unscale-and-check-inf: 5.05 | optimizer-clip-main-grad: 5.22 | optimizer-copy-main-to-model-params: 6.47 | optimizer: 37.12 | batch-generator: 922.56
 iteration    20300/  200000 | consumed samples:      2598400 | elapsed time per iteration (ms): 1505.5 | learning rate: 8.171E-05 | global batch size:   128 | lm loss: 5.177773E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 78.14 | backward-params-all-reduce: 56.91 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 5.95 | optimizer-unscale-and-check-inf: 5.04 | optimizer-clip-main-grad: 4.99 | optimizer-copy-main-to-model-params: 6.29 | optimizer: 36.11 | batch-generator: 998.71
 iteration    20400/  200000 | consumed samples:      2611200 | elapsed time per iteration (ms): 1448.4 | learning rate: 8.162E-05 | global batch size:   128 | lm loss: 4.629243E-07 | loss scale: 4294967296.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 72.23 | backward-params-all-reduce: 57.60 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.98 | optimizer-unscale-and-check-inf: 5.03 | optimizer-clip-main-grad: 5.00 | optimizer-copy-main-to-model-params: 13.48 | optimizer: 43.52 | batch-generator: 960.94
 iteration    20500/  200000 | consumed samples:      2624000 | elapsed time per iteration (ms): 1322.9 | learning rate: 8.153E-05 | global batch size:   128 | lm loss: 4.907270E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 75.38 | backward-params-all-reduce: 46.76 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.70 | optimizer-unscale-and-check-inf: 4.74 | optimizer-clip-main-grad: 4.40 | optimizer-copy-main-to-model-params: 5.52 | optimizer: 33.89 | batch-generator: 869.91
 at iteration 20500, match long value: 0.5423953883762801 | match short value: 0.9553641382453212 
---------------------------------------------------------------------------------------------------
 iteration    20600/  200000 | consumed samples:      2636800 | elapsed time per iteration (ms): 1557.4 | learning rate: 8.144E-05 | global batch size:   128 | lm loss: 4.514758E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 78.32 | backward-params-all-reduce: 58.15 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.35 | optimizer-unscale-and-check-inf: 5.18 | optimizer-clip-main-grad: 5.21 | optimizer-copy-main-to-model-params: 6.73 | optimizer: 37.40 | batch-generator: 1051.79
 iteration    20700/  200000 | consumed samples:      2649600 | elapsed time per iteration (ms): 1532.1 | learning rate: 8.135E-05 | global batch size:   128 | lm loss: 4.857096E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 80.99 | backward-params-all-reduce: 51.63 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.18 | optimizer-unscale-and-check-inf: 5.29 | optimizer-clip-main-grad: 5.37 | optimizer-copy-main-to-model-params: 6.27 | optimizer: 37.02 | batch-generator: 1034.38
 iteration    20800/  200000 | consumed samples:      2662400 | elapsed time per iteration (ms): 1551.3 | learning rate: 8.126E-05 | global batch size:   128 | lm loss: 4.618047E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 79.02 | backward-params-all-reduce: 61.03 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.26 | optimizer-unscale-and-check-inf: 5.31 | optimizer-clip-main-grad: 5.52 | optimizer-copy-main-to-model-params: 6.47 | optimizer: 37.68 | batch-generator: 1049.93
 iteration    20900/  200000 | consumed samples:      2675200 | elapsed time per iteration (ms): 1568.6 | learning rate: 8.117E-05 | global batch size:   128 | lm loss: 4.720044E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 76.71 | backward-params-all-reduce: 67.13 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.69 | optimizer-unscale-and-check-inf: 5.36 | optimizer-clip-main-grad: 5.31 | optimizer-copy-main-to-model-params: 6.42 | optimizer: 37.93 | batch-generator: 1073.04
 iteration    21000/  200000 | consumed samples:      2688000 | elapsed time per iteration (ms): 1557.1 | learning rate: 8.108E-05 | global batch size:   128 | lm loss: 4.664810E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 77.07 | backward-params-all-reduce: 58.53 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.70 | optimizer-unscale-and-check-inf: 5.32 | optimizer-clip-main-grad: 5.62 | optimizer-copy-main-to-model-params: 6.85 | optimizer: 38.59 | batch-generator: 1040.66
 at iteration 21000, match long value: 0.5518370645791799 | match short value: 0.9572376748222796 
---------------------------------------------------------------------------------------------------
 iteration    21100/  200000 | consumed samples:      2700800 | elapsed time per iteration (ms): 1507.9 | learning rate: 8.098E-05 | global batch size:   128 | lm loss: 4.625854E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 78.84 | backward-params-all-reduce: 49.86 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 6.22 | optimizer-unscale-and-check-inf: 5.11 | optimizer-clip-main-grad: 5.31 | optimizer-copy-main-to-model-params: 6.35 | optimizer: 36.91 | batch-generator: 1022.80
 iteration    21200/  200000 | consumed samples:      2713600 | elapsed time per iteration (ms): 1541.9 | learning rate: 8.089E-05 | global batch size:   128 | lm loss: 4.496136E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 84.42 | backward-params-all-reduce: 51.99 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 6.00 | optimizer-unscale-and-check-inf: 5.06 | optimizer-clip-main-grad: 5.05 | optimizer-copy-main-to-model-params: 6.00 | optimizer: 35.95 | batch-generator: 1007.13
 iteration    21300/  200000 | consumed samples:      2726400 | elapsed time per iteration (ms): 1522.7 | learning rate: 8.080E-05 | global batch size:   128 | lm loss: 4.878410E-07 | loss scale: 1073741824.0 | grad norm: 0.000 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 81.51 | backward-params-all-reduce: 56.70 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.20 | optimizer-unscale-and-check-inf: 5.22 | optimizer-clip-main-grad: 5.19 | optimizer-copy-main-to-model-params: 6.35 | optimizer: 36.76 | batch-generator: 1004.88
 iteration    21400/  200000 | consumed samples:      2739200 | elapsed time per iteration (ms): 1518.0 | learning rate: 8.071E-05 | global batch size:   128 | lm loss: 4.410289E-07 | loss scale: 1073741824.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 80.32 | backward-params-all-reduce: 53.18 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.14 | optimizer-unscale-and-check-inf: 5.05 | optimizer-clip-main-grad: 5.27 | optimizer-copy-main-to-model-params: 13.28 | optimizer: 43.61 | batch-generator: 1009.02
 iteration    21500/  200000 | consumed samples:      2752000 | elapsed time per iteration (ms): 1470.9 | learning rate: 8.062E-05 | global batch size:   128 | lm loss: 4.768228E-07 | loss scale: 1073741824.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 74.02 | backward-params-all-reduce: 57.61 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.14 | optimizer-unscale-and-check-inf: 5.10 | optimizer-clip-main-grad: 4.99 | optimizer-copy-main-to-model-params: 6.11 | optimizer: 36.30 | batch-generator: 965.66
 at iteration 21500, match long value: 0.5708843033995856 | match short value: 0.960425021852968 
--------------------------------------------------------------------------------------------------
 iteration    21600/  200000 | consumed samples:      2764800 | elapsed time per iteration (ms): 1547.0 | learning rate: 8.053E-05 | global batch size:   128 | lm loss: 4.219131E-07 | loss scale: 1073741824.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 82.60 | backward-params-all-reduce: 52.37 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 7.41 | optimizer-unscale-and-check-inf: 5.48 | optimizer-clip-main-grad: 5.07 | optimizer-copy-main-to-model-params: 6.37 | optimizer: 38.23 | batch-generator: 1017.54
 iteration    21700/  200000 | consumed samples:      2777600 | elapsed time per iteration (ms): 1416.4 | learning rate: 8.044E-05 | global batch size:   128 | lm loss: 4.554591E-07 | loss scale: 1073741824.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 75.68 | backward-params-all-reduce: 57.30 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.60 | optimizer-unscale-and-check-inf: 4.85 | optimizer-clip-main-grad: 4.56 | optimizer-copy-main-to-model-params: 7.66 | optimizer: 36.31 | batch-generator: 918.96
 iteration    21800/  200000 | consumed samples:      2790400 | elapsed time per iteration (ms): 1566.3 | learning rate: 8.035E-05 | global batch size:   128 | lm loss: 4.167022E-07 | loss scale: 1073741824.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 81.58 | backward-params-all-reduce: 56.31 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 5.92 | optimizer-unscale-and-check-inf: 5.15 | optimizer-clip-main-grad: 4.91 | optimizer-copy-main-to-model-params: 5.96 | optimizer: 35.79 | batch-generator: 1001.72
 iteration    21900/  200000 | consumed samples:      2803200 | elapsed time per iteration (ms): 1479.1 | learning rate: 8.026E-05 | global batch size:   128 | lm loss: 4.742923E-07 | loss scale: 1073741824.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 72.81 | backward-params-all-reduce: 61.72 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.68 | optimizer-unscale-and-check-inf: 4.89 | optimizer-clip-main-grad: 4.90 | optimizer-copy-main-to-model-params: 6.20 | optimizer: 35.38 | batch-generator: 941.87
 iteration    22000/  200000 | consumed samples:      2816000 | elapsed time per iteration (ms): 1540.4 | learning rate: 8.017E-05 | global batch size:   128 | lm loss: 4.191240E-07 | loss scale: 1073741824.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 79.45 | backward-params-all-reduce: 56.90 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.29 | optimizer-unscale-and-check-inf: 5.00 | optimizer-clip-main-grad: 5.17 | optimizer-copy-main-to-model-params: 10.66 | optimizer: 40.92 | batch-generator: 972.52
 at iteration 22000, match long value: 0.5655668023029514 | match short value: 0.9566705061945079 
---------------------------------------------------------------------------------------------------
 iteration    22100/  200000 | consumed samples:      2828800 | elapsed time per iteration (ms): 1488.0 | learning rate: 8.007E-05 | global batch size:   128 | lm loss: 4.334747E-07 | loss scale: 1073741824.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 79.45 | backward-params-all-reduce: 45.75 | backward-embedding-all-reduce: 0.11 | optimizer-copy-to-main-grad: 5.74 | optimizer-unscale-and-check-inf: 5.04 | optimizer-clip-main-grad: 5.00 | optimizer-copy-main-to-model-params: 6.06 | optimizer: 35.74 | batch-generator: 972.00
 iteration    22200/  200000 | consumed samples:      2841600 | elapsed time per iteration (ms): 1361.1 | learning rate: 7.998E-05 | global batch size:   128 | lm loss: 4.236331E-07 | loss scale: 1073741824.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 78.62 | backward-params-all-reduce: 63.77 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.35 | optimizer-unscale-and-check-inf: 4.62 | optimizer-clip-main-grad: 4.25 | optimizer-copy-main-to-model-params: 5.50 | optimizer: 33.28 | batch-generator: 837.48
 iteration    22300/  200000 | consumed samples:      2854400 | elapsed time per iteration (ms): 1525.3 | learning rate: 7.989E-05 | global batch size:   128 | lm loss: 4.515731E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 84.16 | backward-params-all-reduce: 49.63 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.94 | optimizer-unscale-and-check-inf: 5.02 | optimizer-clip-main-grad: 4.87 | optimizer-copy-main-to-model-params: 5.99 | optimizer: 35.66 | batch-generator: 985.79
 iteration    22400/  200000 | consumed samples:      2867200 | elapsed time per iteration (ms): 1435.4 | learning rate: 7.980E-05 | global batch size:   128 | lm loss: 4.537896E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 76.30 | backward-params-all-reduce: 52.14 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.92 | optimizer-unscale-and-check-inf: 5.01 | optimizer-clip-main-grad: 4.85 | optimizer-copy-main-to-model-params: 7.53 | optimizer: 37.00 | batch-generator: 909.18
 iteration    22500/  200000 | consumed samples:      2880000 | elapsed time per iteration (ms): 1616.1 | learning rate: 7.971E-05 | global batch size:   128 | lm loss: 4.074457E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 81.24 | backward-params-all-reduce: 61.31 | backward-embedding-all-reduce: 0.11 | optimizer-copy-to-main-grad: 6.65 | optimizer-unscale-and-check-inf: 5.39 | optimizer-clip-main-grad: 5.22 | optimizer-copy-main-to-model-params: 6.68 | optimizer: 37.95 | batch-generator: 1059.75
 at iteration 22500, match long value: 0.5912549449124943 | match short value: 0.961240049253826 
--------------------------------------------------------------------------------------------------
 iteration    22600/  200000 | consumed samples:      2892800 | elapsed time per iteration (ms): 1371.8 | learning rate: 7.962E-05 | global batch size:   128 | lm loss: 4.223210E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 76.56 | backward-params-all-reduce: 40.15 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.37 | optimizer-unscale-and-check-inf: 4.65 | optimizer-clip-main-grad: 4.30 | optimizer-copy-main-to-model-params: 5.62 | optimizer: 33.54 | batch-generator: 860.68
 iteration    22700/  200000 | consumed samples:      2905600 | elapsed time per iteration (ms): 1521.7 | learning rate: 7.953E-05 | global batch size:   128 | lm loss: 4.211522E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 78.64 | backward-params-all-reduce: 51.87 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.78 | optimizer-unscale-and-check-inf: 5.02 | optimizer-clip-main-grad: 4.91 | optimizer-copy-main-to-model-params: 7.92 | optimizer: 37.41 | batch-generator: 968.13
 iteration    22800/  200000 | consumed samples:      2918400 | elapsed time per iteration (ms): 1655.3 | learning rate: 7.944E-05 | global batch size:   128 | lm loss: 4.282424E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 81.74 | backward-params-all-reduce: 57.82 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.66 | optimizer-unscale-and-check-inf: 5.28 | optimizer-clip-main-grad: 5.43 | optimizer-copy-main-to-model-params: 6.47 | optimizer: 38.00 | batch-generator: 1078.40
 iteration    22900/  200000 | consumed samples:      2931200 | elapsed time per iteration (ms): 1640.9 | learning rate: 7.935E-05 | global batch size:   128 | lm loss: 4.187009E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 72.66 | backward-params-all-reduce: 65.16 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.22 | optimizer-unscale-and-check-inf: 5.31 | optimizer-clip-main-grad: 5.44 | optimizer-copy-main-to-model-params: 6.32 | optimizer: 37.34 | batch-generator: 1052.35
 iteration    23000/  200000 | consumed samples:      2944000 | elapsed time per iteration (ms): 1600.7 | learning rate: 7.925E-05 | global batch size:   128 | lm loss: 4.410003E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 81.52 | backward-params-all-reduce: 54.97 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.94 | optimizer-unscale-and-check-inf: 5.10 | optimizer-clip-main-grad: 5.00 | optimizer-copy-main-to-model-params: 8.15 | optimizer: 38.00 | batch-generator: 1049.83
 at iteration 23000, match long value: 0.5685874699838716 | match short value: 0.9531058031165889 
---------------------------------------------------------------------------------------------------
 iteration    23100/  200000 | consumed samples:      2956800 | elapsed time per iteration (ms): 1656.9 | learning rate: 7.916E-05 | global batch size:   128 | lm loss: 4.255596E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 76.56 | backward-params-all-reduce: 57.12 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.62 | optimizer-unscale-and-check-inf: 5.23 | optimizer-clip-main-grad: 5.61 | optimizer-copy-main-to-model-params: 6.60 | optimizer: 38.07 | batch-generator: 1081.15
 iteration    23200/  200000 | consumed samples:      2969600 | elapsed time per iteration (ms): 1591.0 | learning rate: 7.907E-05 | global batch size:   128 | lm loss: 3.835998E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 75.89 | backward-params-all-reduce: 56.87 | backward-embedding-all-reduce: 0.10 | optimizer-copy-to-main-grad: 6.50 | optimizer-unscale-and-check-inf: 5.27 | optimizer-clip-main-grad: 5.56 | optimizer-copy-main-to-model-params: 6.98 | optimizer: 38.57 | batch-generator: 1032.66
 iteration    23300/  200000 | consumed samples:      2982400 | elapsed time per iteration (ms): 1659.0 | learning rate: 7.898E-05 | global batch size:   128 | lm loss: 4.179437E-07 | loss scale: 4294967296.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 82.75 | backward-params-all-reduce: 60.20 | backward-embedding-all-reduce: 0.10 | optimizer-copy-to-main-grad: 6.47 | optimizer-unscale-and-check-inf: 5.45 | optimizer-clip-main-grad: 5.36 | optimizer-copy-main-to-model-params: 8.26 | optimizer: 39.53 | batch-generator: 1099.42
 iteration    23400/  200000 | consumed samples:      2995200 | elapsed time per iteration (ms): 1615.4 | learning rate: 7.889E-05 | global batch size:   128 | lm loss: 4.032158E-07 | loss scale: 4294967296.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 76.18 | backward-params-all-reduce: 60.82 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.53 | optimizer-unscale-and-check-inf: 5.39 | optimizer-clip-main-grad: 5.45 | optimizer-copy-main-to-model-params: 6.43 | optimizer: 37.79 | batch-generator: 1065.36
 iteration    23500/  200000 | consumed samples:      3008000 | elapsed time per iteration (ms): 1578.4 | learning rate: 7.880E-05 | global batch size:   128 | lm loss: 4.063219E-07 | loss scale: 4294967296.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 72.59 | backward-params-all-reduce: 58.00 | backward-embedding-all-reduce: 0.10 | optimizer-copy-to-main-grad: 6.41 | optimizer-unscale-and-check-inf: 5.35 | optimizer-clip-main-grad: 5.37 | optimizer-copy-main-to-model-params: 6.28 | optimizer: 37.38 | batch-generator: 1031.08
 at iteration 23500, match long value: 0.6041392611346704 | match short value: 0.9615595267255923 
---------------------------------------------------------------------------------------------------
 iteration    23600/  200000 | consumed samples:      3020800 | elapsed time per iteration (ms): 1553.1 | learning rate: 7.871E-05 | global batch size:   128 | lm loss: 4.054838E-07 | loss scale: 4294967296.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 71.08 | backward-params-all-reduce: 58.56 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.84 | optimizer-unscale-and-check-inf: 4.98 | optimizer-clip-main-grad: 4.98 | optimizer-copy-main-to-model-params: 5.96 | optimizer: 35.52 | batch-generator: 1017.90
 iteration    23700/  200000 | consumed samples:      3033600 | elapsed time per iteration (ms): 1554.9 | learning rate: 7.862E-05 | global batch size:   128 | lm loss: 3.903314E-07 | loss scale: 4294967296.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 83.26 | backward-params-all-reduce: 51.90 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 6.32 | optimizer-unscale-and-check-inf: 5.06 | optimizer-clip-main-grad: 5.26 | optimizer-copy-main-to-model-params: 6.80 | optimizer: 37.29 | batch-generator: 1021.44
 iteration    23800/  200000 | consumed samples:      3046400 | elapsed time per iteration (ms): 1610.0 | learning rate: 7.853E-05 | global batch size:   128 | lm loss: 4.278343E-07 | loss scale: 4294967296.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 72.66 | backward-params-all-reduce: 55.06 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.98 | optimizer-unscale-and-check-inf: 5.09 | optimizer-clip-main-grad: 5.00 | optimizer-copy-main-to-model-params: 6.04 | optimizer: 35.92 | batch-generator: 1033.73
 iteration    23900/  200000 | consumed samples:      3059200 | elapsed time per iteration (ms): 1635.3 | learning rate: 7.844E-05 | global batch size:   128 | lm loss: 3.885041E-07 | loss scale: 2147483648.0 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 78.99 | backward-params-all-reduce: 54.37 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.25 | optimizer-unscale-and-check-inf: 5.17 | optimizer-clip-main-grad: 5.03 | optimizer-copy-main-to-model-params: 6.13 | optimizer: 36.20 | batch-generator: 1047.65
 iteration    24000/  200000 | consumed samples:      3072000 | elapsed time per iteration (ms): 1579.7 | learning rate: 7.835E-05 | global batch size:   128 | lm loss: 3.959276E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 79.14 | backward-params-all-reduce: 57.83 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 6.38 | optimizer-unscale-and-check-inf: 5.14 | optimizer-clip-main-grad: 5.32 | optimizer-copy-main-to-model-params: 6.20 | optimizer: 36.92 | batch-generator: 1042.20
 at iteration 24000, match long value: 0.5949771324651603 | match short value: 0.9566162585979413 
---------------------------------------------------------------------------------------------------
 iteration    24100/  200000 | consumed samples:      3084800 | elapsed time per iteration (ms): 1361.1 | learning rate: 7.825E-05 | global batch size:   128 | lm loss: 4.020500E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 75.65 | backward-params-all-reduce: 47.33 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 4.76 | optimizer-unscale-and-check-inf: 4.49 | optimizer-clip-main-grad: 3.91 | optimizer-copy-main-to-model-params: 5.32 | optimizer: 31.80 | batch-generator: 824.24
 iteration    24200/  200000 | consumed samples:      3097600 | elapsed time per iteration (ms): 1540.9 | learning rate: 7.816E-05 | global batch size:   128 | lm loss: 3.788709E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 78.20 | backward-params-all-reduce: 61.49 | backward-embedding-all-reduce: 0.11 | optimizer-copy-to-main-grad: 6.84 | optimizer-unscale-and-check-inf: 5.22 | optimizer-clip-main-grad: 5.07 | optimizer-copy-main-to-model-params: 6.57 | optimizer: 37.52 | batch-generator: 939.89
 iteration    24300/  200000 | consumed samples:      3110400 | elapsed time per iteration (ms): 1616.5 | learning rate: 7.807E-05 | global batch size:   128 | lm loss: 3.976703E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 76.97 | backward-params-all-reduce: 63.45 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.03 | optimizer-unscale-and-check-inf: 5.04 | optimizer-clip-main-grad: 5.17 | optimizer-copy-main-to-model-params: 5.91 | optimizer: 35.99 | batch-generator: 1012.17
 iteration    24400/  200000 | consumed samples:      3123200 | elapsed time per iteration (ms): 1592.9 | learning rate: 7.798E-05 | global batch size:   128 | lm loss: 3.836386E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 73.28 | backward-params-all-reduce: 64.83 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.50 | optimizer-unscale-and-check-inf: 5.26 | optimizer-clip-main-grad: 5.30 | optimizer-copy-main-to-model-params: 6.55 | optimizer: 37.66 | batch-generator: 1011.24
 iteration    24500/  200000 | consumed samples:      3136000 | elapsed time per iteration (ms): 1247.9 | learning rate: 7.789E-05 | global batch size:   128 | lm loss: 3.862957E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 72.32 | backward-params-all-reduce: 47.35 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 7.33 | optimizer-unscale-and-check-inf: 4.34 | optimizer-clip-main-grad: 3.87 | optimizer-copy-main-to-model-params: 5.13 | optimizer: 34.66 | batch-generator: 735.52
 at iteration 24500, match long value: 0.5970498208180014 | match short value: 0.9561577734400716 
---------------------------------------------------------------------------------------------------
 iteration    24600/  200000 | consumed samples:      3148800 | elapsed time per iteration (ms): 1451.1 | learning rate: 7.780E-05 | global batch size:   128 | lm loss: 3.942346E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 73.66 | backward-params-all-reduce: 55.77 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.23 | optimizer-unscale-and-check-inf: 4.62 | optimizer-clip-main-grad: 4.41 | optimizer-copy-main-to-model-params: 5.63 | optimizer: 33.38 | batch-generator: 880.99
 iteration    24700/  200000 | consumed samples:      3161600 | elapsed time per iteration (ms): 1639.1 | learning rate: 7.771E-05 | global batch size:   128 | lm loss: 3.675876E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 76.12 | backward-params-all-reduce: 62.08 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 6.35 | optimizer-unscale-and-check-inf: 5.19 | optimizer-clip-main-grad: 5.21 | optimizer-copy-main-to-model-params: 6.17 | optimizer: 36.80 | batch-generator: 1041.93
 iteration    24800/  200000 | consumed samples:      3174400 | elapsed time per iteration (ms): 1609.5 | learning rate: 7.762E-05 | global batch size:   128 | lm loss: 3.791521E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 79.30 | backward-params-all-reduce: 56.59 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 6.10 | optimizer-unscale-and-check-inf: 5.02 | optimizer-clip-main-grad: 5.12 | optimizer-copy-main-to-model-params: 6.04 | optimizer: 36.27 | batch-generator: 1043.66
 iteration    24900/  200000 | consumed samples:      3187200 | elapsed time per iteration (ms): 1700.1 | learning rate: 7.753E-05 | global batch size:   128 | lm loss: 3.822423E-07 | loss scale: 4294967296.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 84.66 | backward-params-all-reduce: 57.56 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.34 | optimizer-unscale-and-check-inf: 5.31 | optimizer-clip-main-grad: 5.37 | optimizer-copy-main-to-model-params: 6.41 | optimizer: 37.42 | batch-generator: 1068.26
 iteration    25000/  200000 | consumed samples:      3200000 | elapsed time per iteration (ms): 1652.7 | learning rate: 7.743E-05 | global batch size:   128 | lm loss: 3.689571E-07 | loss scale: 4294967296.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 76.59 | backward-params-all-reduce: 61.49 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.98 | optimizer-unscale-and-check-inf: 5.21 | optimizer-clip-main-grad: 5.14 | optimizer-copy-main-to-model-params: 6.38 | optimizer: 36.60 | batch-generator: 1075.59
 at iteration 25000, match long value: 0.606350376473573 | match short value: 0.9576528148590946 
--------------------------------------------------------------------------------------------------
 iteration    25100/  200000 | consumed samples:      3212800 | elapsed time per iteration (ms): 1707.3 | learning rate: 7.734E-05 | global batch size:   128 | lm loss: 3.830087E-07 | loss scale: 4294967296.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 83.75 | backward-params-all-reduce: 61.54 | backward-embedding-all-reduce: 0.10 | optimizer-copy-to-main-grad: 6.71 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 5.44 | optimizer-copy-main-to-model-params: 6.51 | optimizer: 38.10 | batch-generator: 1062.16
 iteration    25200/  200000 | consumed samples:      3225600 | elapsed time per iteration (ms): 1618.6 | learning rate: 7.725E-05 | global batch size:   128 | lm loss: 3.584815E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 81.57 | backward-params-all-reduce: 52.21 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 5.84 | optimizer-unscale-and-check-inf: 5.06 | optimizer-clip-main-grad: 4.92 | optimizer-copy-main-to-model-params: 12.42 | optimizer: 41.86 | batch-generator: 1042.59
 iteration    25300/  200000 | consumed samples:      3238400 | elapsed time per iteration (ms): 1679.7 | learning rate: 7.716E-05 | global batch size:   128 | lm loss: 3.631613E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 84.33 | backward-params-all-reduce: 54.21 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.41 | optimizer-unscale-and-check-inf: 5.17 | optimizer-clip-main-grad: 5.22 | optimizer-copy-main-to-model-params: 6.14 | optimizer: 36.90 | batch-generator: 1049.36
 iteration    25400/  200000 | consumed samples:      3251200 | elapsed time per iteration (ms): 1642.6 | learning rate: 7.707E-05 | global batch size:   128 | lm loss: 3.734859E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 74.28 | backward-params-all-reduce: 58.66 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 5.67 | optimizer-unscale-and-check-inf: 5.05 | optimizer-clip-main-grad: 5.09 | optimizer-copy-main-to-model-params: 6.03 | optimizer: 35.81 | batch-generator: 1032.97
 iteration    25500/  200000 | consumed samples:      3264000 | elapsed time per iteration (ms): 1619.4 | learning rate: 7.698E-05 | global batch size:   128 | lm loss: 3.684948E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 75.81 | backward-params-all-reduce: 64.25 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.20 | optimizer-unscale-and-check-inf: 5.18 | optimizer-clip-main-grad: 5.28 | optimizer-copy-main-to-model-params: 6.37 | optimizer: 36.98 | batch-generator: 1029.08
 at iteration 25500, match long value: 0.6056603223352455 | match short value: 0.9537109553213148 
---------------------------------------------------------------------------------------------------
 iteration    25600/  200000 | consumed samples:      3276800 | elapsed time per iteration (ms): 1608.5 | learning rate: 7.689E-05 | global batch size:   128 | lm loss: 3.774142E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 82.26 | backward-params-all-reduce: 52.55 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 6.09 | optimizer-unscale-and-check-inf: 5.12 | optimizer-clip-main-grad: 5.01 | optimizer-copy-main-to-model-params: 12.51 | optimizer: 42.55 | batch-generator: 1021.18
 iteration    25700/  200000 | consumed samples:      3289600 | elapsed time per iteration (ms): 1574.8 | learning rate: 7.680E-05 | global batch size:   128 | lm loss: 3.464300E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 79.85 | backward-params-all-reduce: 60.62 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.48 | optimizer-unscale-and-check-inf: 4.89 | optimizer-clip-main-grad: 4.85 | optimizer-copy-main-to-model-params: 5.63 | optimizer: 34.49 | batch-generator: 972.72
 iteration    25800/  200000 | consumed samples:      3302400 | elapsed time per iteration (ms): 1449.9 | learning rate: 7.671E-05 | global batch size:   128 | lm loss: 3.557031E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 74.67 | backward-params-all-reduce: 46.26 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.36 | optimizer-unscale-and-check-inf: 4.79 | optimizer-clip-main-grad: 4.46 | optimizer-copy-main-to-model-params: 5.46 | optimizer: 33.58 | batch-generator: 888.18
 iteration    25900/  200000 | consumed samples:      3315200 | elapsed time per iteration (ms): 1688.2 | learning rate: 7.662E-05 | global batch size:   128 | lm loss: 3.756594E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 85.57 | backward-params-all-reduce: 52.41 | backward-embedding-all-reduce: 0.10 | optimizer-copy-to-main-grad: 5.83 | optimizer-unscale-and-check-inf: 5.20 | optimizer-clip-main-grad: 4.95 | optimizer-copy-main-to-model-params: 12.00 | optimizer: 41.85 | batch-generator: 1042.45
 iteration    26000/  200000 | consumed samples:      3328000 | elapsed time per iteration (ms): 1663.7 | learning rate: 7.653E-05 | global batch size:   128 | lm loss: 3.397684E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 84.08 | backward-params-all-reduce: 55.68 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.25 | optimizer-unscale-and-check-inf: 5.21 | optimizer-clip-main-grad: 5.13 | optimizer-copy-main-to-model-params: 6.48 | optimizer: 37.07 | batch-generator: 1024.26
 at iteration 26000, match long value: 0.6316809402127123 | match short value: 0.9606817985702861 
---------------------------------------------------------------------------------------------------
 iteration    26100/  200000 | consumed samples:      3340800 | elapsed time per iteration (ms): 1598.8 | learning rate: 7.643E-05 | global batch size:   128 | lm loss: 3.491325E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 80.18 | backward-params-all-reduce: 52.46 | backward-embedding-all-reduce: 0.14 | optimizer-copy-to-main-grad: 5.91 | optimizer-unscale-and-check-inf: 5.01 | optimizer-clip-main-grad: 4.93 | optimizer-copy-main-to-model-params: 5.88 | optimizer: 35.47 | batch-generator: 980.69
 iteration    26200/  200000 | consumed samples:      3353600 | elapsed time per iteration (ms): 1630.1 | learning rate: 7.635E-05 | global batch size:   128 | lm loss: 3.749207E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 80.37 | backward-params-all-reduce: 56.10 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.89 | optimizer-unscale-and-check-inf: 5.27 | optimizer-clip-main-grad: 4.87 | optimizer-copy-main-to-model-params: 6.06 | optimizer: 35.96 | batch-generator: 960.25
 iteration    26300/  200000 | consumed samples:      3366400 | elapsed time per iteration (ms): 1608.8 | learning rate: 7.625E-05 | global batch size:   128 | lm loss: 3.521697E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 78.68 | backward-params-all-reduce: 62.14 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 6.59 | optimizer-unscale-and-check-inf: 5.07 | optimizer-clip-main-grad: 5.12 | optimizer-copy-main-to-model-params: 6.09 | optimizer: 36.74 | batch-generator: 966.43
 iteration    26400/  200000 | consumed samples:      3379200 | elapsed time per iteration (ms): 1660.3 | learning rate: 7.616E-05 | global batch size:   128 | lm loss: 3.226120E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 76.55 | backward-params-all-reduce: 56.77 | backward-embedding-all-reduce: 0.12 | optimizer-copy-to-main-grad: 6.39 | optimizer-unscale-and-check-inf: 5.14 | optimizer-clip-main-grad: 5.06 | optimizer-copy-main-to-model-params: 6.17 | optimizer: 36.63 | batch-generator: 1020.95
 iteration    26500/  200000 | consumed samples:      3392000 | elapsed time per iteration (ms): 1693.5 | learning rate: 7.607E-05 | global batch size:   128 | lm loss: 3.519857E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 80.50 | backward-params-all-reduce: 55.04 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 5.99 | optimizer-unscale-and-check-inf: 5.18 | optimizer-clip-main-grad: 5.07 | optimizer-copy-main-to-model-params: 6.08 | optimizer: 36.17 | batch-generator: 1009.61
 at iteration 26500, match long value: 0.6208619407013481 | match short value: 0.9565961093931614 
---------------------------------------------------------------------------------------------------
 iteration    26600/  200000 | consumed samples:      3404800 | elapsed time per iteration (ms): 1511.7 | learning rate: 7.598E-05 | global batch size:   128 | lm loss: 3.709820E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 73.54 | backward-params-all-reduce: 63.23 | backward-embedding-all-reduce: 0.11 | optimizer-copy-to-main-grad: 7.22 | optimizer-unscale-and-check-inf: 4.97 | optimizer-clip-main-grad: 5.56 | optimizer-copy-main-to-model-params: 6.06 | optimizer: 37.40 | batch-generator: 877.84
 iteration    26700/  200000 | consumed samples:      3417600 | elapsed time per iteration (ms): 1596.5 | learning rate: 7.589E-05 | global batch size:   128 | lm loss: 3.293646E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 77.73 | backward-params-all-reduce: 62.30 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.81 | optimizer-unscale-and-check-inf: 4.83 | optimizer-clip-main-grad: 4.82 | optimizer-copy-main-to-model-params: 6.11 | optimizer: 35.29 | batch-generator: 940.12
 iteration    26800/  200000 | consumed samples:      3430400 | elapsed time per iteration (ms): 1677.0 | learning rate: 7.580E-05 | global batch size:   128 | lm loss: 3.496811E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 81.93 | backward-params-all-reduce: 51.76 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 5.83 | optimizer-unscale-and-check-inf: 5.06 | optimizer-clip-main-grad: 4.78 | optimizer-copy-main-to-model-params: 7.25 | optimizer: 36.67 | batch-generator: 1030.38
 iteration    26900/  200000 | consumed samples:      3443200 | elapsed time per iteration (ms): 1801.7 | learning rate: 7.571E-05 | global batch size:   128 | lm loss: 3.259759E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 81.74 | backward-params-all-reduce: 61.48 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.55 | optimizer-unscale-and-check-inf: 5.24 | optimizer-clip-main-grad: 5.29 | optimizer-copy-main-to-model-params: 6.08 | optimizer: 37.17 | batch-generator: 1087.22
 iteration    27000/  200000 | consumed samples:      3456000 | elapsed time per iteration (ms): 1783.0 | learning rate: 7.562E-05 | global batch size:   128 | lm loss: 3.444505E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 85.69 | backward-params-all-reduce: 53.99 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.42 | optimizer-unscale-and-check-inf: 5.26 | optimizer-clip-main-grad: 5.32 | optimizer-copy-main-to-model-params: 6.73 | optimizer: 37.71 | batch-generator: 1069.56
 at iteration 27000, match long value: 0.6578839529903217 | match short value: 0.9645279916486679 
---------------------------------------------------------------------------------------------------
 iteration    27100/  200000 | consumed samples:      3468800 | elapsed time per iteration (ms): 1763.0 | learning rate: 7.553E-05 | global batch size:   128 | lm loss: 3.346581E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 80.77 | backward-params-all-reduce: 59.56 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 5.92 | optimizer-unscale-and-check-inf: 5.31 | optimizer-clip-main-grad: 5.08 | optimizer-copy-main-to-model-params: 6.11 | optimizer: 36.37 | batch-generator: 1035.31
 iteration    27200/  200000 | consumed samples:      3481600 | elapsed time per iteration (ms): 1780.1 | learning rate: 7.543E-05 | global batch size:   128 | lm loss: 3.459400E-07 | loss scale: 4294967296.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 85.48 | backward-params-all-reduce: 50.66 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.34 | optimizer-unscale-and-check-inf: 5.12 | optimizer-clip-main-grad: 5.06 | optimizer-copy-main-to-model-params: 6.02 | optimizer: 36.40 | batch-generator: 1102.85
 iteration    27300/  200000 | consumed samples:      3494400 | elapsed time per iteration (ms): 1722.8 | learning rate: 7.535E-05 | global batch size:   128 | lm loss: 3.249243E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   2 | number of nan iterations:   0 |
time (ms) | backward-compute: 80.96 | backward-params-all-reduce: 57.36 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.75 | optimizer-unscale-and-check-inf: 5.09 | optimizer-clip-main-grad: 4.93 | optimizer-copy-main-to-model-params: 5.93 | optimizer: 35.24 | batch-generator: 1022.73
 iteration    27400/  200000 | consumed samples:      3507200 | elapsed time per iteration (ms): 1704.4 | learning rate: 7.525E-05 | global batch size:   128 | lm loss: 3.301153E-07 | loss scale: 2147483648.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 84.13 | backward-params-all-reduce: 52.97 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.54 | optimizer-unscale-and-check-inf: 5.03 | optimizer-clip-main-grad: 4.73 | optimizer-copy-main-to-model-params: 5.80 | optimizer: 34.91 | batch-generator: 1016.16
 iteration    27500/  200000 | consumed samples:      3520000 | elapsed time per iteration (ms): 1776.8 | learning rate: 7.516E-05 | global batch size:   128 | lm loss: 3.364719E-07 | loss scale: 1073741824.0 | grad norm: 0.000 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | backward-compute: 79.51 | backward-params-all-reduce: 63.60 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.64 | optimizer-unscale-and-check-inf: 5.37 | optimizer-clip-main-grad: 5.44 | optimizer-copy-main-to-model-params: 10.98 | optimizer: 42.29 | batch-generator: 1088.27
 at iteration 27500, match long value: 0.6579220745503745 | match short value: 0.9638995170285813 
---------------------------------------------------------------------------------------------------
 iteration    27600/  200000 | consumed samples:      3532800 | elapsed time per iteration (ms): 1721.6 | learning rate: 7.507E-05 | global batch size:   128 | lm loss: 3.258073E-07 | loss scale: 1073741824.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 82.20 | backward-params-all-reduce: 56.44 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.77 | optimizer-unscale-and-check-inf: 5.12 | optimizer-clip-main-grad: 4.94 | optimizer-copy-main-to-model-params: 6.04 | optimizer: 35.53 | batch-generator: 1019.37
 iteration    27700/  200000 | consumed samples:      3545600 | elapsed time per iteration (ms): 1822.8 | learning rate: 7.498E-05 | global batch size:   128 | lm loss: 3.138844E-07 | loss scale: 1073741824.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 88.15 | backward-params-all-reduce: 47.57 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.35 | optimizer-unscale-and-check-inf: 5.19 | optimizer-clip-main-grad: 5.42 | optimizer-copy-main-to-model-params: 6.29 | optimizer: 37.54 | batch-generator: 1093.15
 iteration    27800/  200000 | consumed samples:      3558400 | elapsed time per iteration (ms): 1816.7 | learning rate: 7.489E-05 | global batch size:   128 | lm loss: 3.338674E-07 | loss scale: 1073741824.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 87.98 | backward-params-all-reduce: 53.18 | backward-embedding-all-reduce: 0.09 | optimizer-copy-to-main-grad: 6.51 | optimizer-unscale-and-check-inf: 5.23 | optimizer-clip-main-grad: 5.30 | optimizer-copy-main-to-model-params: 9.76 | optimizer: 41.16 | batch-generator: 1075.47
 iteration    27900/  200000 | consumed samples:      3571200 | elapsed time per iteration (ms): 1713.8 | learning rate: 7.480E-05 | global batch size:   128 | lm loss: 3.118990E-07 | loss scale: 1073741824.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 78.99 | backward-params-all-reduce: 59.24 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.95 | optimizer-unscale-and-check-inf: 5.22 | optimizer-clip-main-grad: 4.99 | optimizer-copy-main-to-model-params: 6.09 | optimizer: 36.13 | batch-generator: 994.57
 iteration    28000/  200000 | consumed samples:      3584000 | elapsed time per iteration (ms): 1786.1 | learning rate: 7.471E-05 | global batch size:   128 | lm loss: 3.447523E-07 | loss scale: 1073741824.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 89.77 | backward-params-all-reduce: 58.95 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 6.14 | optimizer-unscale-and-check-inf: 5.17 | optimizer-clip-main-grad: 5.35 | optimizer-copy-main-to-model-params: 6.33 | optimizer: 37.10 | batch-generator: 1043.43
 at iteration 28000, match long value: 0.6526208402960819 | match short value: 0.9601611095297883 
---------------------------------------------------------------------------------------------------
 iteration    28100/  200000 | consumed samples:      3596800 | elapsed time per iteration (ms): 1732.9 | learning rate: 7.462E-05 | global batch size:   128 | lm loss: 2.935009E-07 | loss scale: 1073741824.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 84.52 | backward-params-all-reduce: 51.56 | backward-embedding-all-reduce: 0.07 | optimizer-copy-to-main-grad: 5.90 | optimizer-unscale-and-check-inf: 4.94 | optimizer-clip-main-grad: 4.81 | optimizer-copy-main-to-model-params: 10.51 | optimizer: 39.89 | batch-generator: 989.07
 iteration    28200/  200000 | consumed samples:      3609600 | elapsed time per iteration (ms): 1655.3 | learning rate: 7.453E-05 | global batch size:   128 | lm loss: 3.289749E-07 | loss scale: 1073741824.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 81.31 | backward-params-all-reduce: 52.00 | backward-embedding-all-reduce: 0.11 | optimizer-copy-to-main-grad: 6.00 | optimizer-unscale-and-check-inf: 4.74 | optimizer-clip-main-grad: 4.77 | optimizer-copy-main-to-model-params: 5.71 | optimizer: 34.82 | batch-generator: 954.17
 iteration    28300/  200000 | consumed samples:      3622400 | elapsed time per iteration (ms): 1706.1 | learning rate: 7.444E-05 | global batch size:   128 | lm loss: 3.223935E-07 | loss scale: 1073741824.0 | grad norm: 0.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | backward-compute: 83.53 | backward-params-all-reduce: 42.85 | backward-embedding-all-reduce: 0.08 | optimizer-copy-to-main-grad: 5.81 | optimizer-unscale-and-check-inf: 5.00 | optimizer-clip-main-grad: 4.83 | optimizer-copy-main-to-model-params: 5.91 | optimizer: 35.27 | batch-generator: 1002.03
examples/tmp.sh: line 54:   667 Killed                  python -m torch.distributed.launch $DISTRIBUTED_ARGS pretrain_gw.py --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --num-layers 24 --hidden-size 1024 --num-attention-heads 16 --micro-batch-size 16 --global-batch-size 128 --segment-length 2048 --seq-length 127 --max-position-embeddings 127 --train-iters 200000 --save $CHECKPOINT_PATH --load $CHECKPOINT_PATH --data-path $DATA_PATH --vocab-file $VOCAB_FILE --data-impl mmap --split 949,50,1 --distributed-backend nccl --lr 0.0001 --lr-decay-style linear --min-lr 1.0e-5 --lr-decay-iters 99000 --weight-decay 1e-2 --clip-grad 1.0 --lr-warmup-fraction .002 --log-interval 100 --save-interval 100000 --eval-interval 500 --eval-iters 1 --dataloader-type cyclic --fp16 --bert-no-binary-head
